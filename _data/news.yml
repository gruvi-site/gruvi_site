#To add a new news item, please copy the form below, remove # characters and change the contents accordingly.
#It is important to preserve the exact same syntax and identation.
#The characters >- transform everything on the next line on a string, allowing us to escape : and - characters normaly, which otherwise are control characters on yml files.
#
#
#- date: October 13, 2017
#  headline: >-
#    SIG Asia and ICCV 2017 papers
#  text: >-
#    Op dit plein stond het hoofdkantoor van de NIROM: Nederlands Indische Radio Omroep
#  highlight: 1
#  image: news_1.PNG


- date: April 22, 2024
  headline: >-
    Zhiqin Chen receives both the 2024 Alain Fournier Award for the best Ph.D. dissertation and 2024 Eurographics PhD Award for Best PhD Thesis
  text: >-
    Congratulations to Zhiqin Chen for receiving both the 2024 <a href="https://graphicsinterface.org/awards/alain-fournier/">Alain Fournier Award</a> for the best Ph.D. dissertation in computer graphics in Canada and the 2024 <a href="https://www.eg.org/wp/eurographics-awards-programme/phd-award/">Eurographics PhD Award</a> for Best PhD Thesis.</p> <p>The <a href="https://graphicsinterface.org/awards/alain-fournier/">Alain Fournier Dissertation Award</a> is given annually for an outstanding doctoral dissertation completed at a Canadian university in the field of Computer Graphics. The award is named in honor of Alain Fournier, a Canadian researcher who did much to promote excellence, both within Canada and internationally, in the field of Computer Graphics.</p> <p><a href="https://www.eg.org/wp/eurographics-awards-programme/phd-award/">Eurographics PhD Award</a> aims to recognize good thesis work, to incentivize young researchers, and to offer them the opportunity to publish the state of the art section of their thesis as a STAR in the Computer Graphics Forum Journal. Eurographics annually grants three PhD thesis awards. They are jointly sponsored by Eurographics and the Computer Graphics Forum Journal.
  highlight: 1
  image: zhiqin_thesis_award.png
  
- date: April 22, 2024
  headline: >-
    Eurographics 2024: Spotlight on GruVi Lab
  text: >-
    Eurographics, the premier conference on computer graphics, will be held in <strong>Limassol, Cyprus</strong> this year (April 22-26, 2024). GrUVi lab will once again have a good show at Eurographics 2024, with:</p> <ul> <li><a href="https://hanhung.github.io/">Han-Hung Lee</a> (along with Manolis Savva and Angel X. Chang) presenting <a href="https://3dlg-hcvc.github.io/tt3dstar/">Text-to-3D Shape Generation STAR</a>.</li> <li><a href="https://czq142857.github.io/">Zhiqin Chen</a> receiving 2024 <a href="https://www.eg.org/wp/eurographics-awards-programme/phd-award/">Eurographics PhD Award</a> for Best PhD Thesis.</li> <li><a href="https://www.sfu.ca/~amahdavi/">Ali Mahdavi-Amiri</a> serving as the International Program Committee member.</li> </ul> <p>Congratulations!
  highlight: 1
  image: sfu_eg_2024.png

- date: March 18, 2024
  headline: >-
    3DV 2024: Spotlight on GruVi Lab
  text: >-
    3D Vision (3DV), the premier conference on computer vision, will be held in-person in Davos, Switzerland on March 18-21, 2024. GrUVi lab will once again have a good show at 3DV 2024, with 2 technical papers. And Andrea Tagliasacchi will be serving as the program co-chair for the conference.</p> <p>Congratulations to all the authors! And here are the 2 accepted papers:</p> <p><a href="https://3dlg-hcvc.github.io/">Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects</a></p> <p><a href="https://3dlg-hcvc.github.io/OPDMulti/">OPDMulti: Openable Part Detection for Multiple Objects</a>
  highlight: 1
  image: sfu_3dv_2024.png

- date: February 23, 2024
  headline: >-
    VCR workshop for AAAI 2024
  text: >-
    VCR seminar will hold a workshop for AAAI visitors from other universities. Below is the schedule for the talks.</p> <ul> <li>11 am-11:40 am <a href="https://webdocs.cs.ualberta.ca/~santanad/">Levi Lelis</a> (U of Alberta)</li> <li>11:40 am-12:20 pm <a href="https://scholar.google.ch/citations?user=Yjh-GHsAAAAJ&amp;hl=en">Vahid Babaei</a> (MPI) Inverse Design with Neural Surrogate Models</li> <li>12:20 pm -1:00 pm lunch break</li> <li>1 pm -1:40 pm <a href="https://scholar.google.ca/citations?user=tpoh43QAAAAJ&amp;hl=en">Sven Koenig</a> (USC) Multi-Agent Path Finding and Its Applications</li> <li>1:40 pm -2:20 pm <a href="https://jiaoyangli.me/">Jiaoyang Li</a> (CMU) Layout Design for Large-Scale Multi-Robot Coordination</li> </ul> <p>Talk 1 (11 am-11:40) <a href="https://webdocs.cs.ualberta.ca/~santanad/">Levi Lelis</a>, Department of Computing Science, University of Alberta</p> <blockquote> <p><strong>Title:</strong> Learning Options by Extracting Programs from Neural Networks</p> <p><strong>Abstract:</strong> In this talk, I argue for a programmatic mindset in reinforcement learning, proposing that agents should generate libraries of programs encoding reusable behaviors. When faced with a new task, the agent learns how to combine existing programs and generate new ones. This approach can be helpful even when policies are encoded in seemingly non-decomposable representations like neural networks. I will show that neural networks with piecewise linear activation functions can be mapped to a program with if-then-else structures. Such a program can then be easily decomposed into sub-programs with the same input type of the original network. In the case of networks encoding policies, each sub-program can be seen as an option---a temporally extended action. All these sub-programs form a library of agent behaviors that can be reused later, in downstream tasks. Considering that even small networks can encode a large number of sub-programs, we select sub-programs that are likely to generalize to unseen tasks. This is achieved through a subset selection procedure that minimizes the Levin loss. Empirical evidence from challenging exploration scenarios in two grid-world domains demonstrates that our methodology can extract helpful programs, thus speeding up the learning process in tasks that are similar and yet distinct from the one used to train the original model.</p> <p><strong>Bio:</strong> Dr. Levi Lelis is an Assistant Professor at the University of Alberta, an Amii Fellow, and a CIFAR AI Chair. Levi’s research is dedicated to the development of principled algorithms to solve combinatorial search problems. These problems are integral to optimizing tasks in various sectors. Levi’s research group is focused on combinatorial search problems arising from the search for programmatic solutions---computer programs written in a domain-specific language encoding problem solutions. Levi believes that the most promising path to creating agents that learn continually, efficiently, and safely is to represent the agents’ knowledge programmatically. While programmatic representations offer many advantages, including modularity and reusability, they present a significant challenge: the need to search over large, non-differentiable spaces not suited for gradient descent methods. Addressing this challenge is the current focus of Levi’s work.</p> </blockquote> <p>Talk 2 (11:40 am-12:20) <a href="https://scholar.google.ch/citations?user=Yjh-GHsAAAAJ&amp;hl=en">Vahid Babaei</a> , Max Planck Institute for Informatics</p> <blockquote> <p><strong>Title</strong>: Inverse Design with Neural Surrogate Models</p> <p><strong>Abstract</strong>: The digitalization of manufacturing is turning fabrication hardware into computers. As traditional tools, such as computer aided design, manufacturing, and engineering (CAD/CAM/CAE) lag behind this new paradigm, the field of computational fabrication has recently emerged from computer graphics to address this knowledge gap with a computer-science mindset. Computer graphics is extremely powerful in creating content for the virtual world. The connection is therefore a natural one as the digital fabrication hardware is starving for innovative content. In this talk, I will focus on inverse design, a powerful paradigm of content synthesis for digital fabrication, which creates fabricable designs given the desired performances. Specifically, I will discuss a class of inverse design problems that deals with data-driven neural surrogate models. These surrogates learn and replace a forward process, such as a computationally heavy simulation.</p> <p><strong>Bio</strong>: <a href="https://scholar.google.ch/citations?user=Yjh-GHsAAAAJ&amp;hl=en">Vahid Babaei</a> leads the AI aided Design and Manufacturing group at the Computer Graphics Department of the Max Planck Institute for Informatics in Saarbrücken, Germany. He was a postdoctoral researcher at the Computational Design and Fabrication Group of Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. He obtained his PhD in Computer Science from EPFL. Vahid Babaei is the recipient of the 2023 Germany-wide Curious Mind Award in the area of ‘AI, Digitalization, and Robotics’, the Hermann Neuhaus Prize of the Max Planck Society, and two postdoctoral fellowships awarded by the Swiss National Science Foundation. He is interested in developing original computer science methods for both engineering design and advanced manufacturing.</p> </blockquote> <p>Talk 3 (1 pm -1:40 pm) <a href="https://scholar.google.ca/citations?user=tpoh43QAAAAJ&amp;hl=en">Sven Koenig</a>, computer science department, University of Southern California</p> <blockquote> <p><strong>Title</strong>: Multi-Agent Path Finding and Its Applications</p> <p><strong>Abstract</strong>: The coordination of robots and other agents becomes more and more important for industry. For example, on the order of one thousand robots already navigate autonomously in Amazon fulfillment centers to move inventory pods all the way from their storage locations to the picking stations that need the products they store (and vice versa). Optimal and even some approximately optimal path planning for these robots is NP-hard, yet one must find high-quality collision-free paths for them in real-time. Algorithms for such multi-agent path-finding problems have been studied in robotics and theoretical computer science for a longer time but are insufficient since they are either fast but of insufficient solution quality or of good solution quality but too slow. In this talk, I will discuss different variants of multi-agent path-finding problems, cool ideas for both solving them and executing the resulting plans robustly, and several of their applications. Our research on this topic has been funded by both NSF and Amazon Robotics.</p> <p><strong>Bio</strong>: <a href="http://idm-lab.org/publications.html">Sven Koenig</a> is a professor of computer science at the University of Southern California. Most of his current research focuses on planning for single agents (such as robots) or multi-agent systems. Additional information about him can be found on his webpages: <a href="http://idm-lab.org/">idm-lab.org</a>.</p> </blockquote> <p>Talk 4 (1:40 pm -2:20 pm) <a href="https://jiaoyangli.me/">Jiaoyang Li</a>, Robotics Institute, Carnegie Mellon University</p> <blockquote> <p>Title: Layout Design for Large-Scale Multi-Robot Coordination</p> <p><strong>Abstract:</strong> Today, thousands of robots are navigating autonomously in warehouses, transporting goods from one location to another. While numerous planning algorithms are developed to coordinate robots more efficiently and robustly, warehouse layouts remain largely unchanged – they still adhere to the traditional pattern designed for human workers rather than robots. In this talk, I will share our recent progress in exploring layout design and optimization to enhance large-scale multi-robot coordination. I will first introduce a direct layout design method, followed by a method to optimize layout generators instead of layouts. I will then extend these ideas to virtual layout design, which does not require changes to the physical world that robots navigate and thus has the potential for applications beyond automated warehouses.</p> <p><strong>Bio:</strong> <a href="https://jiaoyangli.me/">Jiaoyang Li</a> is an assistant professor at the Robotics Institute of CMU School of Computer Science. She received her Ph.D. in computer science from the University of Southern California (USC) in 2022. Her research interests lie in the coordination of large robot teams. Her research received recognition through prestigious paper awards (e.g., best student paper, best demo, and best student paper nomination at ICAPS in 2020, 2021, and 2023, along with the best paper finalist at MRS in 2023) and competition championships (e.g., winners of NeurIPS Flatland Challenge in 2020 and Flatland 3 in 2021, as well as the League of Robot Runners sponsored by Amazon Robotics in 2023). Her Ph.D. dissertation also received the best dissertation awards from ICAPS, AAMAS, and USC in 2023.</p> </blockquote>
  highlight: 1
  image: sfu_aaai_2024.png

- date: February 2, 2024
  headline: >-
    SFU-UBC Visual Computing Meeting
  text: >-
    We are thrilled to announce that a special VCR event: the <strong>SFU-UBC Visual Computing Meeting</strong> will be held in SFU Burnaby campus (Feb 2nd from 10:00 am-3:00 pm). This special session will feature professors and students from the UBC and SFU visual computing community for a day of engaging discussions and networking.</p> <p><em>Schedule</em> – 10:00am → 12:00pm: talks (Section1 and 2) – 12:00pm → 2:00pm: lunch and posters – 2:00pm → 3:00pm: talks (Section 3)
  highlight: 1
  image: sfu_ubc_2024.png

- date: January 26, 2024
  headline: >-
    Talk by Dr. Jamie Shotton from Wayve
  text: >-
    <strong>Title:</strong> FRONTIERS IN EMBODIED AI FOR AUTONOMOUS DRIVING</p> <p><strong>Abstract:</strong> Over the last decade, fundamental advances in AI have driven unprecedented progress across many disciplines and applications. And yet, despite significant progress, autonomous vehicles are still far from mainstream even after billions of dollars of investment. In this talk we’ll explore what’s been holding progress back, and how by adopting a modern embodied AI approach to the problem, Wayve is finally unlocking the potential of autonomous driving in complex and unstructured urban environments such as central London. We’ll also explore some of our latest research in multimodal learning to combine the power of large language models with the driving problem (“LINGO-1”), and in generative world models as learned simulators trained to predict the future conditioned on ego action (“GAIA-1&quot;).</p> <p><strong>Bio:</strong> Jamie Shotton is a leader in AI research and development, with a track record of incubating transformative new technologies and experiences from early stage research to shipping product. He is Chief Scientist at <a href="https://wayve.ai/">Wayve</a>, building foundation models for embodied intelligence, such as <a href="https://wayve.ai/thinking/scaling-gaia-1/">GAIA</a> and <a href="https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/">LINGO</a>, to enable safe and adaptable autonomous vehicles. Prior to this he was Partner Director of Science at Microsoft and head of the <a href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">Mixed Reality &amp; AI Labs</a> where he shipped foundational features including body tracking for Kinect and the hand- and eye-tracking that enable HoloLens 2’s instinctual interaction model. He has explored applications of AI in autonomous driving, mixed reality, virtual presence, human-computer interaction, gaming, robotics, and healthcare. </p> <p>He has received multiple Best Paper and Best Demo awards at top-tier academic conferences, and the <a href="https://www.thecvf.com/?page_id=534">Longuet-Higgins Prize</a> test-of-time award at CVPR 2021. His work on Kinect was awarded the Royal Academy of Engineering’s gold medal <a href="https://raeng.org.uk/news/news-releases/2011/june/cambridge-engineers-kinect-land-uk-prize">MacRobert Award</a> in 2011, and he shares Microsoft’s Outstanding Technical Achievement Award for 2012 with the Kinect engineering team. In 2014 he received the <a href="https://tc.computer.org/tcpami/young-researcher-award/">PAMI Young Researcher Award</a>, and in 2015 the MIT Technology Review <a href="https://www.technologyreview.com/innovators-under-35/2015">Innovator Under 35 Award</a>. He was awarded the Royal Academy of Engineering’s <a href="https://raeng.org.uk/grants-prizes/prizes/prizes-and-medals/individual-medals/silver-medal">Silver Medal</a> in 2020. He was elected a <a href="https://raeng.org.uk/about-us/fellowship">Fellow of the Royal Academy of Engineering</a> in 2021.
  highlight: 1
  image: vcrseminar.png

- date: January 15, 2024
  headline: >-
    Dr. Richard Zhang is recognized as a 2024 IEEE fellow for advancing research in Visual Computing
  text: >-
    Congratulations to Professor Richard Zhang on being recognized as a 2024 IEEE Fellow for his “contributions to shape analysis and synthesis in visual computing.” IEEE is the world&#39;s largest technical professional organization dedicated to advancing technology for the benefit of humanity. It has more than 450,000 members in more than 190 countries and is a leading authority in many areas, including engineering, computing, and technology information. Less than one-tenth of one percent of IEEE members worldwide are selected as Fellows in any year. Fellow status is awarded to individuals with &quot;an outstanding record of accomplishments in any of the IEEE fields of interest.”</p> <p>Please check out <a href="https://www.sfu.ca/computing/newsandevents/2024/dr--richard-zhang-is-a-2024-ieee-fellow-for-advancing-research-i.html">SFU news coverage</a> on his IEEE Fellow elevation.
  highlight: 1
  image: richard_zhang_ieee.png

- date: November 11, 2023
  headline: >-
    SIGGRAPH Asia 2023: Spotlight on GruVI Lab
  text: >-
    SIGGRAPG Asia, the premier conference on computer graphics, will be held in Sydney, Australia this year (Dec 12-15). GrUVi lab will once again have a good show at SIGGRAPG Asia, with 5 technical papers. Congratulations to all the authors!</p> <p>And here are the 5 accepted papers:</p> <p><a href="https://yaksoy.github.io/intrinsicCompositing/">Intrinsic Harmonization for Illumination-Aware Compositing</a></p> <p><a href="https://qiminchen.github.io/shaddr/">ShaDDR: Real-Time Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering</a></p> <p><a href="https://arxiv.org/abs/2306.08226">CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration</a></p> <p><a href="https://github.com/Salingo/Interaction-Driven-Reconstruction">Interaction-Driven Active 3D Reconstruction with Object Interiors</a></p> <p><a href="https://vcc.tech/research/2023/TAPNet++">Neural Packing for Real: from Visual Sensing to Reinforcement Learning</a>
  highlight: 0
  image: SA23-Logo.png

- date: November 6, 2023
  headline: >-
    NeurIPS 2023: Spotlight on GruVi Lab
  text: >-
    NeurIPS, the premier conference on machine learning, will be held in <strong>New Orleans</strong> this year (Dec 10-16). GrUVi lab will once again have a good show at NeurIPS, with <a href="https://gruvi.ca/publications/">7 technical papers and 1 dataset and benchmarks paper</a>!</p> <p>Please refer to <a href="https://gruvi.ca/publications/">our publication page</a> to see more details.
  highlight: 0
  image: neurips_logo.jpg

- date: November 3, 2023
  headline: >-
    Talk by Dr. Taras Kucherenko from EA
  text: >-
    <a href="https://stream.sfu.ca/Media/Play/8e962f33c1474ad2ae7e94cf9c2901831d">Click this link to see the talk replay</a>.</p> <p><strong>Title</strong>: Co-speech gesture generation</p> <p><strong>Abstract</strong>: Gestures accompanying speech are essential to natural and efficient embodied human communication. The automatic generation of such co-speech gestures is a long-standing problem in computer animation. It is considered an enabling technology in film, games, virtual social spaces, and interaction with social robots. The problem is made challenging by the idiosyncratic and non-periodic nature of human co-speech gesture motion, and by the great diversity of communicative functions that gestures encompass. Gesture generation has seen surging interest recently, owing to the emergence of more and larger datasets of human gesture motion, combined with strides in deep-learning-based generative models that benefit from the growing availability of data. This talk will review co-speech gesture generation research development, focusing on deep generative models.</p> <p><strong>Bio</strong>: Dr. Taras Kucherenko is currently a Research Scientist at Electronic Arts. He finished a Ph.D. at the KTH Royal Institute of Technology in Stockholm in 2021. His research is on machine learning models for non-verbal behavior generation, such as hand gestures and facial expressions. For his research papers, he received ICMI 2020 Best Paper Award and IVA 2020 Best Paper Award. Taras was also the main organizer of The GENEA (Generation and Evaluation of Non-verbal Behavior for Embodied Agents) Workshop and Challenge in 2020, 2021, 2022, and 2023.
  highlight: 0
  image: vcrseminar.png

- date: October 27, 2023
  headline: >-
    Talk by Prof. Tat-Jun Chin from University of Adelaide
  text: >-
    <a href="https://stream.sfu.ca/Media/Play/81e9a38ce6b240338561d4ecbf1139101d">Click this link to see the talk replay</a>.</p> <p><strong>Title</strong>: Quantum Computing for Robust Fitting</p> <p><strong>Abstract</strong>: Many computer vision applications need to recover structure from imperfect measurements of the real world. The task is often solved by robustly fitting a geometric model onto noisy and outlier-contaminated data. However, relatively recent theoretical analyses indicate that many commonly used formulations of robust fitting in computer vision are not amenable to tractable solution and approximation. In this paper, we explore the usage of quantum computers for robust fitting. To do so, we examine the feasibility of two types of quantum computer technologies---universal gate quantum computers and quantum annealers---to solve robust fitting. Novel algorithms that are amenable to the quantum machines have been developed, and experimental results on current noisy intermediate scale quantum computers (NISQ) will be reported. Our work thus proposes one of the first quantum treatments of robust fitting for computer vision.</p> <p><strong>Bio</strong>: Tat-Jun (TJ) Chin is SmartSat CRC Professorial Chair of Sentient Satellites at The University of Adelaide. He received his PhD in Computer Systems Engineering from Monash University in 2007, which was partly supported by the Endeavour Australia-Asia Award, and a Bachelor in Mechatronics Engineering from Universiti Teknologi Malaysia in 2004, where he won the Vice Chancellor’s Award. TJ’s research interest lies in computer vision and machine learning for space applications. He has published close to 200 research articles, and has won several awards for his research, including a CVPR award (2015), a BMVC award (2018), Best of ECCV (2018), three DST Awards (2015, 2017, 2021), an IAPR Award (2019) and an RAL Best Paper Award (2021). TJ pioneered the AI4Space Workshop series and is an Associate Editor at the International Journal of Robotics Research (IJRR) and Journal of Mathematical Imaging and Vision (JMIV). He was a Finalist in the Academic of the Year Category at Australian Space Awards 2021.
  highlight: 0
  image: vcrseminar.png

- date: October 20, 2023
  headline: >-
    Talk by Prof. Leonid Sigal from UBC
  text: >-
    <a href="https://stream.sfu.ca/Media/Play/0377be26412347d5934fbf1675821edb1d">Click this link to see the talk replay</a>.</p> <p><strong>Title:</strong> Efficient, Less-biased and Creative Visual Learning</p> <p><strong>Abstract:</strong> In this talk I will discuss recent methods from my group that focus on addressing some of the core challenges of current visual and multi-modal cognition, including efficient learning, bias and user-controlled generation. Centering on these larger themes I will talk about a number of strategies (and corresponding papers) that we developed to address these challenges. I will start by discussing transfer learning techniques in the context of a semi-supervised object detection and segmentation, highlighting a model that is applicable to a range of supervision: from zero to a few instance-level samples per novel class. I will then talk about our recent work on building a foundational image representation model by combining two successful strategies of masking and sequential token prediction. I will also discuss some of our work on scene graph generation which, in addition to improving overall performance, allows for scalable inference and ability to control data bias (by trade off major improvements on rare classes for minor declines on most common classes). The talk will end with some of our recent work on generative modeling which focuses on novel-view synthesis and language-conditioned diffusion-based story generation. The core of the latter approach is visual memory that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft attention over the memories enables effective reference resolution and learns to maintain scene and actor consistency when needed.</p> <p><strong>Biography:</strong> Prof. Leonid Sigal is a Professor at the University of British Columbia (UBC). He was appointed CIFAR AI Chair at the Vector Institute in 2019 and an NSERC Tier 2 Canada Research Chair in Computer Vision and Machine Learning in 2018. Prior to this, he was a Senior Research Scientist, and a group lead, at Disney Research. He completed his Ph.D at Brown University in 2008; received his B.Sc. degrees in Computer Science and Mathematics from Boston University in 1999, his M.A. from Boston University in 1999, and his M.S. from Brown University in 2003. He was a Postdoctoral Researcher at the University of Toronto, between 2007-2009. Leonid’s research interests lie in the areas of computer vision, machine learning, and computer graphics; with the emphasis on approaches for visual and multi-modal representation learning, recognition, understanding and generative modeling. He has won a number of prestigious research awards, including Killam Accelerator Fellowship in 2021 and has published over 100 papers in venues such as CVPR, ICCV, ECCV, NeurIPS, ICLR, and Siggraph.
  highlight: 0
  image: vcrseminar.png

- date: October 13, 2023
  headline: >-
    Talk by Prof. Li Cheng from University of Alberta
  text: >-
    <a href="https://stream.sfu.ca/Media/Play/e1bf6d3177ed4f3288ad7ddde49f286a1d">Click this link to see the talk replay</a>.</p> <p><strong>Title</strong>: Visual Human Motion Analysis</p> <p><strong>Abstract</strong>: Recent advancement of imaging sensors and deep learning techniques has opened door to many interesting applications for visual analysis of human motions. In this talk, I will discuss our research efforts toward addressing the related tasks of 3-D human motion syntheses, pose and shape estimation from images and videos, visual action quality assessment. Looking forward, our results could be applied to everyday life scenarios such as natural user interface, AR/VR, robotics, and gaming, among others.</p> <p><strong>Bio</strong>: Li CHENG is a professor at the Department of Electrical and Computer Engineering, University of Alberta. He is associate editors of IEEE Trans. Multimedia and Pattern Recognition Journal. Prior to joining University of Alberta, He worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. His current research interests are mainly on human motion analysis, mobile and robot vision, and machine learning. More details can be found a <a href="http://www.ece.ualberta.ca/~lcheng5/">http://www.ece.ualberta.ca/~lcheng5/</a>.
  highlight: 0
  image: vcrseminar.png

- date: September 18, 2023
  headline: >-
    ICCV 2023: Spotlight on GruVI Lab
  text: >-
    ICCV, the premier conference on computer vision, will be held in <strong>Paris</strong> this year (Oct 2-6). GrUVi lab will once again have a good show at ICCV, with <a href="https://gruvi.ca/publications/">6 technical papers</a>, 3 co-organized workshops! Also, Prof. Yasutaka Furukawa serves as a program chair for this year’s ICCV!</p> <p>For the workshops, Prof. Richard Zhang co-organizes the <strong><a href="https://3dv-in-ecommerce.github.io/">3D Vision and Modeling Challenges in eCommerce</a>,</strong> Prof. Angel Chang co-organizes the <strong><a href="https://languagefor3dscenes.github.io/ICCV2023/">3rd Workshop on Language for 3D Scenes</a></strong> and <strong><a href="https://iccv-clvl.github.io/2023/">CLVL: 5th Workshop on Closing the Loop between Vision and Language.</a></strong> Also, Prof. Manolis Savva will give a talk at the <strong><a href="https://opensun3d.github.io/">1st Workshop on Open-Vocabulary 3D Scene Understanding</a></strong></p> <p>And here are the 6 accepted papers:</p> <p><a href="https://3dlg-hcvc.github.io/multi3drefer/">Multi3DRefer: Grounding Text Description to Multiple 3D Objects</a></p> <p><a href="https://ds-fusion.github.io/">DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</a></p> <p><a href="https://sked-paper.github.io/">SKED: Sketch-guided Text-based 3D Editing</a></p> <p><a href="https://3dlg-hcvc.github.io/paris/">PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects</a></p> <p><a href="https://arxiv.org/abs/2301.10460">HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling</a></p> <p><a href="https://arxiv.org/abs/2212.00836">UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding</a></p> <p>Congrats for the authors!
  highlight: 0
  image: iccv23_logo.png

- date: June 26, 2023
  headline: >-
    Talk by Mikaela Uy from Stanford 
  text: >-
    The recording is available at this <a href="https://stream.sfu.ca/Media/Play/17d663a3fe654ee3a40d88adb85f456e1d">link</a>.</p> <p><strong>Title</strong>: Towards Controllable 3D Content Creation by leveraging Geometric Priors</p> <p><strong>Abstract</strong>: The growing popularity for extended realities pushes the demand for the automatic creation and synthesis of new 3D content that would otherwise be a tedious and laborious process. A key property needed to make 3D content creation useful is <strong>user controllability</strong> as it allows one to realize specific ideas. User-control can be of various forms, e.g. target scans, input images or programmatic edits etc. In this talk, I will be touching works that enable user-control through i) object parts and ii) sparse scene images by leveraging geometric priors. The former utilizes object semantic priors by proposing a novel shape space factorization through an introduced cross diffusion network that enabled multiple applications in both shape generation and editing. The latter leverages pretrained models of large 2D datasets for sparse view 3D NeRF reconstruction of scenes by learning a distribution of geometry represented as ambiguity-aware depth estimates. As an add-on, we will also briefly revisit the volume rendering equation in NeRFs and reformulate it to piecewise linear density that alleviates underlying issues caused by quadrature instability.</p> <p><strong>Bio:</strong> Mika is a fourth year PhD student at Stanford advised by Leo Guibas. Her research focuses on the representation and generation of objects/scenes for user-controllable 3D content creation. She was a research intern at Adobe, Autodesk and now, Google, and is generously supported by Apple AI/ML PhD Fellowship and Snap Research Fellowship.
  highlight: 0
  image: mikauy_talk.png

- date: August 4, 2023
  headline: >-
    A Trilogy of Character Animation Research in SIGGRAPH 2023
  text: >-
    We are proud to highlight that three of Prof. Jason Peng&#39;s research papers will be presented in the upcoming SIGGRAPH 2023. These papers mark advances in physics-based character animation.</p> <p>Below are titles and links to the related project pages:</p> <p>Learning Physically Simulated Tennis Skills from Broadcast Videos <a href="https://xbpeng.github.io/projects/Vid2Player3D/index.html">https://xbpeng.github.io/projects/Vid2Player3D/index.html</a></p> <p>Synthesizing Physical Character-Scene Interactions <a href="https://xbpeng.github.io/projects/InterPhys/index.html">https://xbpeng.github.io/projects/InterPhys/index.html</a></p> <p>CALM: Conditional Adversarial Latent Models for Directable Virtual Characters <a href="https://xbpeng.github.io/projects/CALM/index.html">https://xbpeng.github.io/projects/CALM/index.html</a></p> <p>Note: The SIGGRAPH conference, short for Special Interest Group on Computer GRAPHics and Interactive Techniques, is the world&#39;s premier annual event for showcasing the latest innovations in computer graphics and interactive techniques. It brings together researchers, artists, developers, filmmakers, scientists, and business professionals from around the globe. The conference offers a unique blend of educational sessions, hands-on workshops, and exhibitions of cutting-edge technology and applications.
  highlight: 0
  image: siggraph23_jason.jpg

- date: June 14, 2023
  headline: >-
    GrUVi making waves at CVPR 2023
  text: >-
    CVPR, the premier conference on computer vision, will be held in <strong>Vancouver</strong> this year (June 18-22). GrUVi lab will once again have an incredible show at CVPR, with <a href="https://gruvi.ca/publications/">12 technical papers</a>, 6 invited talks, 4 co-organized workshops!</p> <h1 id="conference-and-workshop-co-organization">Conference and workshop co-organization</h1> <p>Former GrUVi Professor Greg Mori serves as one of the four general conference chairs for the main CVPR conference!  Prof. Angel Chang, as one of the social activity chairs, is helping to organize the speed mentoring sessions.</p> <p>In addition, we have exciting workshops and challenges that are organized by GrUVi members as well:</p> <ul> <li><a href="https://cv4aec.github.io/">Computer Vision in the Built Environment workshop</a> - co-organized by Prof. Yasutaka Furukawa</li> <li><a href="https://struco3d.github.io/cvpr2023/index.html">Second Workshop on Structural and Compositional Learning on 3D Data</a> (Struco3D) - co-organized by Prof. Richard Zhang.</li> <li><a href="http://www.scan-net.org/cvpr2023workshop/">ScanNet Indoor Scene Understanding Challenge</a> - co-organized by Prof. Angel X. Chang and Prof. Manolis Savva</li> <li><a href="https://embodied-ai.org/">Embodied AI Workshop</a> featuring a variety of challenges including the <a href="http://multion-challenge.cs.sfu.ca/">Multi-Object Navigation (MultiON) challenge</a> (co-organized by Sonia Raychaudhuri, Angel Chang, and Manolis Savva)</li> </ul> <h1 id="workshop-talks">Workshop talks</h1> <p>Prof. Andrea Tagliasacchi is invited to give a keynote talk both in <a href="https://struco3d.github.io/cvpr2023/index.html">Struco3D</a> and <a href="https://generative-vision.github.io/workshop-CVPR-23/">Generative Models for Computer Vision</a> (both on June 18th). He also will give a spotlight at the <a href="https://sites.google.com/view/cvpr23-ac-workshop">Area Chair workshop</a> on Saturday. </p> <p>In the <a href="https://sites.google.com/view/wicvcvpr2023/program/panel">Women in Computer Vision</a><strong><strong>,</strong></strong> Prof. Angel Chang will be giving a talk on June 19th. She is also invited to give talks at workshops on <a href="https://sites.google.com/view/cvpr2023-3d-vision-robotics">3D Vision and Robotics</a> (June 18th),  <a href="https://3dcompat-dataset.org/workshop">Compositional 3D Vision</a> (June 18th), and <a href="https://asu-apg.github.io/odrum/">Open-Domain Reasoning Under Multi-Modal Settings</a> (June 19th).</p> <h1 id="technical-papers">Technical papers</h1> <p>Congratulations to all authors for the accepted papers! The full list of papers featured on CVPR 2023 can be accessed <a href="https://gruvi.ca/publications/">here</a>.
  highlight: 0
  image: gruvi_cvpr23.png

- date: June 9, 2023
  headline: >-
    Talk by Prof. Emily Whiting from Boston University
  text: >-
    Title: Computational Fabrication: Design of Geometry and Materials from Simulation to Physical</p> <p>Abstract: Advancements in rapid prototyping technologies are closing the gap between what we can simulate computationally and what we can build. The effect is opening up new design domains for creating objects with novel functionality, and introducing experimental manufacturing processes. My work applies traditional computer graphics techniques, leveraging tools in simulation, animation, and rendering for making functional objects in the physical world. In this talk I will present several lines of inquiry in computational fabrication workflows. On the topic of manufacturing processes I will discuss work on fabric formwork for creating plaster-cast sculptures. In the domain of design I will discuss mechanics-based strategies for custom-fit knitted garments and discovery of 3D printed structures. Finally I will cover developments on design of tactile illustrations for blind and visually impaired individuals for depicting 3D objects.</p><p>Bio:Emily Whiting is an Associate Professor of Computer Science at Boston University. Her research in Computer Graphics combines digital geometry processing, engineering mechanics, and rapid prototyping to explore the development of computational tools for designing functionally-valid and fabrication-ready real world objects. Her lab&#39;s work builds on collaborations in a broad range of fields including architecture, human computer interaction, and accessible technologies. She received her PhD from MIT in Computer Graphics and Building Technology. She is the recipient of the NSF CAREER Award and Sloan Research Fellowship. She was General Chair of the ACM Symposium on Computational Fabrication in 2020 and 2021.
    
  highlight: 0
  image: talk_emily.jpeg

- date: June 2, 2023
  headline: >-
    Talk by Silvia Sellan from the University of Toronto
  text: >-
    This week in the VCR seminar, we had a guest speaker Silvia Sellan from the University of Toronto who will give a talk on various geometry problems. See the recording here: https://stream.sfu.ca/Media/Play/10fc4229c5cf43e8b7f2efa29689cd241d
  highlight: 0
  image: talk_silvia.jpeg

- date: May 26, 2023
  headline: >-
    Talk by Prof. Daniel Weiskopf from the University of Stuttgart
  text: >-
    This week in the VCR seminar, we had a guest speaker Daniel Weiskopf from the University of Stuttgart who will give a talk on multidimensional visualization (please see below for information about the talk). See the recording here: <a href="https://stream.sfu.ca/Media/Play/a0b66c0d023f48f5bbe1d7790fa1b8681d">https://stream.sfu.ca/Media/Play/a0b66c0d023f48f5bbe1d7790fa1b8681d</a></p><p>Title: Multidimensional Visualization</p><p>Abstract: Multidimensional data analysis is of broad interest for a wide range of applications. In this talk, I discuss visualization approaches that support the analysis of such data. I start with a brief overview of the field, a conceptual model, and a discussion of visualization strategies. This part is accompanied by a few examples of recent advancements, with a focus on results from my own work. In the second part, I detail techniques that enrich basic visual mappings like scatterplots, parallel coordinates, or plots of dimensionality reduction by incorporating local correlation analysis. I also discuss sampling issues in multidimensional visualization, and how we can extend it to uncertainty visualization. The talk closes with an outlook on future research directions.</p><p>Bio: Daniel Weiskopf is a professor and one of the directors of the Visualization Research Center (VISUS) and acting director of the Institute for Visualization and Interactive Systems (VIS), both at the University of Stuttgart, Germany. He received his Dr. rer. nat. (PhD) degree in physics from the University of Tübingen, Germany (2001), and the Habilitation degree in computer science at the University of Stuttgart, Germany (2005). His research interests include visualization, visual analytics, eye tracking, human-computer interaction, computer graphics, augmented and virtual reality, and special and general relativity. He is spokesperson of the DFG-funded Collaborative Research Center SFB/Transregio 161 “Quantitative Methods for Visual Computing” (www.sfbtrr161.de), which covers basic research on visualization, including multidimensional visualization.
  highlight: 0
  image: talk_weiskopf.jpeg

- date: May 18, 2023
  headline: >-
    Prof. Pat Hanrahan gave his Turing Award Lecture at SFU CS
  text: >-
    The recoding is available in this <a href="https://stream.sfu.ca/Media/Play/963b0b1088c3493f8fdc4291ed2f19c91d">link</a> (SFU accounnt is needed).</p><p>Here is some information about the lecture.</p><p><strong>Title:</strong></p><p>Shading Languages and the Emergence of Programmable Graphics Systems</p><p><strong>Abstract:</strong></p><p>A major challenge in using computer graphics for movies and games is to create a rendering system that can create realistic pictures of a virtual world.  The system must handle the variety and complexity of the shapes, materials, and lighting that combine to create what we see every day.  The images must also be free of artifacts, emulate cameras to create depth of field and motion blur, and compose seamlessly with photographs of live action.</p><p>Pixar&#39;s RenderMan was created for this purpose, and has been widely used in feature film production.  A key innovation in the system is to use a shading language to procedurally describe appearance.  Shading languages were subsequently extended to run in real-time on graphics processing units (GPUs), and now shading languages are widely used in game engines.  The final step was the realization that the GPU is a data-parallel computer, and the the shading language could be extended into a general-purpose data-parallel programming language.  This enabled a wide variety of applications in high performance computing, such as physical simulation and machine learning, to be run on GPUs.  Nowadays, GPUs are the fastest computers in the world. This talk will review the history of shading languages and GPUs, and discuss the broader implications for computing.</p><p><strong>Biography:</strong></p><p>Pat Hanrahan is the Canon Professor of Computer Science and Electrical Engineering in the Computer Graphics Laboratory at Stanford University.  His research focuses on rendering algorithms, graphics systems, and visualization.  Hanrahan received a Ph.D. in biophysics from the University of Wisconsin-Madison in 1985.  As a founding employee at Pixar Animation Studios in the 1980s, Hanrahan led the design of the RenderMan Interface Specification and the RenderMan Shading Language.  In 1989, he joined the faculty of Princeton University.  In 1995, he moved to Stanford University.  More recently, Hanrahan served as a co-founder and CTO of Tableau Software.  He has received three Academy Awards for Science and Technology, the SIGGRAPH Computer Graphics Achievement Award, the SIGGRAPH Stephen A. Coons Award, and the IEEE Visualization Career Award.  He is a member of the National Academy of Engineering and the American Academy of Arts and Sciences.  In 2019, he received the ACM A. M. Turing Award.
  highlight: 0
  image: talk_pat.jpg

- date: May 15, 2023
  headline: >-
    Prof. Tagliasacchi Co-Chairs 3DV 2024: Papers Invitation Open
  text: >-
    We are thrilled to share that Prof. <a href="https://taiya.github.io/">Andrea Tagliasacchi</a> will serve as co-chair for the International Conference on 3D Vision (3DV) 2024, alongside Prof. Siyu Tang from ETH and Federico Tombari from Google. Smaller conferences like 3DV play a vital role in fostering lasting networks within the research community, and 3DV 2024 promises to be an exciting opportunity for this.</p> <p>Furthermore, the call for papers for 3DV 2024 is now officially open. The conference will take place in the beautiful location of Davos, Switzerland, where World Economic Forum is annually held there. Researchers are encouraged to submit their papers by July 31st. For additional details, please visit the conference website at <strong><a href="https://3dvconf.github.io/2024/call-for-papers/">https://3dvconf.github.io/2024/call-for-papers/</a></strong>. Don&#39;t miss this incredible chance to share your research with the international community at 3DV 2024!
  highlight: 0
  image: taiya_3dv.png
  
- date: April 18, 2023
  headline: >-
    Talk by Karsten Kreis from NVIDIA
  text: >-
     Record link is <a href="https://stream.sfu.ca/Media/Channel/4b63668f57a046589246cf6c0c8eb2d05f">here</a></p><p><strong>Title</strong>: Diffusion Models: From Foundations to Image, Video and 3D Content Creation</p><p><strong>Abstract</strong>: Denoising diffusion-based generative models have led to multiple breakthroughs in deep generative learning. In this talk, I will provide an overview over recent works by the NVIDIA Toronto AI Lab on diffusion models and their applications for digital content creation. I will start with a short introduction of diffusion models and recapitulate their mathematical formulation. Then, I will briefly discuss our foundational works on diffusion models, which includes advanced diffusion processes for faster and smoother diffusion and denoising, techniques for more efficient model sampling, as well as latent space diffusion models, a flexible diffusion model framework that has been widely used in the literature. Moreover, I will discuss works that use diffusion models for image, video and 3D content creation. This includes large text-to-image models as well as recent work on high resolution video synthesis with latent diffusion models. I will also summarize some of our efforts on 3D generative modeling. This includes object-centric 3D synthesis by training diffusion models on geometric shape datasets or leveraging large-scale text-to-image diffusion models as priors for shape distillation, as well as full scene-level generation with hierarchical latent diffusion models.</p><p><strong>Bio</strong>: Karsten Kreis is a senior research scientist at NVIDIA’s Toronto AI Lab. Prior to joining NVIDIA, he worked on deep generative modeling at D-Wave Systems and co-founded Variational AI, a startup utilizing generative models for drug discovery. Before switching to deep learning, Karsten did his M.Sc. in quantum information theory at the Max Planck Institute for the Science of Light and his Ph.D. in computational and statistical physics at the Max Planck Institute for Polymer Research. Currently, Karsten’s research focuses on developing novel generative learning methods and on applying deep generative models on problems in areas such as computer vision, graphics and digital artistry, as well as in the natural sciences.
  highlight: 0
  image: talk_karsten.jpg
  
- date: April 1, 2023
  headline: >-
    Visual and Interactive Computing Institute (VINCI) is Founded
  text: >-
     We are delighted to announce the establishment of the Visual and Interactive Computing Institute (VINCI), co-directed by our esteemed Prof. Yasutaka Furukawa and Prof. Parmit Chilana. VINCI has been brought to life by 44 dedicated faculty members from 14 different departments and 7 distinct faculties within SFU. The primary objective of the institute is to bolster interdisciplinary research collaborations.
  highlight: 0
  image: yasu_vinci.png
  
- date: March 8, 2023
  headline: >-
    Prof. Furukawa Appointed as Program Chair for ICCV 2023
  text: >-
     We are thrilled to announce that Professor Yasutaka Furukawa, one of our esteemed professors, has been appointed as the Program Chair for the International Conference on Computer Vision (ICCV) 2023. The ICCV, a top-tier event in the field of computer vision, provides an excellent platform to exchange novel ideas and discuss the latest advancements. We invite you to learn more about the ICCV 2023 and Prof. Furukawa&#39;s role by visiting the official conference website at <a href="https://iccv2023.thecvf.com/">https://iccv2023.thecvf.com/</a>.
  highlight: 0
  image: yasu_iccv23.jpg

- date: February 27, 2023
  headline: >-
    Gruviers have 10 Accepted Papers at CVPR 2023
  text: >-
     Congratulations to all Gruviers who are publishing their work at CVPR 2023. Among the accepted papers, <a href="https://mobile-nerf.github.io/">MobileNeRF</a> <strong>is one of the 12 finalists for best paper award!</strong> Congrats for <a href="https://czq142857.github.io/">Zhiqin</a> and <a href="https://taiya.github.io/">Andrea</a> for their excellent work! </p><p>CVPR is the premier conference on computer vision and will be held in <strong>Vancourver</strong> this year. To learn more about the sample work that Gruvi will be presenting checkout our <a href="https://gruvi.ca/publications/">publication page</a> to get more information.
  highlight: 0
  image: gruvi_cvpr23.jpg
  
- date: Sep 14th, 2022
  headline: >-
    Gruviers Awarded with the inaugural CS Outstanding TA Award
  text: >-
    Congratulations to <a href="https://qiminchen.github.io/" target="_blank" >Qimin Chen</a>, <a href="http://www.sfu.ca/~wpintoli/" target="_blank" >Wallace Lira</a>, and <a href="https://www.linkedin.com/in/sonia-raychaudhuri" target="_blank" >Sonia Raychaudhuri</a>! They were selected as recipients for the inaugural <a href="https://www.sfu.ca/computing/current-students/graduate-students/cs-outstanding-teaching-assistant-award.html" target="_blank">CS Outstanding Teaching Assistent Award</a>. <br><br>This award recognizes exceptional TAs in the School of Computing Science for their outstanding work in teaching and performing other TA duties. The recipients  will be acknowledged at the October 20th school meeting.
  highlight: 0
  image: sfu_cs.png

- date: Aug 7th, 2022
  headline: >-
    The First SFU Visual Computing Workshop was held in Vancouver
  text: >-
    <a href="https://gruvi.cs.sfu.ca/vc_workshop_2022" target="_blank">The first SFU Visual Computing Workshop</a> was held on Aug 7th at the Morris J. Wosk Centre for Dialogue in Vancouver, Canada. This workshop preceded SIGGRAPH 2022, so it was conceived as a warmup for the researchers that were gathering for the most prestigious Computer Graphics conference.<br><br>The workshop was organized by <a href="https://danielcohenor.com/" target="_blank">Daniel Cohen-Or</a>, <a href="https://www.cs.sfu.ca/~haoz/" target="_blank">Richard Zhang</a>, <a href="https://www.sfu.ca/~amahdavi/Home.html" target="_blank">Ali Mahdavi-Amiri</a>, and <a href="https://www.sfu.ca/~wpintoli" target="_blank">Wallace Lira</a>. The event featured some of the leading voices and rising stars in the Computer Graphics community, where the panelists included <a href="https://angelxuanchang.github.io/" target="_blank">Angel Chang</a>, <a href="https://www.cs.sfu.ca/~furukawa/" target="_blank">Yasutaka Furukawa</a>, <a href="https://people.cs.uchicago.edu/~ranahanocka/" target="_blank">Rana Hanocka</a>, <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a>, and <a href="https://faculty.runi.ac.il/arik/site/index.asp" target="_blank">Ariel Shamir</a>. Further, the featured speakers were <a href="https://www.cs.tau.ac.il/~amberman/" target="_blank">Amit Bermano</a>, <a href="https://xbpeng.github.io/" target="_blank">Jason Peng</a>, <a href="https://people.cs.uchicago.edu/~ranahanocka/" target="_blank">Rana Hanocka</a>, <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a>, <a href="http://www.cs.unc.edu/~lin/" target="_blank">Ming Lin</a>, <a href="https://yifita.netlify.app/" target="_blank">Yifan Wang</a>, and <a href="https://taiya.github.io/" target="_blank">Andrea Tagliasacchi</a>.
  highlight: 0
  image: VCW2022.jpg

- date: May 5th, 2022
  headline: >-
    Talk by Yotam Nitzan from Tel Aviv University
  text: >-
    <strong>Title:</strong> MyStyle: A Personalized Generative Prior</p> <p style="text-align: justify;"><strong>Time:</strong> Friday, May 6th at 11:00 AM PST</p> <p style="text-align: justify;"><strong>Abstract:&nbsp;</strong>Deep generative models have proved to be successful for many image-to-image applications. Such models hallucinate information based on their large and diverse training datasets. Therefore, when enhancing or editing a portrait image, the model produces a generic and plausible output, but often it isn't the person who actually appears in the image. In this talk, I'll present our latest work, MyStyle - which introduces the notion of a personalized generative model. Trained on ~100 images of the same individual, MyStyle learns a personalized prior, custom to their unique appearance. This prior is then leveraged to solve ill-posed image enhancement and editing tasks - such as super-resolution, inpainting and changing the head pose.</p> <p style="text-align: center;"><a href="https://arxiv.org/pdf/2203.17272.pdf">MyStyle Paper</a></p> <p style="text-align: center;"><a href="https://yotamnitzan.github.io/">Yotam Nitzan's personal webpage</a>
  highlight: 0
  image: yotam_nitzan.jpg

- date: May 4th, 2022
  headline: >-
    Gruviers Receive Awards at Graphics Interface 2022.
  text: >-
    Congratulations to Hao (Richard) Zhang and Manolis Savva for receiving awards at the Graphics Interface 2022. Richard has received the 2022 CHCCS/SCDHM Achievement Award of the Canadian Human-Computer Communications Society. Richard has had numerous high-impact contributions to computer graphics including geometric modeling, shape analysis, geometric deep learning, and computational design and fabrication. Manolis has received the 2022 Early Career Researcher Award. Manolis has established himself as a central figure in topics at the intersection of computer graphics, 3D sensing, and machine learning. To learn more about the research contributions of Richard and Manolis please checkout <a href="https://graphicsinterface.org/awards/chccs-scdhm-achievement/hao-zhang/">here</a> and <a href="https://graphicsinterface.org/awards/early-career-researcher-award/manolis-savva/">here</a>.
  highlight: 0
  image: graphics_interface_2022.png

- date: March 2nd, 2022
  headline: >-
    Gruviers have 12 Accepted Papers at CVPR 2022.
  text: >-
    Congratulations to all Gruviers who are publishing their work at CVPR 2022. CVPR is the premier conference on computer vision and will be held in New Orleans this year. To learn more about the sample work that Gruvi will be presenting checkout <a href="https://www.cs.sfu.ca/~haoz/papers.html">here</a> and <a href="https://www.cs.sfu.ca/~furukawa/">here</a>.
  highlight: 0
  image: CFI_funding.png

- date: Dec 20th, 2021
  headline: >-
    We Wish Everyone a Very Happy New Year.
  text: >-
    We wrap-up the year 2021 with great achievements and look forward to the new year ahead of us. In 2021, Gruviers were able to publish their work at many 1st tier conferences: CVPR (12 papers), SIGGRAPH and SIGGRAPH Asia (4 papers),  ICCV (4 papers), Eurographics and Neurips. Congratulations to all Gruviers for their hard work.
  highlight: 0
  image: graphics_interface_2021.png

- date: Oct 19, 2021
  headline: >-
    Talk by Or Perel from Tel Aviv University
  text: >-
    <strong>Title:</strong> SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization</p> <p style="text-align: justify;"><strong>Time:</strong> Wednesday, Nov 3, 1:30 PM </p> <p style="text-align: justify;"><strong>Abstract:&nbsp;</strong>Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.</p> <p style="text-align: center;"><a href="https://arxiv.org/abs/2104.09125">SAPE Paper</a></p> <p style="text-align: center;"><a href="https://orperel.github.io/">Or Perel's personal webpage</a>
  highlight: 0
  image: OrPerel.jpg

- date: September 24th, 2021
  headline: >-
    Zhiqin Chen Recieves Google PhD Fellowship.
  text: >-
    Congratulations to Zhiqin Chen for receiving a PhD Fellowship from Google. The Google PhD Fellowship Program was created to recognize outstanding graduate students doing exceptional and innovative research in areas relevant to computer science and related fields. Fellowships support promising PhD candidates of all backgrounds who seek to influence the future of technology. To learn more about Zhiqin's research please visit <a href="https://czq142857.github.io/">here</a>.
  highlight: 0
  image: zhiqin_google_fellowship.png

- date: August 31st, 2021
  headline: >-
    Akshay Gadi Patil Receives ICCV 2021 Outstanding Reviewer Award.
  text: >-
    Congratulations to Akshay for receiving the Outstanding Reviewer Award at ICCV 2021. To learn more about Akshay's work please visit <a href="https://www.sfu.ca/~agadipat/">here</a>.   
  highlight: 0
  image: akshay_iccv_reviewer_award.jpg

- date: Aug 5, 2021
  headline: >-
    Talk by Tel Aviv students Or Patashnik and Yuval Alaluf
  text: >-
    <strong>Title:</strong> Recent Advancements in StyleGAN Inversion</p> <p style="text-align: justify;"><strong>Time:</strong> Wednesday, August 11, 10AM </p> <p style="text-align: justify;"><strong>Abstract:&nbsp;</strong>StyleGAN has recently been established as the state-of-the-art unconditional generator, synthesizing images of phenomenal realism and fidelity. With its rich semantic space, many works have attempted to understand and control StyleGAN&rsquo;s latent representations with the goal of performing image manipulations. To perform manipulations on real images, however, one must learn to &ldquo;invert&rdquo; the GAN and encode a given image into StyleGAN&rsquo;s latent space, which remains an open challenge. In this talk, we will discuss recent techniques and advancements in GAN Inversion and explore their importance for real image editing applications. In addition, going beyond the inversion task, we demonstrate how StyleGAN can be used for performing a wide range of image-to-image translation tasks.</p> <p style="text-align: justify;"><strong>Bios:&nbsp;</strong>Or and Yuval are both graduate students studying Computer Science at Tel-Aviv University under the supervision of Professor Daniel Cohen-Or and have collaborated on numerous works in the past year. Their main interests lie in the field of Computer Vision with recent work centered around image generation and manipulation.</p> <p style="text-align: center;"><a href="https://orpatashnik.github.io/">https://orpatashnik.github.io/</a></p> <p style="text-align: center;"><a href="https://yuval-alaluf.github.io/">https://yuval-alaluf.github.io/</a>
  highlight: 0
  image: telaviv_talk.JPG

- date: July 23rd, 2021
  headline: >-
    Nelson Nauata Receives Borealis AI 2021 Fellowship.
  text: >-
    Congratulations to Nelson Nauata for receiving a Borealis AI 2021 Fellowship. Only ten students from the top academic institutions in Canada received fellowships this year, all with the goal of contributing to the advancement of artificial intelligence and machine learning. To learn more about the award and Nelson’s contributions check out <a href="https://www.sfu.ca/computing/newsandevents/2021/computing-science-phd-student-named-borealis-ai-2021-fellow.html">here</a>.
  highlight: 0
  image: nelson_borealis_fellowship.png

- date: Jun 19, 2021
  headline: >-
    GrUVi making waves at CVPR 2021
  text: >-
    <span style="font-weight: 400;">CVPR, the premier conference on computer vision, will be held virtually this year (June 19-25). GrUVi lab will once again have an incredible showing at CVPR, with 16 technical papers, 2 invited talks, 4 co-organized workshops, and 1 hosted challenge!</span></p> <p><br /><br /></p> <p><strong>Workshop co-organization</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">GrUViers will co-organize 4 workshops featuring state-of-the-art research and will host one challenge:</span></p> <p>&nbsp;</p> <ul align="justify"> <li style="font-weight: 400;"><a href="https://sites.google.com/view/cvpr2021-3d-vision-robotics"><span style="font-weight: 400;">3D Vision and Robotics</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="http://www.scan-net.org/cvpr2021workshop/"><span style="font-weight: 400;">ScanNet Indoor Scene Understanding Challenge</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="https://learn3dg.github.io/"><span style="font-weight: 400;">Learning to Generate 3D Shapes and Scenes</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="https://language3dscenes.github.io/"><span style="font-weight: 400;">Language for 3D Scenes</span></a><span style="font-weight: 400;"> </span></li><li style="font-weight: 400;"> <a href="http://multion-challenge.cs.sfu.ca/"><span style="font-weight: 400;">Multi-Object Navigation Challenge at the Embodied-AI workshop</span></a><span style="font-weight: 400;"> </span></li> </ul> <p><br /><br /></p> <p><strong>Invited workshop talks</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">Yasutaka Furukawa will give a talk at the <a href="https://cv4aec.github.io/">&ldquo;Computer Vision in the Built Environment&rdquo;</a> workshop, while Manolis Savva will give a talk at the <a href="https://sites.google.com/view/cvpr2021-3d-vision-robotics">&ldquo;3D Vision and Robotics&rdquo;</a> workshop.</span></p> <p><br /><br /></p> <p><strong>Technical Papers and GrUVi authors</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">Congratulations to all authors for the accepted papers! The full list of papers featured on CVPR 2021 can be accessed </span><a href="https://gruvi.cs.sfu.ca/publications/"><span style="font-weight: 400;">here</span></a>. 
  highlight: 0
  image: CVPR2021.jpeg

- date: April 23, 2021
  headline: >-
    Talk by Yang Wang
  text: >-
    <b>Talk title:</b> Self&ndash;Adaptive Visual Learning  <br><br><b>Time:</b> April 23, from 3:30 to 4:30PM (PST)  <br><br><b>Abstract:</b> There have been significant advances in computer vision in the past few years. Despite the success, current computer vision systems are still hard to use or deploy in many real-world scenarios. In particular, current computer vision systems usually learn a generic model. But in real world applications, a single generic model is often not powerful enough to handle the diverse scenarios. In this talk, I will introduce some of our recent work on self-adaptive visual learning. Instead of learning and deploying one generic model, our goal is to learn a model that can effectively adapt itself to different environments during testing. I will present applications from several computer visions, such as crowd counting, anomaly detection, personalized highlight detection, etc. <br><br><b>Bio:</b> <a target="_blank" href="https://www.cs.umanitoba.ca/~ywang/">Yang Wang</a> is an associate professor in the Department of Computer Science, <a target="_blank" href="https://sci.umanitoba.ca/cs/">University of Manitoba</a>. He is currently on leave and working as the Chief Scientist in Computer Vision, Noah&lsquo;s Ark Lab, Huawei Technologies Canada. He did his PhD from Simon Fraser University, MSc from University of Alberta, and BEng from Harbin Institute of Technology. Before joining UManitoba, he worked as a NSERC postdoc at the University of Illinois at Urbana-Champaign. His research focuses on computer vision and machine learning. He received the 2017 Falconer Emerging Researcher Rh Award in applied science at the University of Manitoba. He currently holds the inaugural Faculty of Science research chair in fundamental science at UManitoba.
  highlight: 0
  image: Yang_Wang.jpeg
  
- date: April 16, 2021
  headline: >-
    Talk by Unnat Jain
  text: >-
    <b>Talk title:</b> AI Agents that can Collaborate and Communicate in Virtual Visual Worlds  <br><br><b>Time:</b> April 16, from 3:30 to 4:30PM (PST)  <br><br><b>Abstract:</b> The past decade in artificial intelligence, particularly computer vision, has been about hammering passively collected datasets with massive deep learning models. As the race to boost metrics on them is saturating, researchers like me are working on visual or embodied AI agents that draw inspiration from how toddlers acquire intelligence, i.e., by exploring, interacting, and navigating in their environments.<br>Particularly, I am excited to study how visual embodied agents can learn key skills of social intelligence – collaboration and communication. In this talk, I’ll discuss how we are building AI Agents that can collaborate and communicate in virtual visual worlds. Moreover, I’ll discuss how simplistic gridworlds and visual worlds can be connected with a ‘GridToPix’ methodology. The relevant papers can be found on my <a target="_blank" href="https://unnat.github.io/">webpage</a>. <br><br><b>Bio:</b> <a target="_blank" href="https://unnat.github.io/">Unnat Jain</a> is a Ph.D. student in Computer Science at <a target="_blank" href="https://cs.illinois.edu/">UIUC</a> working with Alex Schwing and Svetlana Lazebnik. His research is focused on developing collaborative and communicative visual agents. He has worked as a research intern at DeepMind, Facebook AI Research, and Allen Institute for AI. He has won many awards including the Director’s Gold Medal (IIT Kanpur), Cadence Gold Medal for best engineering thesis (IIT Kanpur), David J. Kuck Outstanding MS Thesis Award (UIUC), Siebel Scholars, and was a finalist of Qualcomm Innovation Fellowship 2019.
  highlight: 0
  image: Unnat_Jain.jpeg

- date: April 9, 2021
  headline: >-
    Talk by Kwang Moo Yi
  text: >-
    <b>Talk title:</b> Towards Machines that Understand Geometry  <br><br><b>Time:</b> April 9, from 3:30 to 4:30PM (PST)  <br><br><b>Abstract:</b> Understanding the how the worlds looks like and interacting with the environment is a core ability of an intelligent being. Naturally, it has been a long-lasting research topic in Computer Vision. The capacity of machines in figuring out surrounding geometry has increased dramatically over the last decade, so much so that self-driving cars and autonomous drones are not a distant future. However, the “last mile” has shown to be more difficult than anticipated, delaying the arrival of these machines. Machine learning, like many other applications, have very recently started to help in this regard, again creating a leap from what it could do just a couple years back.<br>In this talk, I will introduce our journey towards machines that understand geometry. I will show that by combining our knowledge about the physical world with machine learning, we can achieve much more than a black-box solution. Specifically, I will show how we use non-differentiable components within deep networks and still train as a whole; how we constrain the network to follow physics via deep network architectures and formulations; how we can tailor architectures for solving image correspondence problems; and how we simplify the role of machine learning by turning the problem into hypothesis testing. <br><br><b>Bio:</b> <a target="_blank" href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a> is an assistant professor in the Department of Computer Science at the <a target="_blank" href="https://www.cs.ubc.ca/">University of British Columbia (UBC)</a>, and a member of the Computer Vision Lab, CAIDA, and ICICS at UBC. Before, he was at the University of Victoria as an assistant professor, where he is currently an adjunct professor. Prior to being a professor, he worked as a post-doctoral researcher at the Computer Vision Lab in École Polytechnique Fédérale de Lausanne (EPFL, Switzerland), working with Prof. Pascal Fua and Prof. Vincent Lepetit. He received his Ph.D. from Seoul National University under the supervision of Prof. Jin Young Choi. He also received his B.Sc. from the same University. He serves as area chair for top Computer Vision conferences (CVPR, ICCV, and ECCV), as well as AAAI. He is part of the organizing committee for CVPR 2023.
  highlight: 0
  image: Kwang_Moo_Yi.jpeg
  
- date: April 10, 2021
  headline: >-
    A. Chang is SGP21's Conference Chair
  text: >-
    Gruvi is glad to announce that Angel Chang will serve as Conference Chair in the SGP 2021. The Symposium on Geometry Processing (SGP) is the flagship conference in geometry processing. To learn more about the conference please visit <a href="https://sgp2021.github.io/organization">here</a>.
  highlight: 0
  image: angel.png
  
- date: April 2, 2021
  headline: >-
    Talk by Kaichun Mo   
  text: >-
    <b>Talk title:</b> Learning 3D Shape Actionable Information from Simulated Interaction  <br><br><b>Time:</b> April 2, from 3:30 to 4:30PM (PST)  <br><br><b>Abstract:</b> We humans accomplish everyday tasks by interacting with a wide range of 3D objects, with diverse geometry, rich semantics, and complicated structures. One fundamental goal of computational visual perception is to equip intelligent agents with similar capabilities to understand how to meaningfully interact with the 3D environment. While great advances have been achieved in both fields of 3D vision (e.g. object detection, pose estimation, shape reconstruction) and robotic manipulation (e.g. planning, control), we ask the question -- what are good visual representations of 3D shapes for various downstream robotic manipulation tasks? In this talk, I will introduce my recent research taking one step along this direction that learns 3D shape actionable information via large-scale annotation-free learning from simulated interaction. More specifically, I will present two works (i.e. Where2Act, O2O-Afford) that learn 3D shape visual affordance post-conditioned on various primitive manipulation policies, using the large-scale ShapeNet/PartNet dataset and SAPIEN physical simulation environment.<br><br><b>Bio:</b> <a target="_blank" href="https://cs.stanford.edu/~kaichun/">Kaichun Mo</a> is a fifth-year Ph.D. Student in Computer Science at Stanford University, advised by Prof. Leonidas Guibas. Before that, he received his BS.E. degree from the ACM Honored Class at Shanghai Jiao Tong University. His research interests focus on understanding 3D shape structure and semantics for various applications in 3D vision, graphics, and robotic manipulation. He has interned at Adobe Research, Autodesk Research (AI Lab), and Facebook AI Research. He has published papers at CVPR, ECCV, NeurIPS, ICLR, Siggraph Asia, ToG, AAAI.
  highlight: 0
  image: Kaichun_Mo.jpeg
  
- date: March 19, 2021
  headline: >-
    Talk by Animesh Garg   
  text: >-
    <b>Talk title:</b> Building Blocks of Generalizable Autonomy in Robot Manipulation  <br><br><b>Time:</b> March 19, from 3:30 to 4:30PM (PST)  <br><br><b>Abstract:</b> My approach to Generalizable Autonomy posits that interactive learning across families of tasks is essential for discovering efficient representation and inference mechanisms. Arguably, a cognitive concept or a dexterous skill should be reusable across task instances to avoid constant relearning. It is insufficient to learn to “open a door”, and then have to re-learn it for a new door, or even windows & cupboards. Thus, I focus on three key questions-  (1) Representational biases for embodied reasoning, (2) Causal Inference in abstract sequential domains,  and (3) Interactive Policy Learning under uncertainty. In this talk, I will first through example lay bare the need for structured biases in modern RL algorithms in the context of robotics. This will span state, actions, learning mechanisms, and network architectures. Secondly,  we will talk about the discovery of latent causal structure in dynamics for planning. Finally, I will demonstrate how large-scale data generation combined with insights from structure learning can enable sample efficient algorithms for practical systems. In this talk, I will focus mainly on manipulation, but my work has been applied to surgical robotics and legged locomotion as well.<br><br><b>Bio:</b> <a target="_blank" href="https://animesh.garg.tech/">Animesh Garg</a> is a CIFAR Chair Assistant Professor of <a target="_blank" href="https://web.cs.toronto.edu/">Computer Science</a> at the University of Toronto and a Faculty Member at the <a target="_blank" href="https://vectorinstitute.ai/">Vector</a> Institute where he leads the Toronto <a target="_blank" href="http://pair.toronto.edu/">People, AI, and Robotics (PAIR)</a> research group. Animesh is affiliated with Mechanical and Industrial Engineering (courtesy) and <a target="_blank" href="https://robotics.utoronto.ca/">UofT Robotics Institute</a>. Animesh also spends time as a Senior Researcher at <a target="_blank" href="https://www.nvidia.com/en-us/research/">Nvidia Research</a> in ML for Robotics. Prior to this, Animesh earned a Ph.D. from <a target="_blank" href="https://bair.berkeley.edu/">UC Berkeley</a>, and was a postdoc at the <a target="_blank" href="http://ai.stanford.edu/">Stanford AI Lab</a>. His research focuses on machine learning algorithms for perception and control in robotics. His work aims to build Generalizable Autonomy in robotics which involves a confluence of representations and algorithms for reinforcement learning, control, and perception. His work has received multiple Best Paper Awards (ICRA, IROS, Hamlyn Symposium, Neurips Workshop, ICML Workshop) and has been covered in the press (New York Times, Nature, BBC).
  highlight: 0
  image: animesh-garg.jpeg

- date: December 3, 2020
  headline: >-
    Talk by Hadar Averbuch-Elor   
  text: >-
    <b>Talk title:</b> Generation by Decomposition  <br><br><b>Time:</b> December 16, from 1:30 to 2:30PM (PST)  <br><br><b>Abstract:</b> Deep learning has revolutionized our ability to generate novel images and 3D shapes. Neural networks are typically trained to map a high-dimensional latent code to full realistic samples. In this talk, I will present two recent works focusing on generation of handwritten text and 3D shapes. In these works, we take a different approach and generate image and shape samples using a more granular part-based decomposition, demonstrating that the whole is not necessarily “greater than the sum of its parts”. I will also discuss how our generation by decomposition approach allows for a semantic manipulation of 3D shapes and improved handwritten text recognition performance.<br><br><b>Bio:</b> <a target="_blank" href="http://www.cs.cornell.edu/~hadarelor/">Hadar Averbuch-Elor</a> is a postdoctoral researcher at Cornell-Tech working with Prof. Noah Snavely. Her research interests lie in the intersection of computer graphics and computer vision. Currently, her research focuses on understanding and manipulating visual concepts by combining pixels with more structured modalities, including natural language and 3D geometry. She completed her PhD in Electrical Engineering at Tel-Aviv University in Israel where she was advised by Prof. Daniel Cohen-Or. She also held research positions at Facebook and Amazon AI. Hadar has received several awards including the Zuckerman Postdoctoral Scholar Fellowship and the Tel Aviv University President Award for Women.
  highlight: 0
  image: hadar.jpg

- date: December 2, 2020
  headline: >-
    Graphics Interface 2021
  text: >-
    Gruvi is glad to announce that Ali Mahdavi-Amiri and Manolis Savva will serve as general and program co-chair at the Graphics Interface 2021. Graphics Interface is the only conference for computer graphics and human computer interaction. To learn more about the conference please visit <a href="http://graphicsinterface.org/conference/2021/">here</a>.
  highlight: 0
  image: graphics_interface_2021.png

- date: October 10, 2020
  headline: >-
    Keynote and award at ChinaGraph'20
  text: >-
    Richard Zhang will deliver <a href="https://chinagraph2020.xmu.edu.cn/keynotes.html#keynote3" target="_blank">one of the keynote talks</a> (virtually) at ChinaGraph 2020 held in Xiamen, China, on October 23. ChinaGraph is a bi-annual conference on computer graphics in China, the most important gathering of graphics researchers, students, and industries in the country. In another news, &nbsp;<a href="https://pages.tmall.com/wow/cab/tianchi/promotion/alibaba-3d-scene-dataset" target="_blank" >3D-FRONT</a>, a large-scale 3D indoor scene dataset&nbsp;published earlier this year by Richard and colleagues from Alibaba and the Chinese Academy of Sciences, has won the inaugural ChinaGraph Best Dataset Award.
  highlight: 0
  image: chinagraph20.png

- date: September 25, 2020
  headline: >-
    Gruviers Receive Funding from CFI
  text: >-
    Congratulations to Angel Xuan Chang, Yasutaka Furukawa and Manolis Savva for receiving fundings form <b>Canada Foundation for Innovation (CFI)</b>. This funding will allow our researchers to take their transformative discoveries to the next level. More information about the funding and the projects can be found here <a href="http://www.sfu.ca/sfunews/stories/2020/09/17-sfu-research-projects-ready-to-roll-with--2-9m-from-canada-fo.html">here</a>.
  highlight: 0
  image: CFI_funding.png

- date: September 23, 2020
  headline: >-
    Talk by He Wang
  text: >-
    <b>Title:</b> Category-Level Object Perception for Physical Interaction<br><b>Time:</b> 1:30 - 2:30, Wednesday, September 23<br><br><b>Abstract:</b> Deep neural networks have shown great success both in semantic perception tasks, e.g. object recognition and semantic segmentation, and in end-to-end perception for reinforcement learning and robotic tasks. However, it is still unclear how to bridge these two perception paradigms to gain a deep semantic and interaction-driven understanding of physical interaction.<br><br>In this talk, I will focus on how to explore categorical actionable information for the sake of perceiving and understanding physical interactions. First, I will show that learning high-level semantic actionable information, e.g. object state, can help with action planning. Second, I will introduce the problem of estimating category-level 6D pose and 3D size for rigid objects. This category pose can be seen as low-level actionable information and can benefit object manipulation tasks. Lastly, I will present my recent works on curating an articulated object dataset and estimating category-level articulated object pose.<br><br><b>Bio:</b> He Wang is a fifth-year PhD student at Stanford University under the supervision of Prof. Leonidas Guibas. His research interests span across computer vision, geometric computing, and robotics.  In his PhD he contributed to generative modeling of human object interactions, opened up a new direction in estimating category-level pose and size for rigid and articulated objects. He receives Eurographics 2019 best paper honorable mention award and three of his works are accepted as CVPR oral presentations. Prior to his PhD he obtained his bachelor in Microelectronics from Tsinghua University.
  highlight: 0
  image: he_profile.png

- date: September 13, 2020
  headline: >-
    New SFU CS Adjunct Prof
  text: >-
    Kevin (Kai) Xu, a former gruvier from 2008 to 2010, when he worked as a visiting Ph.D. student under the supervision of Richard Zhang, is now resuming his affiliation with GrUVi after being appointed as an Adjunct Professor at SFU CS for a three-year term. Welcome back to GrUVi, Kevin!</p><p style="padding: 0 15px; text-align: justify;">Check <a href="https://kevinkaixu.net/" target="_blank">his website</a> for more information about his recent works in computer graphics and computer vision!
  highlight: 0
  image: kevinxu.jpg

- date: July 8, 2020
  headline: >-
    Gruviers Won the SGP Dataset Award
  text: >-
    Congratulations to Angel Xuan Chang and Manolis Savva, whose work <b>ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</b> won the 2020 SGP dataset award. ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations. More information can be found in the paper <a href="http://www.scan-net.org/">here</a>.
  highlight: 0
  image: angel_scannet.png

- date: Jun 23, 2020
  headline: >-
    Gruviers Receive CVPR Awards
  text: >-
    Visual computing researchers from SFU received multiple awards at the annual Conference on Computer Vision and Pattern Recognition (CVPR) this past week. CVPR is the premier conference in computer vision with the highest impact factor among all conferences in computer science and was held virtually for the first time this year from June 14-19. Computing science professor Greg Mori served as one of four program chairs at the conference. <br><br> Zhiqin Chen and Richard Zhang, together with GrUVi alumnus Andrea Tagliasacchi, won the Best Student Paper Award with "BSP-Net: Generating Compact Meshes via Binary Space Partitioning" introduces a deep neural network which applies a classical graphics technique to learn compact shape representations.  <br><br> Yasutaka Furukawa won the PAMI Longuet-Higgins Prize for his CVPR 2007 milestone paper on “multi-view stereo reconstruction”, which has been cited more than 3,000 times! <br><br> Also, Akshay Gadi Patil won the Best Paper Award at the CVPR Workshop on Text and Documents in the Deep Learning Era for his work "READ: Recursive Autoencoders for Document Layout Generation". <br><br> Read more <a href="https://www.sfu.ca/computing/newsandevents/2020/visual-computing-researchers-cvpr.html">here</a>. Well done, gruviers!
  highlight: 0
  image: CVPR_GRUVI.png

- date: Jun 09, 2020
  headline: >-
    GrUVi making waves at CVPR 2020
  text: >-
    <span style="font-weight: 400;">CVPR, the premier conference on computer vision, will be held virtually next week (June 16-20). SFU Computing Science professor Greg Mori is front-n-center as a Program Chair of the major event! GrUVi lab will have an incredible showing at CVPR, with 11 technical papers (5 orals), 3 invited talks, and 4 co-organized workshops!</span></p> <p><br /><br /></p> <p><strong>Workshop co-organization</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">GrUViers will co-organize 4 workshops featuring state-of-the-art research:</span></p> <p>&nbsp;</p> <ul align="justify"> <li style="font-weight: 400;"><a href="https://learn3dgen.github.io/"><span style="font-weight: 400;">Learning 3D Generative Models</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="http://www.scan-net.org/cvpr2020workshop/"><span style="font-weight: 400;">ScanNet Indoor Scene Understanding Challenge</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="https://embodied-ai.org/"><span style="font-weight: 400;">Embodied-AI Workshop</span></a><span style="font-weight: 400;"></span></li> <li style="font-weight: 400;"><a href="https://sites.google.com/view/geometry-learning-foundation/"><span style="font-weight: 400;">Deep Learning Foundations of Geometric Shape Modeling and Reconstruction</span></a><span style="font-weight: 400;"> </span></li> </ul> <p><br /><br /></p> <p><strong>Invited workshop talks</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">Yasutaka Furukawa will give a talk at the aforementioned &ldquo;ScanNet Indoor Scene Understanding Challenge&rdquo; as well as the &ldquo;</span><a href="https://scene-understanding.com/"><span style="font-weight: 400;">3D Scene Understanding for Vision, Graphics, and Robotics</span></a><span style="font-weight: 400;">&rdquo; workshop </span><span style="font-weight: 400;">, while </span><span style="font-weight: 400;">Manolis Savva will participate as an invited speaker at the &ldquo;</span><a href="https://fadetrcv.github.io/"><span style="font-weight: 400;">Fair, Data, Efficient and Trusted Computer Vision</span></a><span style="font-weight: 400;">&rdquo; workshop.</span></p> <p><br /><br /></p> <p><strong>Technical Papers and GrUVi authors</strong></p> <p>&nbsp;</p> <p align="justify"><span style="font-weight: 400;">Congratulations to all authors, especially to Dr. Ping Tan, who got five accepted papers! The full list of papers featured on CVPR 2020 can be accessed </span><a href="http://cvpr2020.thecvf.com/program/main-conference"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;">. In particular, GrUVi papers covered different topics:</span></p> <p>&nbsp;</p> <ul align="justify"> <li style="font-weight: 400;"><span style="font-weight: 400;">3D From a Single Image and Shape-From-X</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">BSP-Net: Generating Compact Meshes via Binary Space Partitioning (<strong>Zhiqin Chen</strong>, Andrea Tagliasacchi, <strong>Hao Zhang</strong> - </span><strong>oral</strong><span style="font-weight: 400;">)</span></li> <li style="font-weight: 400;"><span style="font-weight: 400;">Bundle Pooling for Polygonal Architecture Segmentation Problem (Huayi Zeng, <strong>Kevin Joseph</strong>, Adam Vest, <strong>Yasutaka Furukawa</strong>)</span></li> <li style="font-weight: 400;"><span style="font-weight: 400;">Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching (Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, <strong>Feitong Tan</strong>, <strong>Ping Tan</strong> - </span><strong>oral</strong><span style="font-weight: 400;">)</span></li> <li style="font-weight: 400;"><span style="font-weight: 400;">Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction (<strong>Nelson Nauata</strong>, <strong>Fuyang Zhang</strong>, <strong>Yasutaka Furukawa</strong>)</span></li> <li style="font-weight: 400;"><span style="font-weight: 400;">PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes (Rundi Wu, Yixin Zhuang, Kai Xu, <strong>Hao Zhang</strong>, Baoquan Chen)</span></li> <li style="font-weight: 400;"><span style="font-weight: 400;">Self-Supervised Human Depth Estimation From Monocular Videos (<strong>Feitong Tan</strong>, Hao Zhu, Zhaopeng Cui, Siyu Zhu, Marc Pollefeys, <strong>Ping Tan</strong>)</span></li> </ul> <li style="font-weight: 400;"><span style="font-weight: 400;">3D From Multiview and Sensors; Computational Photography; Efficient Training and Inference Methods for Networks</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">End-to-End Learning Local Multi-View Descriptors for 3D Point Clouds (Lei Li, Siyu Zhu, Hongbo Fu, <strong>Ping Tan</strong>, Chiew-Lan Tai)</span></li> </ul> <li style="font-weight: 400;"><span style="font-weight: 400;">3D From Multiview and Sensors; Face, Gesture, and Body Pose; Image and Video Synthesis</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">Deep Facial Non-Rigid Multi-View Stereo (<strong>Ziqian Bai</strong>, Zhaopeng Cui, <strong>Jamal Ahmed Rahim</strong>, Xiaoming Liu, <strong>Ping Tan</strong>)</span></li> </ul> <li style="font-weight: 400;"><span style="font-weight: 400;">Motion and Tracking</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">LSM: Learning Subspace Minimization for Low-Level Vision (<strong>Chengzhou Tang</strong>, Lu Yuan, <strong>Ping Tan</strong> - </span><strong>oral</strong><span style="font-weight: 400;">)</span></li> </ul> <li style="font-weight: 400;"><span style="font-weight: 400;">Segmentation, Grouping, and Shape</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">AdaCoSeg: Adaptive Shape Co-Segmentation With Group Consistency Loss (<strong>Chenyang Zhu</strong>, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas J. Guibas, <strong>Hao Zhang</strong> - </span><strong>oral</strong><span style="font-weight: 400;">)</span></li> </ul> <li style="font-weight: 400;"><span style="font-weight: 400;">Vision for Robotics and Autonomous Vehicles</span></li> <ul> <li style="font-weight: 400;"><span style="font-weight: 400;">SAPIEN: A SimulAted Part-Based Interactive ENvironment (Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, <strong>Hanxiao Jiang</strong>, Yifu Yuan, He Wang, Li Yi, <strong>Angel X. Chang</strong>, Leonidas J. Guibas, Hao Su - </span><strong>oral</strong><span style="font-weight: 400;">)</span></li> </ul> </ul> <p>
  highlight: 0
  image: CVPR_GRUVI.png
  
- date: Jun 07, 2020
  headline: >-
    GrUVi tackles COVID-19 using AI
  text: >-
    SFU researchers helped to develop an AI system capable of assisting resident and less experienced doctors look over a data set and make a quick diagnosis before a senior doctor can step in. This is accoding to Yağız Aksoy, a gruvier and also a member of the team that proposed the diagnosis tool, which is currently in the validation phase at St. Paul’s Hospital in Vancouver, Canada. Read more about it <a href="http://www.sfu.ca/sfunews/stories/2020/06/sfu-researchers-help-develop-ai-tool-for-speedy-covid-19-diagnos.html">here</a>. Thank you for your hard work, Dr. Aksoy!
  highlight: 0
  image: covid.jpg

- date: Oct 24, 2019
  headline: >-
    LOGAN - SIG Asia Press Release
  text: >-
    Congratulations to Kangxue, whose work <b>LOGAN: Unpaired Shape Transform in Latent Overcomplete Space</b> will be featured for press release at SIGGRAPH Asia 2019!<br><br>LOGAN is able to learn what shape features to preserve during shape translation, either local or non-local, whether content or style, depending solely on the input domains for training, which was an improvement over the state of the art.<br><br>LOGAN is an abbreviation for Latent Overcomplete GAN. Richard Zhang, who is also the last author, created the fun acronym.<br><br>
  highlight: 0
  image: logan.jpg

- date: Jul 23, 2019
  headline: >-
    IGS'19 Organized by GrUVi
  text: >-
    The International Geometry Summit (IGS) 2019, organized by  Richard Zhang and Ali Mahdavi-Amiri, took place from 17th June to the 21th June 2019 at SFU Harbour Center in Vancouver, Canada. IGS 2019 included four conferences: the Symposium on Solid and Physical Modelling, Shape Modeling International, SIAM Conference on Computational Geometric Design, and International Conference on Geometric Modelling and Processing. IGS 2019 featured state of the art researches in its technical paper sessions along with 12 keynote speeches from prominent researchers in the area of geometry.</p><p style="text-align: justify;">Congratulations to Richard Zhang and Ali Mahdavi-Amiri for the successful organization and thanks to all GrUVi students that volunteered to make IGS'19 a success!
  highlight: 0
  image: igs19.png

- date: Jul 22, 2019
  headline: >-
    Richard's SGP Keynote Speech
  text: >-
    Richard Zhang was featured in the Eurographics Symposium on Geometry Processing (SGP) as a keynote speaker! His speech was titled “Can Machines Learn to Generate 3D Shapes?”.</p><p style="text-align: justify;">In his talk, Richard highlighted the representation, data, and output challenges that researchers must tackle and how his research has shaped itself to address these challenges.</p><p style="text-align: justify;">One of the key issues in this domain is not generate shapes that look right; instead, they need to serve their intended function while displaying the right part connections, arrangements, and geometry.
  highlight: 0
  image: Richard_SGP.jpeg

- date: Jul 9, 2019
  headline: >-
    Xiaoming Liu Visits SFU
  text: >-
    <a href"http://www.cse.msu.edu/~liuxm" target="_blank">Dr. Xiaoming Liu</a> visited SFU today following an invitation from Dr. Yasutaka Furukawa! His talk titled "Inverse Graphics for 3D Modeling and Reconstruction: from Face to Generic Objects" has the following abstract: <br><br>"Reconstructing the detailed and complete 3D surface of an object from a single 2D image is a long standing computer vision problem. In this talk, we present an inverse graphics-based framework to learn a 3D face model from a large set of in-the-wild 2D face images, without the need of capturing 3D scans. We also extend this framework to high-fidelity face modeling, as well as generic object modeling and 3D reconstruction. In the end, we will briefly overview other research efforts in the Computer Vision Lab at Michigan State University, including face anti-spoofing, explainable recognition, early sensor fusion of LiDAR and RGB, 2D/3D object detection from urban driving videos, etc."
  highlight: 0
  image: xiaoming.jpg

- date: Jul 3, 2019
  headline: >-
    Yağız Aksoy Joins GrUVi
  text: >-
    GrUVi is glad to announce that Yagiz Aksoy will join our team this August. He is a PhD student of Marc Pollefeys at ETH Zurich. During his PhD, he spent a year at MIT CSAIL working with Wojciech Matusik and three years at Disney Research Zurich.<br>His research is at the intersection of computer vision and computer graphics, focusing on analyzing images to allow for realistic manipulation of photographs. His research has resulted in publications at venues such as SIGGRAPH, CVPR, and ACM Transactions on Graphics.<br>Welcome to the GrUVi team, Yagiz!
  highlight: 0
  image: yagiz.jpg

- date: Jun 3, 2019
  headline: >-
    Zhiqin's and Jon's Thesis Defense
  text: >-
    The GrUVi team congratulates both Zhiqin and Jon for their sucessful MSc theses defense!<br>Jon's thesis is on A Qualitative and Localized Evaluation for 3D Indoor Scene Synthesis<br>Zhiqin's thesis is on IM-NET: Learning Implicit Fields for Generative Shape Modeling<br>Zhiqin's thesis defence also received the honor of "pass as is" today. Both of them will start as PhDs at GrUVi this September. 
  highlight: 0
  image: zhiqin_jon.jpg

- date: May 23, 2019
  headline: >-
    Kangxue Yin wins an Award!
  text: >-
    The GrUVi team congratulates Kangxue Yin, whom received the prestigious Chinese Government Award for Outstanding Self-financed Students Abroad!</p><p style="text-align: justify;">This award was established in 2003 by the&nbsp;<a title="China Scholarship Council" href="https://en.wikipedia.org/wiki/China_Scholarship_Council">China Scholarship Council</a>. The worldwide recipients are chosen annually based on a record of outstanding accomplishments in any discipline.</p><p style="text-align: justify;">This award is considered the highest award given by Chinese government for Chinese graduate students studying abroad who do not receive regular financial support from Chinese government.</p><p style="text-align: justify;">Currently, this award is granted to only 500 students every year. Since there are around half a million Chinese students to study abroad per year, this a highly competitive award. For the academic year 2018, only 6 awardees were based in British Columbia, Canada. Among them, Kangxue was the only student majoring in Computer Science.
  highlight: 0
  image: kangxue_award.jpg

- date: May 1, 2019
  headline: >-
    Accepted Paper on CVPR
  text: >-
    A paper by Zhiqin Chen and Hao Zhang on learning implicit fields for generative modeling of 3D shapes will be presented at CVPR 2019!<br>It has also been invited to be presented at the <a href="https://3dscenegen.github.io/">Workshop on 3D Scene Generation</a> at CVPR; see <a href="https://arxiv.org/abs/1812.02822">paper on arXiv</a>.<br>Congratulations to both Zhiqin and Richard!
  highlight: 0
  image: zhiqin_cvpr.png

- date: April 14, 2019
  headline: >-
    New and Returning GRuVIers!
  text: >-
    The GrUVi team would like to welcome three SFU undergrad students who are joining the lab for summer/visiting research:</p> <ul> <li style="line-height: 100%; text-align: justify;">Leo Li - to start in the summer as a VPA USRA working on representation learning;</li> <li style="line-height: 100%; text-align: justify;">Atticus Shi - who is working on a geometric optimization problem related to fixture design for CNC machining;</li> <li style="line-height: 100%; text-align: justify;">Azmarie Wang - to start in the summer as a special project student working on creative design problems.</li> </ul> <p align="justify">In addition, Xiaogang Wang is joining us as a visiting PhD from Beihang University, the same university as Jiongchao. He comes with an already impressive research record (one SIG Asia and one CVPR), working with Kevin, a GrUVi alumni himself.</p> <p align="justify">Fenggen (Fogg) Yu, will be a new PhD starting this fall. Fenggen is from Nanjing University and he had already collaborated with GrUVi members on a ACM TOG paper. He also has a CVPR paper this year.</p> <p align="justify">We are also pleased to announce that two current members of the lab, Zhiqin and Jon, will continue as PhD students this fall.</p> <p align="justify">Finally, Akshay is returning from his internship at Amazon (Israel) and Manyi, as a newly minted PhD, will be back as a postdoc this Summer.
  highlight: 0
  image: gruvi_logo.png

- date: October 2, 2018
  headline: >-
    5 SIG Asia 2018 papers
  text: >-
    GrUVi members co-author five technical papers at SIGGRAPH Asia 2018, which will be held at Tokyo in December 4-7. In particular, Wallace, Shuhua, Akshay, Manyi, Ali, and Han all got their very first SIG papers! The papers cover topics in fabrication, machine learning, shape and scene modeling, NLP, and even a fun (fabricable) puzzle! The papers are: <a target="_blank" rel="noopener noreferrer" href="http://www.sfu.ca/~agadipat/publications/2018/T2S/project_page.html">"Language-Driven Synthesis of 3D Scenes Using Scene Databases"</a>, <a target="_blank" rel="noopener noreferrer" href="https://sites.google.com/site/alimahdaviamiri/projects/reversible-shapes">"Construction and Fabrication of Reversible Shape Transforms"</a>, <a target="_blank" rel="noopener noreferrer" href="http://gruvi.cs.sfu.ca/project/eulerianwires/">"Fabricable Eulerian Wires for 3D Shape Abstraction"</a>, <a target="_blank" rel="noopener noreferrer" href="https://kevinkaixu.net/projects/scores.html">"SCORES: Shape Composition with Recursive Substructure Priors"</a>, and <a target="_blank" rel="noopener noreferrer" href="http://irc.cs.sdu.edu.cn/~xuelin/prefab/index.html">"3D Fabrication with Universal Building Blocks and Pyramidal Shells"</a>. Congratulations to all authors!
  highlight: 0
  image: ma_siga18_t2s.png

- date: August 23, 2018
  headline: >-
    Gruviers Volunteered for SIG'2018
  text: >-
    Two gruviers were student volunteers (SV) at the SIGGRAPH 2018 conference. While the attendees were certainly thankful for all the work that all the volunteers put in the event, our gruvi volunteers described how valuable the experience was for them! Zhiqin got the chance to use the hardware available at the exhibition hall to print the awesome gruvi logo on the hat shown in the picture. He described the great impact that experiencing the latest developments on VR technology had on him:<br /><br /> "I'm honored to be a student volunteer of such a great conference. VR was definitely a highlight of SIGGRAPH 2018. There were many showcases of applying haptic technology to VR and utilizing VR for designing and creating content. The immersive pavilion was awesome and provided me all kinds of VR experiences. I am looking forward to next SIGGRAPH!"<br /><br />Akshay, on the other hand, emphasized the connections he was able to forge during his time as a volunteer:<br /><br />"This was my first SIGGRAPH and my first time as a SIG-SV. The kind of atmosphere, the exuberance of energy and experience that I felt was surreal and I was left wanting for more when it ended. Memories of a lifetime with awesome fellow SV's, coordinating various programs and above all, the latest technology!! Definitely want to relive this again at SIG'19."
  highlight: 0
  image: gruvi_hat.jpg


- date: August 21, 2018
  headline: >-
    GrUVi presentations @ SIG'2018
  text: >-
    SIGGRAPH 2018 was held in Vancouver from August 12 to 16, at the Vancouver Convention Centre. The majority of GrUVi members attended the conference, some as paper presenters and some as student volunteers. This year, the lab contributed four papers that were presented in the Technical Papers program. Ali presented his paper on semi-supervised analysis of 3D shape styles and Kangxue presented his paper on P2P-NET, both on Thursday, in the paper session on Shape Analysis. Also notably, long-term collaborator with GrUVi and adjunct professor with the SFU School of Computing Science, Prof. Daniel Cohen-Or, is featured as the annual SIGGRAPH Achievement Award winner. Danny has been collaborating with Richard Zhang and students in the lab since 2008!
  highlight: 0
  image: Kangxue_Ali.PNG

- date: August 21, 2018
  headline: >-
    Eugene's 60th Aniversary Celebrated
  text: >-
    A special event honouring Professor Eugene Fiume, on the occasion of his 60-th birthday, was held at the SFU Harbour Center on August 17, right after SIGGRAPH 2018. Many of Eugene's first-generation PhDs from the University of Toronto attended the celebratory workshop and gave their account of fun and inspiring stories about what it was like to be his graduate students. Eugene's computer graphics instructor during his undergraduate years at the University of Waterloo provided hint of how Eugene was already "ahead of the class" then. The event featured Prof. Pat Hanrahan from Stanford University, as the keynote speaker. Pat spoke of "theory vs. practice" to highlight how over the years, Eugene has been among the rare group of people in computer graphics, who have emphasized the importance of theory in the field.
  highlight: 0
  image: eugene_event1.jpg

- date: July 22, 2018
  headline: >-
    12 papers from SFU at ECCV 18
  text: >-
    Congratulations to all SFU researchers that were able to have an impressive count of 12 papers accepted at the European Conference on Computer Vision 2018 (ECCV 2018)! ECCV is considered, together with ICCV and CVPR, one of the top level conferences in computer vision. Here is the full list of accepted papers with SFU co-authors:  <br /><br />&#9679;"Hierarchical Relational Networks for Group Activity Recognition and Retrieval" by  Mostafa Ibrahim*, Simon Fraser University; Greg Mori, Simon Fraser University;<br />&#9679;"Neural Procedural Reconstruction for Residential Buildings" by  Huayi Zeng*, Washington University in St.Louis; Jiaye Wu, Washington University in St.Louis; Yasutaka Furukawa, Simon Fraser University;<br />&#9679;"IM2Hand3D: Leveraging Multi-task Network for 3D Hand Pose Estimation from a Color Image" by  Xiaoming Deng*, Chinese Academy of Sciences; Wenyong Zheng, Chinese Academy of Sciences ; Yinda Zhang, Princeton University; Jian Shi, Chinese Academy of Sciences ; Ping Tan, Simon Fraser University; Liang Chang, Beijing Normal University; Yuying Zhu, Chinese Academy of Sciences;<br />&#9679;"FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans" by  Chen Liu*, Washington University in St. Louis; Jiaye Wu, Washington University in St.Louis; Yasutaka Furukawa, Simon Fraser University;<br />&#9679;"Probabilistic Video Generation using Holistic Attribute Control" by  Jiawei He*, Simon Fraser University; Andreas Lehrmann, Facebook; Joe Marino, California Institute of Technology; Greg Mori, Simon Fraser University; Leonid Sigal, University of British Columbia;<br />&#9679;"Constraints Matter in Deep Neural Network Compression" by  Changan Chen, Simon Fraser University; Fred Tung*, Simon Fraser University; Naveen Vedula, Simon Fraser University; Greg Mori, Simon Fraser University;<br />&#9679;"Characterize Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation" by  CHAOWEI XIAO, University of Michigan, Ann Arbor; Ruizhi Deng, Simon Fraser University; Bo Li*, University of Illinois at Urbana Champaign; Fisher Yu, UC Berkeley; mingyan liu, university of Michigan, Ann Arbor; Dawn Song, UC Berkeley;<br />&#9679;"Faces as Lighting Probes via Unsupervised Deep Highlight Extraction" by  Renjiao Yi*, Simon Fraser University; Chenyang Zhu, Simon Fraser University; Ping Tan, Simon Fraser University; Stephen Lin, Microsoft Research;<br />&#9679;"Sparsely Aggregated Convolutional Networks" by  Ligeng Zhu*, Simon Fraser University; Ruizhi Deng, Simon Fraser University; Michael Maire, Toyota Technological Institute at Chicago; Zhiwei Deng, Simon Fraser University; Greg Mori, Simon Fraser University; Ping Tan, Simon Fraser University;<br />&#9679;"RIDI: Robust IMU Double Integration" by  Hang Yan*, Washington University in St. Louis; Qi Shan, Zillow Group; Yasutaka Furukawa, Simon Fraser University;<br />&#9679;"Object Level Visual Reasoning in Videos" by  Fabien Baradel, LIRIS; Natalia Neverova*, Facebook AI Research; Christian Wolf, INSA Lyon, France; Julien Mille, INSA Centre Val de Loire; Greg Mori, Simon Fraser University.<br />&#9679;"SketchyScene: Richly-Annotated Scene Sketches" by  Changqing Zou*, University of Maryland (UMD); Qian Yu, Queen Mary University of London; Ruofei Du, UMD; Haoran Mo, sun yat sen university; Yi-Zhe Song, Queen Mary University of London; Tao Xiang, Queen Mary, University of London, UK; Chengying Gao, sun yat sen university; Baoquan Chen, Shandong University; Hao Zhang, SFU.
  highlight: 0
  image: eccv18.png

- date: May 18, 2018
  headline: >-
    Three new visiting Gruviers!
  text: >-
    GrUVi welcomes three new visiting students this summer: <a href="https://www.cse.iitb.ac.in/~manas/">Manas Bhargave</a>, IIT Bombay undergrad and visiting under MITACS-IIT; Ruiqi Ni, an undergrad student from University of Science and Technology (USTC) China and Chen Song, SFU-ZJU DDP student under USRA. <br /><br /> Welcome to GrUVi!
  highlight: 0
  image: threenew.png

- date: May 17, 2018
  headline: >-
    GrUVi co-authors three SIG'18 papers
  text: >-
    Three papers co-authored by GrUVi members have been accepted at SIGGRAPH 2018: P2P-NET: Bidirectional Point Displacement Net for Shape Transform, Predictive and Generative Neural Networks for Object Functionality, and DSCarver: Decompose-and-Spiral-Carve for Subtractive Manufacturing. P2P-NET was first-authored by GrUVi's PhD student Kangxue Yin. For more details, please visit the <a href="http://gruvi.cs.sfu.ca/publications/">publications page</a>.
  highlight: 0
  image: P2P-NET.PNG

- date: May 16, 2018
  headline: >-
    Two talks at SFU-ZJU Symposium
  text: >-
    Richard and Ping give talks at &nbsp;<a href="http://www.cad.zju.edu.cn/home/dengcai/Symposium2018/">"Third SFU-ZJU Joint Symposium on Big Data and Visual Computing"</a>. The Third Joint Symposium on Big Data and Visual Computing Research is a forum where researchers and students from the two universities will meet, exchange ideas, and explore opportunities.
  highlight: 0
  image: richard_ping.png

- date: April 12, 2018
  headline: >-
    Kangkang's NSERC DAS Award
  text: >-
    Congrats to Kangkang who has received a NSERC Discovery Accelerator Supplement (DAS) for 2018. Each year 125 applicants to NSERC Discovery grants receive this award, which comes with $40K per year for three years on top of a Discovery Grant award! <br /><br /> The DAS Program provides substantial and timely resources to researchers who have a superior research program that is highly rated in terms of originality and innovation, and who show strong potential to become international leaders within their field. Furthermore, the Discovery Grants Program prioritizes research with long-term goals and recognizes the creativity and innovation that are at the heart of all research advances. <br /><br />Well done Kangkang!
  highlight: 0
  image: KKYIN.jpg

- date: April 10, 2018
  headline: >-
    Yasu's CS-Can/Info-Can Award
  text: >-
    Congrats to Yasu who has received an Outstanding Young CS Researcher Award for 2017.  <br /><br />These prizes recognize excellence in research, and are made to top young faculty members in Canadian Computer Science Departments/Schools/Faculties who are within the first 10 years of their career beyond the completion of their PhD.<br /><br />Well done Yasu!
  highlight: 0
  image: furukawa4.png

- date: April 8, 2018
  headline: >-
    GI'18 to feature two GrUVi alumni
  text: >-
    Graphics Interface 2018 (GI'18) will be held in York University, May 8-11. Two of our GrUVi alumni, Oliver van Kaick and Andrea Tagliasacchi, are invited speakers on the program!  <br /><br />Graphics Interface is an annual international conference devoted to computer graphics and human-computer interaction (HCI). GI is the longest running conference in the field (the first conference was held in 1969), consistently attracting high-quality submissions from graphics, HCI, as well as visualization.
  highlight: 0
  image: oliver_andrea.jpg

- date: February 23, 2018
  headline: >-
    Yasu's 2018 Google Research Award
  text: >-
    GrUVi congratulates Yasu Furukawa, who received a Google Research Award in the current cycle! Yasu's project is titled "Automatic Floorplan Generation from Tango", and is valued at $43,000 (USD).  The works funded through Google Research Awards tend to have a high impact, since research results are often published at top conferences and in top publications in Computer Science.  <br /><br />Well done Yasu!
  highlight: 0
  image: news_3.PNG


- date: October 13, 2017
  headline: >-
    SIG Asia and ICCV 2017 papers
  text: >-
    GrUVi members co-author two technical papers:&nbsp;<a href="http://www.cs.sfu.ca/~haoz/pubs/hu_siga17_icon3.pdf">"Learning to Predict Part Mobility from a Single Static Snapshot"</a> and <a href="http://www.cs.sfu.ca/~haoz/pubs/zou_siga17_group.pdf">"Learning to Group Discrete Graphical Patterns"</a>; and <a href="https://3dworldsblog.wordpress.com/">one course at SIGGRAPH Asia 2017</a>, to be held in Bangkok, Thailand, November 27-30. At ICCV 2017, three papers co-authored by GrUVi members have been presented.
  highlight: 0
  image: news_1.PNG

- date: October 13, 2017
  headline: >-
    Gruvi welcomes Jon Liu!
  text: >-
    Jon Liu as Gruvi's new MSc student. Jon obtained his Master's degree from National Tsinghua University in computer graphics and since then, he has worked in the industry for many years. He recently immigrated to Canada and started his grad studies at SFU in the summer this year. Jon is interested in robotic 3D vision problems. Welcome Jon!
  highlight: 0
  image: jonliu.jpg

- date: October 1, 2017
  headline: Conference co-chairs
  text: GrUVi faculty Yasu Furukawa will be a program co-chair for 3DV 2017, which will be held in Qingdao, China between October 10 and 12. Richard Zhang will also be a program co-chair for Computer Graphics International (CGI) 2018, which will be hosted at Bintan Island, Indonesia, June 11-14, 2018.
  highlight: 0
  image: news_3.PNG


- date: September 19, 2017
  headline: Rui Ma's thesis "pass as is"
  text: Rui has successfully defended his PhD thesis on "Sub-Scene Level Analysis and Synthesis of 3D Indoor Scenes". The thesis and defence has been designated with a "pass as is" distinction by the defence committee members Oliver Deussen (external examiner), Yasu (internal examiner), Ze-Nian (thesis committee members), and Kangkang (chair). Rui Ma also landed a position at Research Scientist at Altumview Systems Inc. Congratulations Rui!
  highlight: 0
  image: ruima.png

- date: August 27, 2017
  headline: Warunika wins best paper award
  text: The paper "An Exquisite Corpse Tool for Collaborative 3D Shape Design", co-authored by Warunika, Parmit Chilana, Daniel Cohen-Or, and Richard Zhang won a best paper award at the recent CAD/Graphics conference. This work was part of Warunika's MSc thesis; she graduated in Dec 2016 and is now at IBM.
  highlight: 0
  image: news_2.PNG

- date: August 26, 2017
  headline: GRASS featured at SIG'17
  text: The paper "Generative Recursive Autoencoder for Shape Structures (GRASS)" co-authored by Prof. Richard Zhang and former GrUVi member Kai Xu, was selected as one of the six technical papers from SIGGRAPH 2017 for press release. See the press release article <a href="https://www.eurekalert.org/pub_releases/2017-07/afcm-md3071817.php">here</a> and an <a href="https://www.sfu.ca/fas/news-and-outreach/years/2017/grass-a-new-algorithm-for-3d-content-creation-to-be-featured-at-siggraph.html">invited article</a> by Richard Zhang on SFU's official website.
  highlight: 0
  image: news_4.PNG

- date: August 25, 2017
  headline: Richard keynote talk at CAD/Graphics
  text: Richard Zhang gave a keynote talk on "Can Machines Learn to Generate 3D Shapes" at the Biannual International Conference on Computer Aided Design and Graphics (CAD/Graphics) in Zhangjiajie, China.
  highlight: 0
  image: Richard.jpeg

- date: August 10, 2017
  headline: GrUVi co-authors three SIG'17 papers
  text: Three papers co-authored by GrUVi members have been accepted to SIGGRAPH 2017&#58; <a href="http://kevinkaixu.net/projects/grass.html">GRASS:Generative Recursive Autoencoders for Shape Structures</a>, <a href="http://gruvi.cs.sfu.ca/project/timeslice/">Time Slice Video Synthesis by Robust Video Alignment</a>, and <a href="http://www.cs.sfu.ca/~haoz/pubs/zhu_sig17_scsr.pdf">Deformation-Driven Shape Correspondence via Shape Recognition</a>.<br /><br />Two of these papers are first-authored by PhD students from GrUVi <a href="http://www.zhpcui.org/"> Zhaopeng Cui</a> and <a href="http://www.sfu.ca/~cza68/">Chenyang Zhu</a>. In addition, one paper accepted to ACM Transaction on Graphics, <a href="http://www.cs.sfu.ca/~haoz/pubs/hu_tog17_style.pdf">Co-Locating Style-Defining Elements on 3D Shapes</a>, will also be presented at SIGGRAPH 2017 in Los Angeles. Collaborators on these papers include researchers from Adobe Research, Tel Aviv University, Stanford University, National University of Defense Technology, Carleton University, IIT Bombay, Shenzhen University, and University of Cyprus.
  highlight: 0
  image: SIG17.png

- date: July 29, 2017
  headline: Zhaopeng goes to ETH
  text: Zhaopeng, who successfully defended his PhD thesis earlier this month, will start his postdoc position at ETH. His thesis also received the rare distinction of "pass as is". Previous gruviers who received such a distinction include Ibraheem Alhashim (2015), Andrea Tagliasacchi (2013), and Oliver van Kaick (2011). Congratulations Zhaopeng!
  highlight: 0
  image: zhaopeng.png

- date: July 25, 2017
  headline: Zhiqhin and Manyi are new Gruviers!
  text: Gruvi welcomes two new members, Zhiqin Chen and Manyi Li, who will join our group during Fall 2017. Zhiqin is a new master student who obtained his bachelor's degree from Shanghai Jiaotong University, while Manyi is a visiting Ph.D. from Shandong University, China. Welcome!
  highlight: 0

- date: July 22, 2017
  headline: Changqing Zou goes to UMIACS
  text: Postdoc Changqing Zou will take up an Assistant Research Scientist position at the University of Maryland Institute for Advanced Computer Studies (UMIACS). Congratulations Changqing!
  highlight: 0

- date: May 9, 2017
  headline: Ibraheem speaks at KAUST
  text: GrUVi alumni Ibraheem Alhashim, who was a PhD student supervised by Richard Zhang between 2011 and 2015, was recently invited to speak at the <a href="https://vcc.kaust.edu.sa/Conference-2017/Pages/default.aspx">KAUST Visual Computing (KAUST RC-VC) “ Modeling and Reconstruction conference</a>. <a href="https://vcc.kaust.edu.sa/Conference-2017/Pages/Speakers.aspx">Speakers at the conference</a> include many of the top experts in the fields of computer graphics and visual computing. Please follow <a href="https://www.kaust.edu.sa/en/news/visualizing-the-future-of-computing?utm_content=buffer0c13e&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">this link</a> for news coverage on Ibraheem's talk and other presentations at the conference.
  highlight: 0
  image: ibrahimKAUST.jpg

- date: May 1, 2017
  headline: New GrUVi faculty Kangkang Yin
  text: It is official! <a href="http://www.cs.sfu.ca/~kkyin/">Prof. Kangkang Yin</a> will be joining SFU CS as an Associate Professor starting July 2017. As one of the leading experts in computer animation and simulation, she will be a fantastic addition to the GrUVi lab. Most recently, KangKang Yin was an Associate Professor in the Department of Computer Science at the National University of Singapore. She worked as an Associate Researcher at Microsoft Research Asia from 2008 to 2010, after she obtained her PhD degree from the University of British Columbia in 2007.
  highlight: 0
  image: KKYIN.jpg

- date: December 23, 2016
  headline: Jaime's Sucessful Msc Thesis Defense
  text: Congrats to Jaime for successfully defending his MSC thesis on "Towards Learning of a Joint Geometry-Structure Manifold for Shape Exploration".<br /><br /> Here is the abstract of his thesis.<br /><br /> We present a first attempt at producing a continuous generative model of 3D objects from a joint representation that incorporates the discrete structural variability as well as the continuous geometric variability that are often present in collections of man-made shapes. Starting from a set of compatibly segmented shapes, our main contribution consists in demonstrating the construction of the joint representation. Then, by using Gaussian Process learning to produce a predictive manifold from the joint representation, we investigate its capabilities and limitations for reproducing and synthesizing new shapes.
  highlight: 0
  image: Jaime.png

- date: December 9, 2016
  headline: Warunika's Successful Msc Thesis Defense
  text: Congrats to Warunika for successfully defending her MSc thesis on "ExquiMo&#58; An Exquisite Corpse Tool for Co-Creative 3D Shape Modeling". The researchers&nbsp;would like to assure you that no person, dog, or any other animals were killed as the result of conducting this thesis research.<br /><br /> Here is the abstract of her work:<br /><br /> We introduce a shape modeling tool, ExquiMo, which is guided by the idea of improving the creativity of 3D shape designs through collaboration. Inspired by the game of Exquisite Corpse, our tool allocates distinct parts of a shape to multiple players who model the assigned parts in a sequence. Our approach is motivated by the understanding that effective surprise leads to creative outcomes. Hence, to maintain the surprise factor of the output, we conceal the previously modeled parts from the most recent player. Part designs from individual players are fused together to produce an often unexpected, hence creative, end result. We demonstrate the effectiveness of collaborative modeling for both man-made and natural shapes. Our results show that, when compared to models designed by individual users, multi-user collaborative modeling via ExquiMo tends to lead to more creative designs in terms of the most common criteria used to identify creative artifacts.
  highlight: 0
  image: Warunika.jpg

- date: September 7, 2016
  headline: "New students joining GrUVi"
  text: Nine new members are joining the&nbsp;GrUVi&nbsp;lab in the fall semester of 2016.&nbsp;We welcome:<br /><br /> Ali&nbsp;Mahdavi-Amiri, a new postdoc&nbsp;to start in January 2017, who obtained his&nbsp;PhD from&nbsp;University of Calgary, Calgary, Canada,&nbsp;his&nbsp;MSc from&nbsp;Sharif University of Technology,&nbsp;Tehran, Iran, and his BSc from&nbsp;Ferdowsi&nbsp;University of&nbsp;Mashhad,&nbsp;Mashhad, Iran.<br /><br /> Han Liu, a new postdoc&nbsp;starting in June 2016, who obtained her PhD&nbsp;from&nbsp;King Abdullah University of Science and Technology, Saudi Arabia, her MSc from&nbsp;Xiamen&nbsp;University, China, and her BSc from&nbsp;Huangshang&nbsp;University, China.<br /><br /> Kiana Mostaghasi, a new MSc student starting in September 2016, who obtained her BSc from&nbsp;Amirkabir University of Technology, Iran.<br /><br /> Nelson Nuata, a new MSc student starting in September 2016, who obtained B.Eng. from&nbsp;University of Campinas, Brazil.<br /><br /> Akshay&nbsp;Gadi&nbsp;Patil, a new Ph.D. student starting in September 2016, who obtained his&nbsp;M.Tech&nbsp;from&nbsp; Indian Institute of Technology&nbsp;Gandhinagar, and his B.Eng.&nbsp;from the&nbsp;P.E.S Institute of Technology (now PES University).<br /><br /> Rakesh&nbsp;Shrestha, a new M.Sc. student starting in September 2016, who obtained his B.Sc. from Bangladash.<br /><br /> Feitong Tan, a new Ph.D. student starting in September 2016, who obtained his M.Sc. from Chengdu, China.<br /><br /> Sicong Tang, a new Ph.D. starting in September 2016, who obtained his M.Sc. from&nbsp;Northwestern Polytechnical University in China.<br /><br /> Zili&nbsp;Yi, a visiting PhD student starting in September 2016, who obtained his&nbsp;M.E. from&nbsp;University&nbsp;of Chinese Academy of Sciences,&nbsp;China and his BSc from&nbsp;Nanjing&nbsp;University,&nbsp;China.
  highlight: 0
  image: 9new.png

- date: August 5, 2016
  headline: Richard keynote talk at CCCG'16
  text: GrUVi faculty Richard Zhang was invited to give a keynote talk at the 2016 Canadian Conference on Computational Geometry (CCCG). His talk is titled "New Geometry Problems in Computational Design and Fabrication".
  highlight: 0
  image: Richard.jpeg

- date: July 24, 2016
  headline: GrUVi co-authors three SIG papers
  text: The three papers are&#58; <a href="http://vcc.szu.edu.cn/research/2016/Icon2/">Learning How Objects Function via Co-Analysis of Interactions</a>, <a href="http://www.cs.sfu.ca/~haoz/pubs/zhao_sig16_fermat.pdf">Connected Fermat Spirals for Layered Fabrication</a>, and <a href="http://www.cs.sfu.ca/~haoz/pubs/zou_sig16_calli.pdf">Legible Compact Calligrams</a>.&nbsp;The latter paper was led by&nbsp;<a href="http://changqingzou.weebly.com/">Changqing Zou</a>,&nbsp;a postdoc co-supervised by Ping and Richard, who just&nbsp;had his first paper accepted to SIGGRAPH. This work was&nbsp;a collaboration between SFU and UBC. Other collaborators on these papers include those from Carleton University, Shenzhen University, Purdue University, Shandong University, Tel-Aviv University, the Interdisciplinary Center (IDC, Israel),&nbsp;Toyota Technical Institute (TTI, USA), Dalian University of Technology,&nbsp;and University of Southern California.
  highlight: 0
  image: SIG16.png

- date: May 19, 2016
  headline: Richard wins SFU service award
  text: Richard Zhang completed his four-year term as the Graduate Program Director at the School of Computing Science on April 30, 2016. With his services and contributions, he was selected as the sole winner of the SFU Dean of Graduate Studies Award of Excellence in Leadership in 2016. Each award consists of a prize of a $1,000 graduate student travel award that will be disbursed in the following academic year to a graduate student of the award recipient™s choice to attend a conference to present a paper.
  highlight: 0
  image: SFUserviceAward.jpg

- date: March 14, 2016
  headline: Ibraheem wins Alain Fournier Award
  text: This award is given to the top computer graphics Ph.D. dissertation defended in a Canadian university in a given year and will be officially announced at Graphics Interface '16. The award recognizes the strength and quality of Ibraheem's contributions in his dissertation titled "Topology Varying Shape Matching and Modelling".
  highlight: 0
  image: ibrahimAlain.png
