{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_url: https://api.notion.com/v1/databases/54cb3960b40246e2be42cfb58e2b45fd/query\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "KEY = \"secret_dhDJCxIGksOzhxiKis5UfH8VWA6JXTnF5T44BANarrG\"\n",
    "headers = {\"Authorization\":KEY, \"Notion-Version\":\"2022-06-28\", \"Content-Type\":\"application/json\"}\n",
    "base_url = \"https://api.notion.com/v1/databases/\"\n",
    "publication_database_id = '54cb3960b40246e2be42cfb58e2b45fd'\n",
    "people_database_id = '778b56a297cc4fd7b45d8f40c3d4e1cb'\n",
    "query_url = base_url+publication_database_id+\"/query\"\n",
    "print(\"query_url:\", query_url)\n",
    "#filter_rule = {\"or\": [dict(property=\"年月\", \"\")]}\n",
    "filter_rule={}\n",
    "sort_rules = [dict(property=\"ConferenceDate\", direction=\"descending\")]\n",
    "\n",
    "query_data = dict(page_size=100, sorts=sort_rules)#, filter=filter_rule)\n",
    "\n",
    "#res = requests.post(query_url, headers=headers, query_data=query_data)\n",
    "#result = res.content\n",
    "#obj = json.loads( result.decode() )\n",
    "#df = pd.DataFrame(obj[\"results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'object': 'list', 'results': [{'object': 'page', 'id': '6ba78a9f-244d-494e-b4e7-299a9dda8692', 'created_time': '2023-10-26T18:21:00.000Z', 'last_edited_time': '2023-10-26T18:30:00.000Z', 'created_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}, 'last_edited_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'gcmic.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/787e60b3-a264-4aed-9e48-4d390886928c/gcmic.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=40ca4b33a6cae91eead3ef396b6f90c1b008feb4e173ca85fa9fa53ba050af8a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': []}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': []}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'href': None}]}}, 'url': 'https://www.notion.so/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692', 'public_url': 'https://yanxg.notion.site/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692'}, {'object': 'page', 'id': '99d34d16-28ec-4ada-abee-307b42832af2', 'created_time': '2023-10-26T02:16:00.000Z', 'last_edited_time': '2023-10-26T02:21:00.000Z', 'created_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}, 'last_edited_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opdmulti.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a14d25de-bf9a-4152-8e96-bd50e29c39da/opdmulti.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=f0fedd6a46931a9182b38ff6ebe5a0c646b7197562bf1da8e93ec13990979abb&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPDMulti/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPDMulti/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPDMulti/', 'href': 'https://3dlg-hcvc.github.io/OPDMulti/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.14087', 'link': {'url': 'https://arxiv.org/abs/2303.14087'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.14087', 'href': 'https://arxiv.org/abs/2303.14087'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPDMulti: Openable Part Detection for Multiple Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPDMulti: Openable Part Detection for Multiple Objects', 'href': None}]}}, 'url': 'https://www.notion.so/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2', 'public_url': 'https://yanxg.notion.site/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2'}, {'object': 'page', 'id': 'af27631d-4936-4a32-b742-dca9d9379a50', 'created_time': '2023-10-25T17:44:00.000Z', 'last_edited_time': '2023-10-25T17:52:00.000Z', 'created_by': {'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4ece1f23-aa46-4ba4-9fc7-35328b2bca56/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=cfe68c7880d32a199c6ab2cd272bf53fc6d67100c5375bc39bf12ec21da1e229&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/shaddr/', 'link': {'url': 'https://qiminchen.github.io/shaddr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/shaddr/', 'href': 'https://qiminchen.github.io/shaddr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH Asia 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH Asia 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'd0404583-7d96-4756-b656-bb910bc1e011', 'name': 'SIGGRAPH Asia', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04889', 'link': {'url': 'https://arxiv.org/abs/2306.04889'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04889', 'href': 'https://arxiv.org/abs/2306.04889'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'href': None}]}}, 'url': 'https://www.notion.so/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50', 'public_url': 'https://yanxg.notion.site/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50'}, {'object': 'page', 'id': '8fcbf0dc-f008-47d3-9190-245544f99ad3', 'created_time': '2023-11-02T23:49:00.000Z', 'last_edited_time': '2023-11-02T23:57:00.000Z', 'created_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}, 'last_edited_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help \"stitch\" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help \"stitch\" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'DiViNeT_long.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1c8a5bf0-df4c-4877-9ba3-57a0efb585d0/DiViNeT_long.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=3807a842f9e8e54a02069abc1dadd74a965f4f10359f7d8f44d39e03b5f30a54&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aditya-vora.github.io/divinetpp/', 'link': {'url': 'https://aditya-vora.github.io/divinetpp/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aditya-vora.github.io/divinetpp/', 'href': 'https://aditya-vora.github.io/divinetpp/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04699', 'link': {'url': 'https://arxiv.org/abs/2306.04699'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04699', 'href': 'https://arxiv.org/abs/2306.04699'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'href': None}]}}, 'url': 'https://www.notion.so/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3', 'public_url': 'https://yanxg.notion.site/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3'}, {'object': 'page', 'id': 'a6aec9d2-5b4e-485c-b437-baa336078736', 'created_time': '2023-10-27T07:36:00.000Z', 'last_edited_time': '2023-10-27T07:38:00.000Z', 'created_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}, 'last_edited_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://jcchen.me/assets/img/publications/poly-diffuse.png', 'type': 'external', 'external': {'url': 'https://jcchen.me/assets/img/publications/poly-diffuse.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://poly-diffuse.github.io/', 'link': {'url': 'https://poly-diffuse.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://poly-diffuse.github.io/', 'href': 'https://poly-diffuse.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.01461', 'link': {'url': 'https://arxiv.org/abs/2306.01461'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.01461', 'href': 'https://arxiv.org/abs/2306.01461'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': '', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '', 'href': None}, {'type': 'text', 'text': {'content': 'PolyDiffuse', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PolyDiffuse', 'href': None}, {'type': 'text', 'text': {'content': ': Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ': Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'href': None}]}}, 'url': 'https://www.notion.so/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736', 'public_url': 'https://yanxg.notion.site/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736'}, {'object': 'page', 'id': '52024903-b5ec-47c0-83bf-a5f65531d5f8', 'created_time': '2023-10-26T03:19:00.000Z', 'last_edited_time': '2023-10-27T20:58:00.000Z', 'created_by': {'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S', 'link': {'url': 'https://tangshitao.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S', 'href': 'https://tangshitao.github.io/'}, {'type': 'text', 'text': {'content': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper introduces \\\\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\\\url{', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper introduces \\\\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\\\url{', 'href': None}, {'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}, {'type': 'text', 'text': {'content': '}.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '}.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'mvdiffusion_teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1ae24cf4-18ab-4c9b-a147-dd9297c251bd/mvdiffusion_teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=1e34f60a8518114e95bf1c13e7d5c4ca28711cb51e58a3b693d25df0b0961a5a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.01097', 'link': {'url': 'https://arxiv.org/abs/2307.01097'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.01097', 'href': 'https://arxiv.org/abs/2307.01097'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'href': None}]}}, 'url': 'https://www.notion.so/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8', 'public_url': 'https://yanxg.notion.site/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8'}, {'object': 'page', 'id': 'e465f55d-74c4-42c4-bb93-c056ef05e56a', 'created_time': '2023-10-26T02:49:00.000Z', 'last_edited_time': '2023-10-26T02:51:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\\\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\\\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'href': None}]}}, 'url': 'https://www.notion.so/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a', 'public_url': 'https://yanxg.notion.site/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a'}, {'object': 'page', 'id': 'e40d79ba-70ee-4aee-86a4-023047c0b26d', 'created_time': '2023-10-26T02:48:00.000Z', 'last_edited_time': '2023-10-26T02:51:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/LDM_correspondences/', 'link': {'url': 'https://ubc-vision.github.io/LDM_correspondences/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/LDM_correspondences/', 'href': 'https://ubc-vision.github.io/LDM_correspondences/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2305.15581', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2305.15581', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'href': None}]}}, 'url': 'https://www.notion.so/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d', 'public_url': 'https://yanxg.notion.site/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d'}, {'object': 'page', 'id': '062f2450-a093-4a1d-afc7-127b9ff2af39', 'created_time': '2023-10-26T02:00:00.000Z', 'last_edited_time': '2023-10-26T02:09:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\\xa0Joakim Bruslund Haurum,\\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\\xa0Joakim Bruslund Haurum,\\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Graham W Taylor,\\xa0Paul Fieguth', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Graham W Taylor,\\xa0Paul Fieguth', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'bioscan1m.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f8b23333-750f-442c-88b7-76d10c28bb28/bioscan1m.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=7b63d15789791240c12381c90b37d9a1212e1da0953b048bcdd6d28fb04a1337&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://biodiversitygenomics.net/1M_insects/', 'link': {'url': 'https://biodiversitygenomics.net/1M_insects/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://biodiversitygenomics.net/1M_insects/', 'href': 'https://biodiversitygenomics.net/1M_insects/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.10455', 'link': {'url': 'https://arxiv.org/abs/2307.10455'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.10455', 'href': 'https://arxiv.org/abs/2307.10455'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'href': None}]}}, 'url': 'https://www.notion.so/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39', 'public_url': 'https://yanxg.notion.site/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39'}, {'object': 'page', 'id': 'b595d4de-624c-430c-a679-0429d50269dc', 'created_time': '2023-10-25T23:13:00.000Z', 'last_edited_time': '2023-10-25T23:25:00.000Z', 'created_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}, 'last_edited_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yanshu Zhang*,\\xa0Shichong Peng*,\\xa0Seyed Alireza Moazenipourasil,\\xa0Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yanshu Zhang*,\\xa0Shichong Peng*,\\xa0Seyed Alireza Moazenipourasil,\\xa0Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\\xa0geometry editing,\\xa0object manipulation,\\xa0texture transfer, and\\xa0exposure control.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\\xa0geometry editing,\\xa0object manipulation,\\xa0texture transfer, and\\xa0exposure control.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'papr-thumbnail2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/cb623732-7a24-44bf-b9de-f5bd5be90bad/papr-thumbnail2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=fbf5436150de5127dbee10600f9e941c8eb7a801b1f1a5c7d38a29d66db5d332&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://zvict.github.io/papr/', 'link': {'url': 'https://zvict.github.io/papr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://zvict.github.io/papr/', 'href': 'https://zvict.github.io/papr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.11086', 'link': {'url': 'https://arxiv.org/abs/2307.11086'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.11086', 'href': 'https://arxiv.org/abs/2307.11086'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PAPR: Proximity Attention Point Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PAPR: Proximity Attention Point Rendering', 'href': None}]}}, 'url': 'https://www.notion.so/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc', 'public_url': 'https://yanxg.notion.site/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc'}, {'object': 'page', 'id': 'e9f11fd2-9460-41d8-8179-5cd059dcdb98', 'created_time': '2023-10-25T17:45:00.000Z', 'last_edited_time': '2023-10-25T23:52:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust∗\\n, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust∗\\n, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural architecture based on Diffusion Models\\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\\nIn the latter task, for instance, the proposed system takes a set of room layouts\\nas polygonal curves in the top-down view and aligns the room layout pieces by\\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\\nof room layouts. A surprising discovery of the paper is that the simple use of\\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\\nconditional generation process. To enable learning of an end-to-end neural system,\\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\\nits production pipeline, where pieces are room layouts constructed by augmented\\nreality App by real-estate consumers. The qualitative and quantitative evaluations\\ndemonstrate that our approach outperforms the competing methods by significant\\nmargins in all the tasks. We will publicly share all our code and data.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural architecture based on Diffusion Models\\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\\nIn the latter task, for instance, the proposed system takes a set of room layouts\\nas polygonal curves in the top-down view and aligns the room layout pieces by\\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\\nof room layouts. A surprising discovery of the paper is that the simple use of\\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\\nconditional generation process. To enable learning of an end-to-end neural system,\\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\\nits production pipeline, where pieces are room layouts constructed by augmented\\nreality App by real-estate consumers. The qualitative and quantitative evaluations\\ndemonstrate that our approach outperforms the competing methods by significant\\nmargins in all the tasks. We will publicly share all our code and data.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'dataset2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1dfce877-56d4-4f3c-9c8d-283e92dd4c10/dataset2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=439dc43e75987b83580dc02ee7681a69b21d6fac4e255eed81ffb747522a8ec2&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.332Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/sepidsh/PuzzleFussion', 'link': {'url': 'https://github.com/sepidsh/PuzzleFussion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/sepidsh/PuzzleFussion', 'href': 'https://github.com/sepidsh/PuzzleFussion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'link': {'url': 'https://arxiv.org/pdf/2211.13785v2.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'href': 'https://arxiv.org/pdf/2211.13785v2.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'href': None}]}}, 'url': 'https://www.notion.so/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98', 'public_url': 'https://yanxg.notion.site/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98'}, {'object': 'page', 'id': '10ad0ead-0668-44d3-8be8-e1627048574a', 'created_time': '2023-10-25T18:01:00.000Z', 'last_edited_time': '2023-10-26T02:17:00.000Z', 'created_by': {'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-20', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0c1a87a7-75d8-4b7f-8a5c-9cd4a02feca6/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=d81f158fe3339b022481c252d6530b2044181ab0eaaa93f8fa31d5c42322b455&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sepidsh.github.io/floorplan_restore/', 'link': {'url': 'https://sepidsh.github.io/floorplan_restore/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sepidsh.github.io/floorplan_restore/', 'href': 'https://sepidsh.github.io/floorplan_restore/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'BMVC 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BMVC 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2c73299d-5d90-4ce0-bdf0-5a4fb4bf7e49', 'name': 'BMVC', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2206.00645.pdf', 'link': {'url': 'https://arxiv.org/pdf/2206.00645.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2206.00645.pdf', 'href': 'https://arxiv.org/pdf/2206.00645.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'href': None}]}}, 'url': 'https://www.notion.so/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a', 'public_url': 'https://yanxg.notion.site/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a'}, {'object': 'page', 'id': '1b12c251-91ff-4e6d-9098-46e325abba8f', 'created_time': '2023-10-26T01:54:00.000Z', 'last_edited_time': '2023-10-26T02:10:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-06', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sriram Yenamandra, Arun Ramachandran,\\xa0Karmesh Yadav, Austin Wang,\\xa0Mukul Khanna,\\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\\xa0Zsolt Kira,\\xa0Manolis Savva,\\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sriram Yenamandra, Arun Ramachandran,\\xa0Karmesh Yadav, Austin Wang,\\xa0Mukul Khanna,\\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\\xa0Zsolt Kira,\\xa0Manolis Savva,\\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Devendra Chaplot,\\xa0Dhruv Batra,\\xa0Roozbeh Mottaghi,\\xa0Yonatan Bisk,\\xa0Chris Paxton', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Devendra Chaplot,\\xa0Dhruv Batra,\\xa0Roozbeh Mottaghi,\\xa0Yonatan Bisk,\\xa0Chris Paxton', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'this https URL', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'this https URL', 'href': 'https://ovmm.github.io/'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'ovmm.jpeg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/5540b6c9-77d5-4d68-89cd-07a836f8d185/ovmm.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=7ee8e512f2f7ba91b0cabe6925b2ea577754179b7bfa027615132d52ae227d7a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ovmm.github.io/', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ovmm.github.io/', 'href': 'https://ovmm.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CoRL 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CoRL 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'link': {'url': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'href': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.11565', 'link': {'url': 'https://arxiv.org/abs/2306.11565'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.11565', 'href': 'https://arxiv.org/abs/2306.11565'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'href': None}]}}, 'url': 'https://www.notion.so/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f', 'public_url': 'https://yanxg.notion.site/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f'}, {'object': 'page', 'id': 'cdc1a96a-7fa0-4e3b-8f2b-4ba676ac8d73', 'created_time': '2023-10-26T01:54:00.000Z', 'last_edited_time': '2023-10-26T21:01:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Enrico Cancelli,\\xa0Tommaso Campari,\\xa0Luciano Serafini,\\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Enrico Cancelli,\\xa0Tommaso Campari,\\xa0Luciano Serafini,\\xa0Angel X. Chang', 'href': None}, {'type': 'text', 'text': {'content': ',\\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ',\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Lamberto Ballan', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Lamberto Ballan', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'prox.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/98c951ee-51cf-449f-b00f-3974721133f9/prox.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=9f67099be6aad1a713e7cfbcf1c979348bb788f2abe97147e5df1cb6ad513654&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'href': None}]}}, 'url': 'https://www.notion.so/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73', 'public_url': 'https://yanxg.notion.site/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73'}, {'object': 'page', 'id': '96b1c296-9893-4784-801a-f0daed0f43ef', 'created_time': '2023-09-18T23:26:00.000Z', 'last_edited_time': '2023-09-25T23:41:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen,\\xa0Ronghang Hu,\\xa0Xinlei Chen,\\xa0Matthias Nießner,\\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen,\\xa0Ronghang Hu,\\xa0Xinlei Chen,\\xa0Matthias Nießner,\\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'unit3d.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/412818c8-c973-4209-b9a6-8cad6cbdf2c4/unit3d.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=93f83dfe9d99d495eb1fe4a37c03e165cc6481f535c63cb53bf9e2ee5e3a6a79&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'href': None}]}}, 'url': 'https://www.notion.so/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef', 'public_url': 'https://yanxg.notion.site/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef'}, {'object': 'page', 'id': '97a335a8-5de9-4af8-97fe-b8cdbbc02c06', 'created_time': '2023-09-18T22:17:00.000Z', 'last_edited_time': '2023-09-18T22:23:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'href': None}]}}, 'url': 'https://www.notion.so/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06', 'public_url': 'https://yanxg.notion.site/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06'}, {'object': 'page', 'id': 'c34ba525-ac8d-4b9b-91e7-d412e7fce348', 'created_time': '2023-09-18T21:52:00.000Z', 'last_edited_time': '2023-09-18T22:26:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiayi Liu,\\xa0Ali Mahdavi-Amiri, Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiayi Liu,\\xa0Ali Mahdavi-Amiri, Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-09-18_14-56-16.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/93befd22-b013-4613-bee2-c9e2bb53c1a5/Snipaste_2023-09-18_14-56-16.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=7556dff945405200944e3d59b3087f5b801c7363c62e8a4314f44209c5209341&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.332Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/paris/', 'link': {'url': 'https://3dlg-hcvc.github.io/paris/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/paris/', 'href': 'https://3dlg-hcvc.github.io/paris/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735', 'link': {'url': 'https://arxiv.org/abs/2308.07391'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735', 'href': 'https://arxiv.org/abs/2308.07391'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'href': None}]}}, 'url': 'https://www.notion.so/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348', 'public_url': 'https://yanxg.notion.site/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348'}, {'object': 'page', 'id': '76c3667b-3a91-4d35-94e6-0d33b0b6a293', 'created_time': '2023-09-18T21:52:00.000Z', 'last_edited_time': '2023-09-18T22:24:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://sked-paper.github.io/resources/teaser.png', 'type': 'external', 'external': {'url': 'https://sked-paper.github.io/resources/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sked-paper.github.io/', 'link': {'url': 'https://sked-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sked-paper.github.io/', 'href': 'https://sked-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735v3', 'link': {'url': 'https://arxiv.org/abs/2303.10735v3'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735v3', 'href': 'https://arxiv.org/abs/2303.10735v3'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SKED: Sketch-guided Text-based 3D Editing', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SKED: Sketch-guided Text-based 3D Editing', 'href': None}]}}, 'url': 'https://www.notion.so/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293', 'public_url': 'https://yanxg.notion.site/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293'}, {'object': 'page', 'id': '40055d39-bb6d-44ea-8d68-453f503ef32d', 'created_time': '2023-09-11T20:58:00.000Z', 'last_edited_time': '2023-10-26T02:00:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Maham Tanveer, Yizhi Wang,\\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Maham Tanveer, Yizhi Wang,\\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'href': None}]}}, 'url': 'https://www.notion.so/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d', 'public_url': 'https://yanxg.notion.site/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d'}, {'object': 'page', 'id': '770d6d8f-f95b-46d8-8cab-db33928836de', 'created_time': '2023-09-11T20:58:00.000Z', 'last_edited_time': '2023-10-26T01:58:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'abstract-b1185106.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/aaeef2a9-fa43-468e-a201-2ed47a1c9214/abstract-b1185106.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=6907a8af96c8fcf74b13de5058fb610d4543c774c7d88fdc64caf30af18d7120&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multi3drefer/', 'link': {'url': 'https://3dlg-hcvc.github.io/multi3drefer/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multi3drefer/', 'href': 'https://3dlg-hcvc.github.io/multi3drefer/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2309.05251', 'link': {'url': 'https://arxiv.org/abs/2309.05251'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2309.05251', 'href': 'https://arxiv.org/abs/2309.05251'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'href': None}]}}, 'url': 'https://www.notion.so/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de', 'public_url': 'https://yanxg.notion.site/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de'}, {'object': 'page', 'id': 'fd9e8fb3-ecc3-4c62-abb8-631e62ebd6e1', 'created_time': '2023-08-05T03:12:00.000Z', 'last_edited_time': '2023-09-09T22:41:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Xue Bin Peng', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xue Bin Peng', 'href': None}, {'type': 'text', 'text': {'content': ', Kayvon Fatahalian', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ', Kayvon Fatahalian', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'href': None}]}}, 'url': 'https://www.notion.so/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1', 'public_url': 'https://yanxg.notion.site/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1'}, {'object': 'page', 'id': 'd34f0d36-07ec-4c96-a246-3213aa65f9e0', 'created_time': '2023-08-05T03:11:00.000Z', 'last_edited_time': '2023-08-05T03:12:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Synthesizing Physical Character-Scene Interactions', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Synthesizing Physical Character-Scene Interactions', 'href': None}]}}, 'url': 'https://www.notion.so/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0', 'public_url': 'https://yanxg.notion.site/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0'}, {'object': 'page', 'id': '2a5a8d54-c945-4a54-bc8f-ea9130197bc3', 'created_time': '2023-08-05T03:08:00.000Z', 'last_edited_time': '2023-08-05T03:12:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'href': None}]}}, 'url': 'https://www.notion.so/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3', 'public_url': 'https://yanxg.notion.site/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3'}, {'object': 'page', 'id': '59e4ea73-0aa4-4d38-8eb7-88a3428b942e', 'created_time': '2023-06-14T19:18:00.000Z', 'last_edited_time': '2023-06-14T22:27:00.000Z', 'created_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}, 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': \"Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations.\", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': \"Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations.\", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'realisticEditing.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ea8fb772-c58b-4bcd-b06a-ffb4eddbd84b/realisticEditing.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=b79c455d7daea60757ec774ff8c77d2921e007e9eba544b08e3223ce142ff10c&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'link': {'url': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'href': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Realistic Saliency Guided Image Enhancement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Realistic Saliency Guided Image Enhancement', 'href': None}]}}, 'url': 'https://www.notion.so/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e', 'public_url': 'https://yanxg.notion.site/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e'}, {'object': 'page', 'id': '6a1f6f77-f792-415f-a4b9-15a19cda6d15', 'created_time': '2023-05-13T22:38:00.000Z', 'last_edited_time': '2023-06-14T19:28:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'intrinsicFlash.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4ce64201-44b3-4acf-a0e1-02215d5c0add/intrinsicFlash.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=2e9a642806dab99e816c905179ee58cb3cbe0c3b22087eb7cbce64fef20be3c0&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Computational Flash Photography through Intrinsics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Computational Flash Photography through Intrinsics', 'href': None}]}}, 'url': 'https://www.notion.so/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15', 'public_url': 'https://yanxg.notion.site/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15'}, {'object': 'page', 'id': '2ff4a77a-2833-429c-b89c-3c0c2873aeae', 'created_time': '2023-10-25T19:16:00.000Z', 'last_edited_time': '2023-10-25T19:19:00.000Z', 'created_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}, 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'href': None}, {'type': 'text', 'text': {'content': 'https://github.com/mehranagh20/AdaIMLE', 'link': {'url': 'https://github.com/mehranagh20/AdaIMLE'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/mehranagh20/AdaIMLE', 'href': 'https://github.com/mehranagh20/AdaIMLE'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2023-10-25 at 12.18.12 PM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f6dc87d0-a612-4235-a8f1-cd49492d7456/Screenshot_2023-10-25_at_12.18.12_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=146048013392e0b319fe0a270c6653057a9a7bc973c5873594a9c963e4dd7549&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mehranagh20.github.io/AdaIMLE/', 'link': {'url': 'https://mehranagh20.github.io/AdaIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mehranagh20.github.io/AdaIMLE/', 'href': 'https://mehranagh20.github.io/AdaIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/xKt6YYY4hq8', 'link': {'url': 'https://youtu.be/xKt6YYY4hq8'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/xKt6YYY4hq8', 'href': 'https://youtu.be/xKt6YYY4hq8'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'link': {'url': 'https://openreview.net/pdf?id=CNq0JvrDfw'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'href': 'https://openreview.net/pdf?id=CNq0JvrDfw'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'href': None}]}}, 'url': 'https://www.notion.so/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae', 'public_url': 'https://yanxg.notion.site/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae'}, {'object': 'page', 'id': 'c08df0c7-8898-4d63-95ee-20e93f9f5b8f', 'created_time': '2023-05-15T03:18:00.000Z', 'last_edited_time': '2023-10-26T02:10:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'eom-teaser.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4419448b-ae1c-4c46-9acc-f315f86a53fd/eom-teaser.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=939c7fc39f0e71d1d088a148abc90826eefef04e46e77b4bb45f7c0e44cf5571&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://wijmans.xyz/publication/eom/', 'link': {'url': 'https://wijmans.xyz/publication/eom/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://wijmans.xyz/publication/eom/', 'href': 'https://wijmans.xyz/publication/eom/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.13261', 'link': {'url': 'https://arxiv.org/abs/2301.13261'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.13261', 'href': 'https://arxiv.org/abs/2301.13261'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'href': None}]}}, 'url': 'https://www.notion.so/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f', 'public_url': 'https://yanxg.notion.site/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f'}, {'object': 'page', 'id': 'a71873ed-851a-4a82-a39c-d96ac1486e2b', 'created_time': '2023-05-14T00:54:00.000Z', 'last_edited_time': '2023-10-25T19:13:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-24', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\\neration tasks.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\\neration tasks.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/32c28145-f878-45b9-8c86-e4d6cc3c82dd/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=f92948e862298916dbf170c3e9050ec0f68a91dab52c8b85c5e5dfa142919724&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://hnc-cad.github.io/', 'link': {'url': 'https://hnc-cad.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://hnc-cad.github.io/', 'href': 'https://hnc-cad.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'link': {'url': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'href': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2307.00149.pdf', 'link': {'url': 'https://arxiv.org/pdf/2307.00149.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2307.00149.pdf', 'href': 'https://arxiv.org/pdf/2307.00149.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'href': None}]}}, 'url': 'https://www.notion.so/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b', 'public_url': 'https://yanxg.notion.site/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b'}, {'object': 'page', 'id': '4e0e1234-968a-4b5b-af83-3f46de5449b5', 'created_time': '2023-05-14T00:34:00.000Z', 'last_edited_time': '2023-10-26T02:18:00.000Z', 'created_by': {'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Karmesh Yadav,\\xa0Ram Ramrakhya,\\xa0Santhosh Kumar Ramakrishnan,\\xa0Theo Gervet,\\xa0John Turner,\\xa0Aaron Gokaslan,\\xa0Noah Maestre,\\xa0Angel Xuan Chang,\\xa0Dhruv Batra,\\xa0Manolis Savva,\\xa0Alexander William Clegg,\\xa0Devendra Singh Chaplot', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Karmesh Yadav,\\xa0Ram Ramrakhya,\\xa0Santhosh Kumar Ramakrishnan,\\xa0Theo Gervet,\\xa0John Turner,\\xa0Aaron Gokaslan,\\xa0Noah Maestre,\\xa0Angel Xuan Chang,\\xa0Dhruv Batra,\\xa0Manolis Savva,\\xa0Alexander William Clegg,\\xa0Devendra Singh Chaplot', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://angelxuanchang.github.io/files/hm3dsem.png', 'type': 'external', 'external': {'url': 'https://angelxuanchang.github.io/files/hm3dsem.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d-semantics/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d-semantics/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d-semantics/', 'href': 'https://aihabitat.org/datasets/hm3d-semantics/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2210.05633.pdf', 'link': {'url': 'https://arxiv.org/pdf/2210.05633.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2210.05633.pdf', 'href': 'https://arxiv.org/pdf/2210.05633.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Semantics Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Semantics Dataset', 'href': None}]}}, 'url': 'https://www.notion.so/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5', 'public_url': 'https://yanxg.notion.site/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5'}, {'object': 'page', 'id': '09e8bfb7-cc10-4c12-91be-940f77be6a1e', 'created_time': '2023-05-13T23:06:00.000Z', 'last_edited_time': '2023-05-13T23:20:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'aronet[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f445b0e-49d2-48f5-a046-7eede103bfb8/aronet1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=9f7abdb73ddf2f0704636c53292739abf373f535cb1c8335a49ec71c344519ab&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.332Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'href': None}]}}, 'url': 'https://www.notion.so/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e', 'public_url': 'https://yanxg.notion.site/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e'}, {'object': 'page', 'id': 'fb5c2a9b-b3da-4f72-9213-ab918b301345', 'created_time': '2023-05-13T23:03:00.000Z', 'last_edited_time': '2023-06-13T03:49:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/', 'link': {'url': 'https://cuf-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/', 'href': 'https://cuf-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/cuf.pdf', 'link': {'url': 'https://cuf-paper.github.io/cuf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/cuf.pdf', 'href': 'https://cuf-paper.github.io/cuf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CUF: Continuous Upsampling Filters', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CUF: Continuous Upsampling Filters', 'href': None}]}}, 'url': 'https://www.notion.so/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345', 'public_url': 'https://yanxg.notion.site/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345'}, {'object': 'page', 'id': 'd050430b-4429-465d-9ad0-3697d4124f37', 'created_time': '2023-05-13T23:01:00.000Z', 'last_edited_time': '2023-06-13T03:49:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'href': None}]}}, 'url': 'https://www.notion.so/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37', 'public_url': 'https://yanxg.notion.site/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37'}, {'object': 'page', 'id': '14c20053-18fd-405a-bcd5-64ff012adf1c', 'created_time': '2023-05-13T23:01:00.000Z', 'last_edited_time': '2023-06-13T03:49:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'href': None}]}}, 'url': 'https://www.notion.so/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c', 'public_url': 'https://yanxg.notion.site/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c'}, {'object': 'page', 'id': 'c5d52e42-8a31-439a-b18a-110cbaf1692d', 'created_time': '2023-05-13T23:01:00.000Z', 'last_edited_time': '2023-06-13T03:48:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'href': None}]}}, 'url': 'https://www.notion.so/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d', 'public_url': 'https://yanxg.notion.site/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d'}, {'object': 'page', 'id': 'e597ffc2-5b25-4391-b4a1-6f533e25e983', 'created_time': '2023-05-13T22:59:00.000Z', 'last_edited_time': '2023-05-13T23:12:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cvpr23_mobilenerf[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e3ff6c29-1c6e-49bd-aadd-192b8d03620b/cvpr23_mobilenerf1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=00a64d05595622ad75c3a7366f7abfff9134d901f7d1391c71668056f5e9cad1&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mobile-nerf.github.io/', 'link': {'url': 'https://mobile-nerf.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mobile-nerf.github.io/', 'href': 'https://mobile-nerf.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'link': {'url': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'href': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'link': None}, 'annotations': {'bold': True, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'href': None}]}}, 'url': 'https://www.notion.so/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983', 'public_url': 'https://yanxg.notion.site/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983'}, {'object': 'page', 'id': '11d5ea9f-22c9-42a4-80fb-a9ab21e7b616', 'created_time': '2023-05-13T22:56:00.000Z', 'last_edited_time': '2023-05-13T23:11:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-event-camera[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f982027f-5810-4b38-9b08-73b69c219ee9/2023-cvpr-event-camera1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=ef121a1c4115d1ccde96b8b90b63517043052b8deef27a73a80c0b4f0080a8ed&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'href': None}]}}, 'url': 'https://www.notion.so/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616', 'public_url': 'https://yanxg.notion.site/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616'}, {'object': 'page', 'id': '86fb7dc9-1442-41a1-8d90-8291a3e07865', 'created_time': '2023-05-13T22:56:00.000Z', 'last_edited_time': '2023-05-13T23:10:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-house-diffusion[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/af4ac260-ebc3-4317-91ce-27e164dbfee3/2023-cvpr-house-diffusion1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=bd49ba0eb547503e995c0a87a40ac9739ef4eea5efc58cce2bcef366113a6df1&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/aminshabani/house_diffusion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/aminshabani/house_diffusion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.13287'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.13287'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'href': None}]}}, 'url': 'https://www.notion.so/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865', 'public_url': 'https://yanxg.notion.site/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865'}, {'object': 'page', 'id': '73a510c5-02c6-4a27-9b9a-09e875390a45', 'created_time': '2023-05-13T22:38:00.000Z', 'last_edited_time': '2023-05-14T00:08:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-neumap[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/656beba7-f157-4b8a-ad8c-ee8fd1317813/2023-cvpr-neumap1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=792b78e28e6c65f64ce69150a79b7ed03382e9586889fe7f42915b8332b54ddb&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/Tangshitao/NeuMap'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/Tangshitao/NeuMap'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.11177'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.11177'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'href': None}]}}, 'url': 'https://www.notion.so/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45', 'public_url': 'https://yanxg.notion.site/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45'}, {'object': 'page', 'id': '555d2cb2-472e-4c25-bd27-ae0be3ea3524', 'created_time': '2023-06-14T22:40:00.000Z', 'last_edited_time': '2023-06-15T03:14:00.000Z', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-05', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png', 'type': 'external', 'external': {'url': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Transaction of Visualization and Computer Graphics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Transaction of Visualization and Computer Graphics', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SAC-GAN: Structure-Aware Image Composition', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SAC-GAN: Structure-Aware Image Composition', 'href': None}]}}, 'url': 'https://www.notion.so/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524', 'public_url': 'https://yanxg.notion.site/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524'}, {'object': 'page', 'id': 'c9b9e48d-4f8c-48b8-ba80-17396fad3344', 'created_time': '2023-06-14T20:15:00.000Z', 'last_edited_time': '2023-06-14T20:19:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ACCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ACCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1580931b-4de8-4792-97a6-9c715191a9b9', 'name': 'ACCV', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Vectorizing Building Blueprints', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Vectorizing Building Blueprints', 'href': None}]}}, 'url': 'https://www.notion.so/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344', 'public_url': 'https://yanxg.notion.site/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344'}, {'object': 'page', 'id': '89b26044-a63b-46bb-8335-b310c47194f2', 'created_time': '2023-05-15T03:14:00.000Z', 'last_edited_time': '2023-05-15T03:16:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yongsen Mao, Yiming Zhang,\\xa0Hanxiao Jiang,\\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yongsen Mao, Yiming Zhang,\\xa0Hanxiao Jiang,\\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'multiscan.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/27b5a4e7-1a49-4d92-be15-57ac47feaaa6/multiscan.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=f5d7de6ddde860f555b986b8eaccc1f3d84e084a6b08cf5f3bc28e1fc1bcb50e&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multiscan/', 'link': {'url': 'https://3dlg-hcvc.github.io/multiscan/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multiscan/', 'href': 'https://3dlg-hcvc.github.io/multiscan/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'link': {'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'href': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'href': None}]}}, 'url': 'https://www.notion.so/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2', 'public_url': 'https://yanxg.notion.site/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2'}, {'object': 'page', 'id': '62603cb1-eb87-4492-9360-62a4f88afdc3', 'created_time': '2023-10-26T01:45:00.000Z', 'last_edited_time': '2023-10-26T01:52:00.000Z', 'created_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}, 'last_edited_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shichong Peng, Alireza Moazeni, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shichong Peng, Alireza Moazeni, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.gif', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a7cc1543-a9bc-4308-b103-9565c87941c6/teaser.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=10d3c17051647a620f0892ede413cc66cf5878145a6149bf5e4da9eecdd9f31d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://niopeng.github.io/CHIMLE/', 'link': {'url': 'https://niopeng.github.io/CHIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://niopeng.github.io/CHIMLE/', 'href': 'https://niopeng.github.io/CHIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'link': {'url': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'href': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.14286', 'link': {'url': 'https://arxiv.org/abs/2211.14286'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.14286', 'href': 'https://arxiv.org/abs/2211.14286'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'href': None}]}}, 'url': 'https://www.notion.so/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3', 'public_url': 'https://yanxg.notion.site/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3'}, {'object': 'page', 'id': '6acd66a6-a889-4438-bcc7-551da232ee3f', 'created_time': '2023-05-15T03:07:00.000Z', 'last_edited_time': '2023-05-15T03:10:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen, Qirui Wu,\\xa0Matthias Nießner,\\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen, Qirui Wu,\\xa0Matthias Nießner,\\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'd3net.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/dd7d9ce0-1958-445f-8fdd-9bc398c939ac/d3net.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=34cdde968efd24dec305f4e1c99c491c60cfdd617e7b63d9d4b6fc0f1b603e28&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://daveredrum.github.io/D3Net/', 'link': {'url': 'https://daveredrum.github.io/D3Net/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://daveredrum.github.io/D3Net/', 'href': 'https://daveredrum.github.io/D3Net/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.01551', 'link': {'url': 'https://arxiv.org/abs/2112.01551'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.01551', 'href': 'https://arxiv.org/abs/2112.01551'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'href': None}]}}, 'url': 'https://www.notion.so/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f', 'public_url': 'https://yanxg.notion.site/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f'}, {'object': 'page', 'id': '7cc9a376-f7a0-47e6-b725-ee0a6d7550de', 'created_time': '2023-05-15T03:03:00.000Z', 'last_edited_time': '2023-05-15T03:08:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hanxiao Jiang,\\xa0Yongsen Mao, Manolis Savva,\\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hanxiao Jiang,\\xa0Yongsen Mao, Manolis Savva,\\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opd.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0858bf93-e96c-477a-9774-efacd682ce29/opd.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=627a781ed487b8a69f3467bac9b4830b97528f4ab2cca3da9e5dee0e5a5d2f83&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPD/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPD/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPD/', 'href': 'https://3dlg-hcvc.github.io/OPD/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2203.16421', 'link': {'url': 'https://arxiv.org/abs/2203.16421'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2203.16421', 'href': 'https://arxiv.org/abs/2203.16421'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPD: Single-view 3D Openable Part Detection', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPD: Single-view 3D Openable Part Detection', 'href': None}]}}, 'url': 'https://www.notion.so/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de', 'public_url': 'https://yanxg.notion.site/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de'}, {'object': 'page', 'id': '4ba354c2-1b60-493d-97d3-1d9213f4c3df', 'created_time': '2023-06-14T23:20:00.000Z', 'last_edited_time': '2023-06-14T23:58:00.000Z', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Robotics and Automation Letters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Robotics and Automation Letters', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'href': None}]}}, 'url': 'https://www.notion.so/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df', 'public_url': 'https://yanxg.notion.site/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df'}, {'object': 'page', 'id': '315625ea-6b60-4cf6-b1df-990c9c46a53b', 'created_time': '2023-05-15T03:10:00.000Z', 'last_edited_time': '2023-05-15T03:13:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-09-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sanjay Haresh,\\xa0Xiaohao Sun,\\xa0Hanxiao Jiang,\\xa0Angel X. Chang,\\xa0Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sanjay Haresh,\\xa0Xiaohao Sun,\\xa0Hanxiao Jiang,\\xa0Angel X. Chang,\\xa0Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dhoi.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0d1ee7fa-0137-43f3-838a-9fad087503cc/3dhoi.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=9df31597bb5e1085af32d73ba1fb275992cf2eb0beac5a2b04988b63805cdd16&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3dhoi/', 'link': {'url': 'https://3dlg-hcvc.github.io/3dhoi/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3dhoi/', 'href': 'https://3dlg-hcvc.github.io/3dhoi/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2209.05612', 'link': {'url': 'https://arxiv.org/abs/2209.05612'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2209.05612', 'href': 'https://arxiv.org/abs/2209.05612'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'href': None}]}}, 'url': 'https://www.notion.so/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b', 'public_url': 'https://yanxg.notion.site/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b'}, {'object': 'page', 'id': 'd678f388-423a-436a-8fb4-e65ec066498c', 'created_time': '2023-06-14T23:13:00.000Z', 'last_edited_time': '2023-06-14T23:58:00.000Z', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-07-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'link': {'url': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'href': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2205.14313', 'link': {'url': 'https://arxiv.org/abs/2205.14313'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2205.14313', 'href': 'https://arxiv.org/abs/2205.14313'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'href': None}]}}, 'url': 'https://www.notion.so/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c', 'public_url': 'https://yanxg.notion.site/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c'}, {'object': 'page', 'id': 'c98548ee-e3c8-4015-8b44-f2af87f8c61f', 'created_time': '2023-05-15T02:59:00.000Z', 'last_edited_time': '2023-05-15T03:08:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-31', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yasaman Etesam, Leon Kochiev,\\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yasaman Etesam, Leon Kochiev,\\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dvqa.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9525ee57-f626-43f5-a153-603c609aa764/3dvqa.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=0949ea87c978b936d07e15accc87649c2870d2f134d97b7a395e8faa1e3c870d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3DVQA/', 'link': {'url': 'https://3dlg-hcvc.github.io/3DVQA/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3DVQA/', 'href': 'https://3dlg-hcvc.github.io/3DVQA/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CRV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CRV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '8492aa9e-2202-4887-9296-8c313873f664', 'name': 'CRV', 'color': 'green'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ieeexplore.ieee.org/document/9866910', 'link': {'url': 'https://ieeexplore.ieee.org/document/9866910'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ieeexplore.ieee.org/document/9866910', 'href': 'https://ieeexplore.ieee.org/document/9866910'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': '3DVQA: Visual Question Answering for 3D Environments', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DVQA: Visual Question Answering for 3D Environments', 'href': None}]}}, 'url': 'https://www.notion.so/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f', 'public_url': 'https://yanxg.notion.site/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f'}, {'object': 'page', 'id': '93aea945-8b99-4fa3-aaef-ae5b7d9e943e', 'created_time': '2023-05-14T00:50:00.000Z', 'last_edited_time': '2023-06-15T05:34:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cad_rand[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8f4caa5c-6a35-406b-aed4-8a8595b252ec/cad_rand1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=3aa5970dce1353a9ff9838f8dd94fbe37e893a84b62597195fbc25d56d4a0137&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://samxuxiang.github.io/skexgen/index.html', 'link': {'url': 'https://samxuxiang.github.io/skexgen/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://samxuxiang.github.io/skexgen/index.html', 'href': 'https://samxuxiang.github.io/skexgen/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2207.04632', 'link': {'url': 'https://arxiv.org/abs/2207.04632'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2207.04632', 'href': 'https://arxiv.org/abs/2207.04632'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'href': None}]}}, 'url': 'https://www.notion.so/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e', 'public_url': 'https://yanxg.notion.site/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e'}, {'object': 'page', 'id': '7d93afe8-9aea-4cba-b951-e14e6849e611', 'created_time': '2023-06-14T20:19:00.000Z', 'last_edited_time': '2023-06-14T20:21:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-04-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://heat-structured-reconstruction.github.io/', 'link': {'url': 'https://heat-structured-reconstruction.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://heat-structured-reconstruction.github.io/', 'href': 'https://heat-structured-reconstruction.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2111.15143', 'link': {'url': 'https://arxiv.org/abs/2111.15143'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2111.15143', 'href': 'https://arxiv.org/abs/2111.15143'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'href': None}]}}, 'url': 'https://www.notion.so/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611', 'public_url': 'https://yanxg.notion.site/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611'}, {'object': 'page', 'id': 'a0971bb6-97d2-4960-a91b-747093d53f8b', 'created_time': '2023-06-14T22:47:00.000Z', 'last_edited_time': '2023-06-15T03:14:00.000Z', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://qiminchen.github.io/unist/images/teaser.svg', 'type': 'external', 'external': {'url': 'https://qiminchen.github.io/unist/images/teaser.svg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/unist/', 'link': {'url': 'https://qiminchen.github.io/unist/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/unist/', 'href': 'https://qiminchen.github.io/unist/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'link': {'url': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'href': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'href': None}]}}, 'url': 'https://www.notion.so/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b', 'public_url': 'https://yanxg.notion.site/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b'}, {'object': 'page', 'id': '386b9443-f01c-4dbe-b712-18d7b1910e35', 'created_time': '2023-06-14T20:21:00.000Z', 'last_edited_time': '2023-06-14T20:22:00.000Z', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICRA 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICRA 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2d02ae48-7ae4-4c33-9b05-fb8723245ad7', 'name': 'ICRA', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Single User WiFi Structure from Motion in the Wild', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single User WiFi Structure from Motion in the Wild', 'href': None}]}}, 'url': 'https://www.notion.so/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35', 'public_url': 'https://yanxg.notion.site/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35'}, {'object': 'page', 'id': '23711644-bc80-4dc8-b9f6-5a112d0a4200', 'created_time': '2023-05-15T03:24:00.000Z', 'last_edited_time': '2023-06-14T19:19:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hm3d.jpeg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c4db31f4-116e-4d67-8189-a9eb0e7a1e5e/hm3d.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=3f17eeb5d29c6b994d2f805521e850a9190747f793f055633df24cce34c16400&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.334Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d/', 'href': 'https://aihabitat.org/datasets/hm3d/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.08238', 'link': {'url': 'https://arxiv.org/abs/2109.08238'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.08238', 'href': 'https://arxiv.org/abs/2109.08238'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'href': None}]}}, 'url': 'https://www.notion.so/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200', 'public_url': 'https://yanxg.notion.site/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200'}, {'object': 'page', 'id': 'bcca4a74-3f90-456c-b372-a45393aeb3ca', 'created_time': '2023-05-15T03:21:00.000Z', 'last_edited_time': '2023-05-15T03:27:00.000Z', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hab2.jpeg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b5c2b23a-6b97-45c9-bd38-8ff135a65906/hab2.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=d61bc520e18ecce4217cabcb5593d1ed93652b1315cd65681443a8914650d65a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.333Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/', 'link': {'url': 'https://aihabitat.org/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/', 'href': 'https://aihabitat.org/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.14405', 'link': {'url': 'https://arxiv.org/abs/2106.14405'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.14405', 'href': 'https://arxiv.org/abs/2106.14405'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'href': None}]}}, 'url': 'https://www.notion.so/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca', 'public_url': 'https://yanxg.notion.site/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca'}, {'object': 'page', 'id': '772535f6-7463-4bd9-a12c-36c8030b9b9e', 'created_time': '2023-06-14T22:36:00.000Z', 'last_edited_time': '2023-06-15T03:13:00.000Z', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-06-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Himanshu Arora,\\xa0Saurabh Mishra,\\xa0Shichong Peng,\\xa0Ke Li,\\xa0Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Himanshu Arora,\\xa0Saurabh Mishra,\\xa0Shichong Peng,\\xa0Ke Li,\\xa0Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-06-14_20-13-32.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5610ebd8-2712-46d0-93e6-c42a6e3336ac/Snipaste_2023-06-14_20-13-32.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T043502Z&X-Amz-Expires=3600&X-Amz-Signature=bc35de5d59772770c50e642c1303793acd89a704772793a072a1bffeccfc2ff7&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T05:35:02.335Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multimodal Shape Completion via IMLE', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multimodal Shape Completion via IMLE', 'href': None}]}}, 'url': 'https://www.notion.so/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e', 'public_url': 'https://yanxg.notion.site/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e'}, {'object': 'page', 'id': '87e25aef-ee38-4efe-ab82-5ad3b3edaebb', 'created_time': '2023-06-14T23:59:00.000Z', 'last_edited_time': '2023-06-15T00:02:00.000Z', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-05-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://arpspoof.github.io/project/jump/teaser.png', 'type': 'external', 'external': {'url': 'https://arpspoof.github.io/project/jump/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arpspoof.github.io/project/jump/jump.html', 'link': {'url': 'https://arpspoof.github.io/project/jump/jump.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arpspoof.github.io/project/jump/jump.html', 'href': 'https://arpspoof.github.io/project/jump/jump.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'link': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'href': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Discovering Diverse Athletic Jumping Strategies', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Discovering Diverse Athletic Jumping Strategies', 'href': None}]}}, 'url': 'https://www.notion.so/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb', 'public_url': 'https://yanxg.notion.site/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb'}, {'object': 'page', 'id': '8e49a3c8-3bc1-4910-ab61-559e02920f96', 'created_time': '2023-06-14T22:50:00.000Z', 'last_edited_time': '2023-06-15T03:12:00.000Z', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}, 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}, 'archived': False, 'properties': {'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-04-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://fenggenyu.github.io/images/capri/teaser.png', 'type': 'external', 'external': {'url': 'https://fenggenyu.github.io/images/capri/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://fenggenyu.github.io/capri.html', 'link': {'url': 'https://fenggenyu.github.io/capri.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://fenggenyu.github.io/capri.html', 'href': 'https://fenggenyu.github.io/capri.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2104.05652.pdf', 'link': {'url': 'https://arxiv.org/pdf/2104.05652.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2104.05652.pdf', 'href': 'https://arxiv.org/pdf/2104.05652.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'href': None}]}}, 'url': 'https://www.notion.so/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96', 'public_url': 'https://yanxg.notion.site/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96'}], 'next_cursor': None, 'has_more': False, 'type': 'page_or_database', 'page_or_database': {}, 'request_id': '7dd2ca85-7ebd-4dc4-9e78-ed6bd3747fcc'}\n",
      "57\n",
      "False None None\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def fetch_notion_db(query_url, headers, query_data):\n",
    "    query_data = copy.deepcopy(query_data)\n",
    "    result_list = []\n",
    "    counter = 0\n",
    "    #query_data[\"start_cursor\"] = \"undefined\"\n",
    "    while True:\n",
    "        print(counter)\n",
    "        res = json.loads( requests.post(query_url, headers=headers, data=json.dumps(query_data)).content.decode() )\n",
    "        #print(res)\n",
    "        has_more = res.get(\"has_more\")\n",
    "        result_list = result_list + res[\"results\"]\n",
    "        print(len(result_list))\n",
    "        print(has_more, res[\"next_cursor\"], query_data.get(\"start_cursor\", None))\n",
    "        if has_more is None or has_more==False:\n",
    "            break\n",
    "        query_data[\"start_cursor\"] = res[\"next_cursor\"]\n",
    "        #query_data[\"start_cursor\"] = res[\"next_cursor\"]\n",
    "        counter+=1\n",
    "    return result_list\n",
    "\n",
    "datalist = fetch_notion_db(query_url, headers, query_data)\n",
    "df = pd.DataFrame(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding\n",
      "Downloading thumbnail 2023-10-02_96b1c296-9893-4784-801a-f0daed0f43ef.jpg from https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/412818c8-c973-4209-b9a6-8cad6cbdf2c4/unit3d.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=d2238480ad42e07cda09dcfd62ba3ccbebee5b48c027e837070299b1474e0793&X-Amz-SignedHeaders=host&x-id=GetObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling\n",
      "Downloading thumbnail 2023-10-02_97a335a8-5de9-4af8-97fe-b8cdbbc02c06.png from https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png\n",
      "2023-10-02 PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects\n",
      "Downloading thumbnail 2023-10-02_c34ba525-ac8d-4b9b-91e7-d412e7fce348.png from https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/93befd22-b013-4613-bee2-c9e2bb53c1a5/Snipaste_2023-09-18_14-56-16.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=14881fcec7a66dd4282c4a932b54ab38d73494515e660d9eeeb980932d03e693&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-10-02 SKED: Sketch-guided Text-based 3D Editing\n",
      "Downloading thumbnail 2023-10-02_76c3667b-3a91-4d35-94e6-0d33b0b6a293.png from https://sked-paper.github.io/resources/teaser.png\n",
      "2023-10-02 DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion\n",
      "Downloading thumbnail 2023-10-02_40055d39-bb6d-44ea-8d68-453f503ef32d.png from https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png\n",
      "2023-10-02 Multi3DRefer: Grounding Text Description to Multiple 3D Objects\n",
      "Downloading thumbnail 2023-10-02_770d6d8f-f95b-46d8-8cab-db33928836de.jpg from https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/aaeef2a9-fa43-468e-a201-2ed47a1c9214/abstract-b1185106.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=2d9b02e618e3fbf0a46b974959aeda575efd45732727cd708307970e779f03c6&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-07-01 Learning Physically Simulated Tennis Skills from Broadcast Videos\n",
      "Downloading thumbnail 2023-07-01_fd9e8fb3-ecc3-4c62-abb8-631e62ebd6e1.png from https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png\n",
      "2023-07-01 Synthesizing Physical Character-Scene Interactions\n",
      "Downloading thumbnail 2023-07-01_d34f0d36-07ec-4c96-a246-3213aa65f9e0.png from https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png\n",
      "2023-07-01 CALM: Conditional Adversarial Latent Models for Directable Virtual Characters\n",
      "Downloading thumbnail 2023-07-01_2a5a8d54-c945-4a54-bc8f-ea9130197bc3.png from https://xbpeng.github.io/projects/CALM/calm_thumb.png\n",
      "2023-06-14 Realistic Saliency Guided Image Enhancement\n",
      "Downloading thumbnail 2023-06-14_59e4ea73-0aa4-4d38-8eb7-88a3428b942e.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ea8fb772-c58b-4bcd-b06a-ffb4eddbd84b/realisticEditing.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=c774b88d39a9c214bdf2b3983129fe86d3fee444466ff79f76f8961ef1814ccb&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-06-14 Computational Flash Photography through Intrinsics\n",
      "Downloading thumbnail 2023-06-14_6a1f6f77-f792-415f-a4b9-15a19cda6d15.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4ce64201-44b3-4acf-a0e1-02215d5c0add/intrinsicFlash.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=ef7cc925e41badb680c172ba990d60270d3afe7228a5a94cc95255b3ad634d0a&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-05-01 Emergence of Maps in the Memories of Blind Navigation Agents\n",
      "Downloading thumbnail 2023-05-01_c08df0c7-8898-4d63-95ee-20e93f9f5b8f.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4419448b-ae1c-4c46-9acc-f315f86a53fd/eom-teaser.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=33642b1598966c7697ee02fc550e61f3267abc8648060a167db28fc80284764c&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-24 Hierarchical Neural Coding for Controllable CAD Model Generation\n",
      "Downloading thumbnail 2023-04-24_a71873ed-851a-4a82-a39c-d96ac1486e2b.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/32c28145-f878-45b9-8c86-e4d6cc3c82dd/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=ce7f6ef7032727cb91ac71bcd482d70d4b635882dcd57a99ff4db5b935e4632f&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-01 Habitat-Matterport 3D Semantics Dataset\n",
      "Downloading thumbnail 2023-04-01_4e0e1234-968a-4b5b-af83-3f46de5449b5.png from https://angelxuanchang.github.io/files/hm3dsem.png\n",
      "2023-04-01 ARO-Net: Learning Implicit Fields from Anchored Radial Observations\n",
      "Downloading thumbnail 2023-04-01_09e8bfb7-cc10-4c12-91be-940f77be6a1e.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f445b0e-49d2-48f5-a046-7eede103bfb8/aronet1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=ced7ab6a30269e4a68fbb37c6f4944ad9472751ac29c4ded15527b1d36ce644f&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-01 CUF: Continuous Upsampling Filters\n",
      "Downloading thumbnail 2023-04-01_fb5c2a9b-b3da-4f72-9213-ab918b301345.jpg from https://taiya.github.io/images/placeholder.jpg\n",
      "2023-04-01 OpenScene: 3D Scene Understanding with Open Vocabularies\n",
      "Downloading thumbnail 2023-04-01_d050430b-4429-465d-9ad0-3697d4124f37.jpg from https://taiya.github.io/images/placeholder.jpg\n",
      "2023-04-01 SparsePose: Sparse-View Camera Pose Regression and Refinement\n",
      "Downloading thumbnail 2023-04-01_14c20053-18fd-405a-bcd5-64ff012adf1c.jpg from https://taiya.github.io/images/placeholder.jpg\n",
      "2023-04-01 BlendFields: Few-Shot Example-Driven Facial Modeling\n",
      "Downloading thumbnail 2023-04-01_c5d52e42-8a31-439a-b18a-110cbaf1692d.jpg from https://taiya.github.io/images/placeholder.jpg\n",
      "2023-04-01 MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures\n",
      "Downloading thumbnail 2023-04-01_e597ffc2-5b25-4391-b4a1-6f533e25e983.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e3ff6c29-1c6e-49bd-aadd-192b8d03620b/cvpr23_mobilenerf1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=a5bc3621a888c2012df4abc51aef132a2164fcf139d8632c9abae58f277bbd3f&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-01 Hierarchical Neural Memory Network for Low Latency Event Processing\n",
      "Downloading thumbnail 2023-04-01_11d5ea9f-22c9-42a4-80fb-a9ab21e7b616.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f982027f-5810-4b38-9b08-73b69c219ee9/2023-cvpr-event-camera1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=98e1ba7c0b9e1d4b41583aee7fab975c8936ba06b421abd1994e8bc3e83af125&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-01 HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising\n",
      "Downloading thumbnail 2023-04-01_86fb7dc9-1442-41a1-8d90-8291a3e07865.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/af4ac260-ebc3-4317-91ce-27e164dbfee3/2023-cvpr-house-diffusion1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=19eb356ce5c229c8a5d1d8bc9a288212583a1d0e4964f73cefc542ba2bb84b9c&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2023-04-01 NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization\n",
      "Downloading thumbnail 2023-04-01_73a510c5-02c6-4a27-9b9a-09e875390a45.jpg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/656beba7-f157-4b8a-ad8c-ee8fd1317813/2023-cvpr-neumap1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=5eab54eadd99b8c4d82cc999f7d1639367695d99392a983a72e225d7f85e5408&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-12-05 SAC-GAN: Structure-Aware Image Composition\n",
      "Downloading thumbnail 2022-12-05_555d2cb2-472e-4c25-bd27-ae0be3ea3524.png from https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png\n",
      "2022-12-01 Vectorizing Building Blueprints\n",
      "Downloading thumbnail 2022-12-01_c9b9e48d-4f8c-48b8-ba80-17396fad3344.jpg from https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg\n",
      "2022-11-29 MultiScan: Scalable RGBD scanning for 3D environments with articulated objects\n",
      "Downloading thumbnail 2022-11-29_89b26044-a63b-46bb-8335-b310c47194f2.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/27b5a4e7-1a49-4d92-be15-57ac47feaaa6/multiscan.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=23c781ef341d55c8c767494c5dfb7da604ebdcde23e36474cd39affdf09144bb&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-10-25 D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding\n",
      "Downloading thumbnail 2022-10-25_6acd66a6-a889-4438-bcc7-551da232ee3f.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/dd7d9ce0-1958-445f-8fdd-9bc398c939ac/d3net.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=0d899d2a77f8324a424ae09533c96bc5f884b04235401e782b1453ec1efc78c2&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-10-25 OPD: Single-view 3D Openable Part Detection\n",
      "Downloading thumbnail 2022-10-25_7cc9a376-f7a0-47e6-b725-ee0a6d7550de.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0858bf93-e96c-477a-9774-efacd682ce29/opd.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=ff9b894fa642bae37f19fc186ad01db2001413818fbf3511ee055a221abc650a&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-10-01 Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps\n",
      "Downloading thumbnail 2022-10-01_4ba354c2-1b60-493d-97d3-1d9213f4c3df.jpg from https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg\n",
      "2022-09-12 Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges\n",
      "Downloading thumbnail 2022-09-12_315625ea-6b60-4cf6-b1df-990c9c46a53b.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0d1ee7fa-0137-43f3-838a-9fad087503cc/3dhoi.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=7c81df95528a48fd9be44e32b03919df19f80c0b56f460d403b8a171617f1995&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-07-30 Learning to Use Chopsticks in Diverse Gripping Styles\n",
      "Downloading thumbnail 2022-07-30_d678f388-423a-436a-8fb4-e65ec066498c.jpg from https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg\n",
      "2022-05-31 3DVQA: Visual Question Answering for 3D Environments\n",
      "Downloading thumbnail 2022-05-31_c98548ee-e3c8-4015-8b44-f2af87f8c61f.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9525ee57-f626-43f5-a153-603c609aa764/3dvqa.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=a94ae637758124dce278baa268c5c6f32ebe0e190eafbd0225e44cad8cc441cc&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-05-14 SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks\n",
      "Downloading thumbnail 2022-05-14_93aea945-8b99-4fa3-aaef-ae5b7d9e943e.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8f4caa5c-6a35-406b-aed4-8a8595b252ec/cad_rand1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=52f34d0bea19405cb27ad0108b7f0212c7ad3d425d6c4295cb6f785c2a2e75db&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2022-04-14 HEAT: Holistic Edge Attention Transformer for Structured Reconstruction\n",
      "Downloading thumbnail 2022-04-14_7d93afe8-9aea-4cba-b951-e14e6849e611.jpg from https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg\n",
      "2022-03-30 UNIST: Unpaired Neural Implicit Shape Translation Network\n",
      "Downloading thumbnail 2022-03-30_a0971bb6-97d2-4960-a91b-747093d53f8b.svg from https://qiminchen.github.io/unist/images/teaser.svg\n",
      "2022-03-14 Single User WiFi Structure from Motion in the Wild\n",
      "Downloading thumbnail 2022-03-14_386b9443-f01c-4dbe-b712-18d7b1910e35.jpg from https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg\n",
      "2021-12-07 Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI\n",
      "Downloading thumbnail 2021-12-07_23711644-bc80-4dc8-b9f6-5a112d0a4200.jpeg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c4db31f4-116e-4d67-8189-a9eb0e7a1e5e/hm3d.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=fb99f8db47ba9fd00c63473c2e84c940930f69fb06a199e2deef8d6a746fcd91&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2021-12-07 Habitat 2.0: Training Home Assistants to Rearrange their Habitat\n",
      "Downloading thumbnail 2021-12-07_bcca4a74-3f90-456c-b372-a45393aeb3ca.jpeg from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b5c2b23a-6b97-45c9-bd38-8ff135a65906/hab2.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=a6cd6a5b4872df8a7396bc722553c080ca5d915f74676c01418ef99624e18e68&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2021-06-30 Multimodal Shape Completion via IMLE\n",
      "Downloading thumbnail 2021-06-30_772535f6-7463-4bd9-a12c-36c8030b9b9e.png from https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5610ebd8-2712-46d0-93e6-c42a6e3336ac/Snipaste_2023-06-14_20-13-32.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230925%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230925T234532Z&X-Amz-Expires=3600&X-Amz-Signature=87be857af13e740fc7b86de6e41a39b850dd3b58ed2fb3221c7ed836e46d4249&X-Amz-SignedHeaders=host&x-id=GetObject\n",
      "2021-05-02 Discovering Diverse Athletic Jumping Strategies\n",
      "Downloading thumbnail 2021-05-02_87e25aef-ee38-4efe-ab82-5ad3b3edaebb.png from https://arpspoof.github.io/project/jump/teaser.png\n",
      "2021-04-12 CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly\n",
      "Downloading thumbnail 2021-04-12_8e49a3c8-3bc1-4910-ab61-559e02920f96.png from https://fenggenyu.github.io/images/capri/teaser.png\n",
      "\n",
      "- title: >-\n",
      "    UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding\n",
      "  image: 2023-10-02_96b1c296-9893-4784-801a-f0daed0f43ef.jpg\n",
      "  description: >-\n",
      "  authors: Dave Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner, Angel X. Chang\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2212.00836\n",
      "  project_page: https://arxiv.org/abs/2212.00836\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling\n",
      "  image: 2023-10-02_97a335a8-5de9-4af8-97fe-b8cdbbc02c06.png\n",
      "  description: >-\n",
      "  authors: Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2301.10460\n",
      "  project_page: https://arxiv.org/abs/2301.10460\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects\n",
      "  image: 2023-10-02_c34ba525-ac8d-4b9b-91e7-d412e7fce348.png\n",
      "  description: >-\n",
      "  authors: Jiayi Liu, Ali Mahdavi-Amiri, Manolis Savva\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2303.10735\n",
      "  project_page: https://3dlg-hcvc.github.io/paris/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    SKED: Sketch-guided Text-based 3D Editing\n",
      "  image: 2023-10-02_76c3667b-3a91-4d35-94e6-0d33b0b6a293.png\n",
      "  description: >-\n",
      "  authors: Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2303.10735v3\n",
      "  project_page: https://sked-paper.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion\n",
      "  image: 2023-10-02_40055d39-bb6d-44ea-8d68-453f503ef32d.png\n",
      "  description: >-\n",
      "  authors: Maham Tanveer, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://ds-fusion.github.io/\n",
      "  project_page: https://ds-fusion.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Multi3DRefer: Grounding Text Description to Multiple 3D Objects\n",
      "  image: 2023-10-02_770d6d8f-f95b-46d8-8cab-db33928836de.jpg\n",
      "  description: >-\n",
      "  authors: Yiming Zhang, ZeMing Gong, Angel X. Chang\n",
      "  venue: >-\n",
      "    ICCV 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2309.05251\n",
      "  project_page: https://3dlg-hcvc.github.io/multi3drefer/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Learning Physically Simulated Tennis Skills from Broadcast Videos\n",
      "  image: 2023-07-01_fd9e8fb3-ecc3-4c62-abb8-631e62ebd6e1.png\n",
      "  description: >-\n",
      "  authors: Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, Kayvon Fatahalian\n",
      "  venue: >-\n",
      "    SIGGRAPH 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf\n",
      "  project_page: https://xbpeng.github.io/projects/Vid2Player3D/index.html\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Synthesizing Physical Character-Scene Interactions\n",
      "  image: 2023-07-01_d34f0d36-07ec-4c96-a246-3213aa65f9e0.png\n",
      "  description: >-\n",
      "  authors: Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng\n",
      "  venue: >-\n",
      "    SIGGRAPH 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf\n",
      "  project_page: https://xbpeng.github.io/projects/CALM/index.html\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    CALM: Conditional Adversarial Latent Models for Directable Virtual Characters\n",
      "  image: 2023-07-01_2a5a8d54-c945-4a54-bc8f-ea9130197bc3.png\n",
      "  description: >-\n",
      "  authors: Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng\n",
      "  venue: >-\n",
      "    SIGGRAPH 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf\n",
      "  project_page: https://xbpeng.github.io/projects/CALM/index.html\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Realistic Saliency Guided Image Enhancement\n",
      "  image: 2023-06-14_59e4ea73-0aa4-4d38-8eb7-88a3428b942e.jpg\n",
      "  description: >-\n",
      "  authors: S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: http://yaksoy.github.io/realisticEditing/\n",
      "  project_page: http://yaksoy.github.io/realisticEditing/\n",
      "  video: https://www.youtube.com/watch?v=5dKUDMnnjuo\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Computational Flash Photography through Intrinsics\n",
      "  image: 2023-06-14_6a1f6f77-f792-415f-a4b9-15a19cda6d15.jpg\n",
      "  description: >-\n",
      "  authors: Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: http://yaksoy.github.io/intrinsicFlash/\n",
      "  project_page: http://yaksoy.github.io/intrinsicFlash/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Emergence of Maps in the Memories of Blind Navigation Agents\n",
      "  image: 2023-05-01_c08df0c7-8898-4d63-95ee-20e93f9f5b8f.jpg\n",
      "  description: >-\n",
      "  authors: Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra\n",
      "  venue: >-\n",
      "    ICLR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2301.13261\n",
      "  project_page: https://wijmans.xyz/publication/eom/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Hierarchical Neural Coding for Controllable CAD Model Generation\n",
      "  image: 2023-04-24_a71873ed-851a-4a82-a39c-d96ac1486e2b.png\n",
      "  description: >-\n",
      "  authors: Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    ICML 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/pdf/2307.00149.pdf\n",
      "  project_page: https://hnc-cad.github.io/\n",
      "  video: https://www.youtube.com/watch?v=1XVUJIKioO4\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Habitat-Matterport 3D Semantics Dataset\n",
      "  image: 2023-04-01_4e0e1234-968a-4b5b-af83-3f46de5449b5.png\n",
      "  description: >-\n",
      "  authors: Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre, Angel Xuan Chang, Dhruv Batra, Manolis Savva, Alexander William Clegg, Devendra Singh Chaplot\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/pdf/2210.05633.pdf\n",
      "  project_page: https://aihabitat.org/datasets/hm3d-semantics/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    ARO-Net: Learning Implicit Fields from Anchored Radial Observations\n",
      "  image: 2023-04-01_09e8bfb7-cc10-4c12-91be-940f77be6a1e.png\n",
      "  description: >-\n",
      "  authors: Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2212.10275\n",
      "  project_page: https://arxiv.org/abs/2212.10275\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    CUF: Continuous Upsampling Filters\n",
      "  image: 2023-04-01_fb5c2a9b-b3da-4f72-9213-ab918b301345.jpg\n",
      "  description: >-\n",
      "  authors: Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://cuf-paper.github.io/cuf.pdf\n",
      "  project_page: https://cuf-paper.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    OpenScene: 3D Scene Understanding with Open Vocabularies\n",
      "  image: 2023-04-01_d050430b-4429-465d-9ad0-3697d4124f37.jpg\n",
      "  description: >-\n",
      "  authors: Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2211.15654\n",
      "  project_page: https://arxiv.org/abs/2211.15654\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    SparsePose: Sparse-View Camera Pose Regression and Refinement\n",
      "  image: 2023-04-01_14c20053-18fd-405a-bcd5-64ff012adf1c.jpg\n",
      "  description: >-\n",
      "  authors: Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://taiya.github.io/\n",
      "  project_page: https://taiya.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    BlendFields: Few-Shot Example-Driven Facial Modeling\n",
      "  image: 2023-04-01_c5d52e42-8a31-439a-b18a-110cbaf1692d.jpg\n",
      "  description: >-\n",
      "  authors: Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://taiya.github.io/\n",
      "  project_page: https://taiya.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures\n",
      "  image: 2023-04-01_e597ffc2-5b25-4391-b4a1-6f533e25e983.png\n",
      "  description: >-\n",
      "  authors: Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://taiya.github.io/pubs/chen2023mobilenerf.pdf\n",
      "  project_page: https://mobile-nerf.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Hierarchical Neural Memory Network for Low Latency Event Processing\n",
      "  image: 2023-04-01_11d5ea9f-22c9-42a4-80fb-a9ab21e7b616.jpg\n",
      "  description: >-\n",
      "  authors: Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://www.cs.sfu.ca/~furukawa/\n",
      "  project_page: https://www.cs.sfu.ca/~furukawa/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising\n",
      "  image: 2023-04-01_86fb7dc9-1442-41a1-8d90-8291a3e07865.jpg\n",
      "  description: >-\n",
      "  authors: Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2211.11177\n",
      "  project_page: https://github.com/Tangshitao/NeuMap\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization\n",
      "  image: 2023-04-01_73a510c5-02c6-4a27-9b9a-09e875390a45.jpg\n",
      "  description: >-\n",
      "  authors: Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    CVPR 2023\n",
      "  year: 2023\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2211.11177\n",
      "  project_page: https://github.com/Tangshitao/NeuMap\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    SAC-GAN: Structure-Aware Image Composition\n",
      "  image: 2022-12-05_555d2cb2-472e-4c25-bd27-ae0be3ea3524.png\n",
      "  description: >-\n",
      "  authors: Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao, Ali Mahdavi-Amiri, and Hao Zhang\n",
      "  venue: >-\n",
      "    IEEE Transaction of Visualization and Computer Graphics\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2112.06596\n",
      "  project_page: https://arxiv.org/abs/2112.06596\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Vectorizing Building Blueprints\n",
      "  image: 2022-12-01_c9b9e48d-4f8c-48b8-ba80-17396fad3344.jpg\n",
      "  description: >-\n",
      "  authors: Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    ACCV 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view\n",
      "  project_page: https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    MultiScan: Scalable RGBD scanning for 3D environments with articulated objects\n",
      "  image: 2022-11-29_89b26044-a63b-46bb-8335-b310c47194f2.png\n",
      "  description: >-\n",
      "  authors: Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel X. Chang, Manolis Savva\n",
      "  venue: >-\n",
      "    NeurIPS 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf\n",
      "  project_page: https://3dlg-hcvc.github.io/multiscan/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding\n",
      "  image: 2022-10-25_6acd66a6-a889-4438-bcc7-551da232ee3f.png\n",
      "  description: >-\n",
      "  authors: Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, Angel X. Chang\n",
      "  venue: >-\n",
      "    ECCV 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2112.01551\n",
      "  project_page: https://daveredrum.github.io/D3Net/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    OPD: Single-view 3D Openable Part Detection\n",
      "  image: 2022-10-25_7cc9a376-f7a0-47e6-b725-ee0a6d7550de.png\n",
      "  description: >-\n",
      "  authors: Hanxiao Jiang, Yongsen Mao, Manolis Savva, Angel X. Chang\n",
      "  venue: >-\n",
      "    ECCV 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2203.16421\n",
      "  project_page: https://3dlg-hcvc.github.io/OPD/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps\n",
      "  image: 2022-10-01_4ba354c2-1b60-493d-97d3-1d9213f4c3df.jpg\n",
      "  description: >-\n",
      "  authors: M.Mahdavian, KangKang Yin, and Mo Chen\n",
      "  venue: >-\n",
      "    IEEE Robotics and Automation Letters\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2109.10445\n",
      "  project_page: https://arxiv.org/abs/2109.10445\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges\n",
      "  image: 2022-09-12_315625ea-6b60-4cf6-b1df-990c9c46a53b.png\n",
      "  description: >-\n",
      "  authors: Sanjay Haresh, Xiaohao Sun, Hanxiao Jiang, Angel X. Chang, Manolis Savva\n",
      "  venue: >-\n",
      "    3DV 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2209.05612\n",
      "  project_page: https://3dlg-hcvc.github.io/3dhoi/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Learning to Use Chopsticks in Diverse Gripping Styles\n",
      "  image: 2022-07-30_d678f388-423a-436a-8fb4-e65ec066498c.jpg\n",
      "  description: >-\n",
      "  authors: Zeshi Yang, KangKang Yin, and Libin Liu\n",
      "  venue: >-\n",
      "    SIGGRAPH 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2205.14313\n",
      "  project_page: https://github.com/chopsticks-research2022/learning2usechopsticks\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    3DVQA: Visual Question Answering for 3D Environments\n",
      "  image: 2022-05-31_c98548ee-e3c8-4015-8b44-f2af87f8c61f.png\n",
      "  description: >-\n",
      "  authors: Yasaman Etesam, Leon Kochiev, Angel X. Chang\n",
      "  venue: >-\n",
      "    CRV 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://ieeexplore.ieee.org/document/9866910\n",
      "  project_page: https://3dlg-hcvc.github.io/3DVQA/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks\n",
      "  image: 2022-05-14_93aea945-8b99-4fa3-aaef-ae5b7d9e943e.png\n",
      "  description: >-\n",
      "  authors: Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    ICML\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2207.04632\n",
      "  project_page: https://samxuxiang.github.io/skexgen/index.html\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    HEAT: Holistic Edge Attention Transformer for Structured Reconstruction\n",
      "  image: 2022-04-14_7d93afe8-9aea-4cba-b951-e14e6849e611.jpg\n",
      "  description: >-\n",
      "  authors: Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    CVPR 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2111.15143\n",
      "  project_page: https://heat-structured-reconstruction.github.io/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    UNIST: Unpaired Neural Implicit Shape Translation Network\n",
      "  image: 2022-03-30_a0971bb6-97d2-4960-a91b-747093d53f8b.svg\n",
      "  description: >-\n",
      "  authors: Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, Hao Zhang\n",
      "  venue: >-\n",
      "    CVPR 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf\n",
      "  project_page: https://qiminchen.github.io/unist/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Single User WiFi Structure from Motion in the Wild\n",
      "  image: 2022-03-14_386b9443-f01c-4dbe-b712-18d7b1910e35.jpg\n",
      "  description: >-\n",
      "  authors: Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa\n",
      "  venue: >-\n",
      "    ICRA 2022\n",
      "  year: 2022\n",
      "  highlight: 1\n",
      "  pdf: https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing\n",
      "  project_page: https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI\n",
      "  image: 2021-12-07_23711644-bc80-4dc8-b9f6-5a112d0a4200.jpeg\n",
      "  description: >-\n",
      "  authors: Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra\n",
      "  venue: >-\n",
      "    NeurIPS Datasets and Benchmarks Track 2021\n",
      "  year: 2021\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2109.08238\n",
      "  project_page: https://aihabitat.org/datasets/hm3d/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Habitat 2.0: Training Home Assistants to Rearrange their Habitat\n",
      "  image: 2021-12-07_bcca4a74-3f90-456c-b372-a45393aeb3ca.jpeg\n",
      "  description: >-\n",
      "  authors: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra\n",
      "  venue: >-\n",
      "    NeurIPS 2021\n",
      "  year: 2021\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2106.14405\n",
      "  project_page: https://aihabitat.org/\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Multimodal Shape Completion via IMLE\n",
      "  image: 2021-06-30_772535f6-7463-4bd9-a12c-36c8030b9b9e.png\n",
      "  description: >-\n",
      "  authors: Himanshu Arora, Saurabh Mishra, Shichong Peng, Ke Li, Ali Mahdavi-Amiri\n",
      "  venue: >-\n",
      "    CVPR 2022\n",
      "  year: 2021\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/abs/2106.16237\n",
      "  project_page: https://arxiv.org/abs/2106.16237\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    Discovering Diverse Athletic Jumping Strategies\n",
      "  image: 2021-05-02_87e25aef-ee38-4efe-ab82-5ad3b3edaebb.png\n",
      "  description: >-\n",
      "  authors: Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin\n",
      "  venue: >-\n",
      "    SIGGRAPH 2021\n",
      "  year: 2021\n",
      "  highlight: 1\n",
      "  pdf: https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf\n",
      "  project_page: https://arpspoof.github.io/project/jump/jump.html\n",
      "\n",
      "\n",
      "\n",
      "- title: >-\n",
      "    CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly\n",
      "  image: 2021-04-12_8e49a3c8-3bc1-4910-ab61-559e02920f96.png\n",
      "  description: >-\n",
      "  authors: Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali Mahdavi-Amiri, Hao Zhang.\n",
      "  venue: >-\n",
      "    CVPR 2022\n",
      "  year: 2021\n",
      "  highlight: 1\n",
      "  pdf: https://arxiv.org/pdf/2104.05652.pdf\n",
      "  project_page: https://fenggenyu.github.io/capri.html\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "def gather_plain_text(richtext_obj):\n",
    "    richtext_list = richtext_obj[\"rich_text\"]\n",
    "    plain = \"\"\n",
    "    for rt in richtext_list:\n",
    "        plain = plain + rt[\"plain_text\"]\n",
    "    return plain\n",
    "def parse_thumbnail(thumb_obj):\n",
    "    thumbfs  = thumb_obj[\"files\"]\n",
    "    assert len(thumbfs)>0\n",
    "    thumbf = thumbfs[0]\n",
    "    if thumbf[\"type\"]==\"external\":\n",
    "        thumbnail_link = thumbf[\"external\"][\"url\"]\n",
    "    else:\n",
    "        thumbnail_link = thumbf[\"file\"][\"url\"]\n",
    "    return thumbnail_link\n",
    "def convert2gruvi(datalist, download_thumbnails=True):\n",
    "    if download_thumbnails and not os.path.exists(\"downloads\"):\n",
    "        os.makedirs(\"downloads\")\n",
    "    all_str = \"\"\n",
    "    for i, data in enumerate(datalist):\n",
    "        dID = data[\"id\"]\n",
    "        diprop = data[\"properties\"]\n",
    "        year, title, authors, paper_link, venue = None, None, None, None, None\n",
    "        if diprop.get(\"Date\", None) is not None:\n",
    "            date = diprop[\"Date\"][\"date\"][\"start\"]\n",
    "            year = date[:4]\n",
    "        title = diprop[\"Name\"][\"title\"][0][\"text\"][\"content\"]\n",
    "        if diprop.get(\"Authors\", None) is not None:\n",
    "            authors  = gather_plain_text( diprop[\"Authors\"] )\n",
    "        if diprop.get(\"paper_link\", None) is not None:\n",
    "            paper_link  = gather_plain_text( diprop[\"paper_link\"] )\n",
    "        if diprop.get(\"project_link\", None) is not None:\n",
    "            project_link  = gather_plain_text( diprop[\"project_link\"] )\n",
    "        if diprop.get(\"video_link\", None) is not None:\n",
    "            video_link  = gather_plain_text( diprop[\"video_link\"] )\n",
    "        if diprop.get(\"presentation_link\", None) is not None:\n",
    "            presentation_link  = gather_plain_text( diprop[\"presentation_link\"] )\n",
    "        if diprop.get(\"Venue\", None) is not None:\n",
    "            venue  = gather_plain_text( diprop[\"Venue\"] )\n",
    "        print(date, title)\n",
    "\n",
    "\n",
    "        thumbnail_link = parse_thumbnail(diprop[\"Thumbnail\"])\n",
    "        fsuffix = thumbnail_link.split(\"?\")[0].split(\".\")[-1]\n",
    "        thumb_name = date + '_' + dID + '.' + fsuffix \n",
    "        if download_thumbnails:\n",
    "            print(f'Downloading thumbnail {thumb_name} from {thumbnail_link}')\n",
    "            urllib.request.urlretrieve(thumbnail_link, f\"../images/pubpic/{thumb_name}\")\n",
    "\n",
    "        export_str = f\"\"\"\n",
    "- title: >-\n",
    "    {title}\n",
    "  image: {thumb_name}\n",
    "  description: >-\n",
    "  authors: {authors}\n",
    "  venue: >-\n",
    "    {venue}\n",
    "  year: {year}\n",
    "  highlight: 1\n",
    "  pdf: {paper_link}\n",
    "\"\"\"\n",
    "        if project_link is not None:\n",
    "            export_str = export_str + f\"  project_page: {project_link}\\n\"\n",
    "        if video_link is not None and len(video_link)>0:\n",
    "            export_str = export_str + f\"  video: {video_link}\\n\"\n",
    "        if presentation_link is not None and len(presentation_link)>0:\n",
    "            export_str = export_str + f\"  presentation: {presentation_link}\\n\"\n",
    "        all_str = all_str + export_str+ \"\\n\\n\"\n",
    "    return all_str\n",
    "\n",
    "all_str = convert2gruvi(datalist, download_thumbnails=True)\n",
    "with open(\"../_data/old_manual_publist_stopped_at_2021.yml\", \"r\") as f:\n",
    "    old_str = f.read()\n",
    "with open(\"../_data/publist.yml\", \"w\") as f:\n",
    "    f.write(all_str + \"\\n ################################# BELOW: MANUAL ENTRIES ################################# \\n\" + old_str)\n",
    "print(all_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/local-scratch/localhome/xya120/studio/gruvi/gruvilab.github.io/scripts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'down' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m down\u001b[39m.\u001b[39mkeys(), down[\u001b[39m\"\u001b[39m\u001b[39mnext_cursor\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'down' is not defined"
     ]
    }
   ],
   "source": [
    "down.keys(), down[\"next_cursor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'down' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(down[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m]), down[\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mproperties\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'down' is not defined"
     ]
    }
   ],
   "source": [
    "len(down[\"results\"]), down[\"results\"][1][\"properties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
