,object,id,created_time,last_edited_time,created_by,last_edited_by,cover,icon,parent,archived,properties,url,public_url
0,page,6ba78a9f-244d-494e-b4e7-299a9dda8692,2023-10-26T18:21:00.000Z,2023-10-26T18:30:00.000Z,"{'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}","{'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'gcmic.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/787e60b3-a264-4aed-9e48-4d390886928c/gcmic.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=51e0fc88ed85f4530aa791eefc696965b268a315cf53db5b36a15ed0fe177433&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': []}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': []}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'href': None}]}}",https://www.notion.so/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692,https://yanxg.notion.site/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692
1,page,99d34d16-28ec-4ada-abee-307b42832af2,2023-10-26T02:16:00.000Z,2023-10-26T02:21:00.000Z,"{'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}","{'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opdmulti.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a14d25de-bf9a-4152-8e96-bd50e29c39da/opdmulti.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=9f7e424b37c4085d51d5dec9e85ded5fcbd0b99666983b3a17d47da07f6454c0&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPDMulti/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPDMulti/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPDMulti/', 'href': 'https://3dlg-hcvc.github.io/OPDMulti/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.14087', 'link': {'url': 'https://arxiv.org/abs/2303.14087'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.14087', 'href': 'https://arxiv.org/abs/2303.14087'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPDMulti: Openable Part Detection for Multiple Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPDMulti: Openable Part Detection for Multiple Objects', 'href': None}]}}",https://www.notion.so/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2,https://yanxg.notion.site/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2
2,page,af27631d-4936-4a32-b742-dca9d9379a50,2023-10-25T17:44:00.000Z,2023-10-25T17:52:00.000Z,"{'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4ece1f23-aa46-4ba4-9fc7-35328b2bca56/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=5110914e87720e7e48a03031277b39c4c8e18b036a6baf16c7c5b00bc573dbf4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/shaddr/', 'link': {'url': 'https://qiminchen.github.io/shaddr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/shaddr/', 'href': 'https://qiminchen.github.io/shaddr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH Asia 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH Asia 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'd0404583-7d96-4756-b656-bb910bc1e011', 'name': 'SIGGRAPH Asia', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04889', 'link': {'url': 'https://arxiv.org/abs/2306.04889'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04889', 'href': 'https://arxiv.org/abs/2306.04889'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'href': None}]}}",https://www.notion.so/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50,https://yanxg.notion.site/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50
3,page,8fcbf0dc-f008-47d3-9190-245544f99ad3,2023-11-02T23:49:00.000Z,2023-11-02T23:57:00.000Z,"{'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}","{'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help ""stitch"" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help ""stitch"" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'DiViNeT_long.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1c8a5bf0-df4c-4877-9ba3-57a0efb585d0/DiViNeT_long.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=a3a1ea3aa1b754d1f82c22a874cf82661642dfeb14144246c1e3549ba8be29d4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aditya-vora.github.io/divinetpp/', 'link': {'url': 'https://aditya-vora.github.io/divinetpp/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aditya-vora.github.io/divinetpp/', 'href': 'https://aditya-vora.github.io/divinetpp/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04699', 'link': {'url': 'https://arxiv.org/abs/2306.04699'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04699', 'href': 'https://arxiv.org/abs/2306.04699'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'href': None}]}}",https://www.notion.so/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3,https://yanxg.notion.site/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3
4,page,a6aec9d2-5b4e-485c-b437-baa336078736,2023-10-27T07:36:00.000Z,2023-11-05T06:36:00.000Z,"{'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://jcchen.me/assets/img/publications/poly-diffuse.png', 'type': 'external', 'external': {'url': 'https://jcchen.me/assets/img/publications/poly-diffuse.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://poly-diffuse.github.io/', 'link': {'url': 'https://poly-diffuse.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://poly-diffuse.github.io/', 'href': 'https://poly-diffuse.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.01461', 'link': {'url': 'https://arxiv.org/abs/2306.01461'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.01461', 'href': 'https://arxiv.org/abs/2306.01461'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'href': None}]}}",https://www.notion.so/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736,https://yanxg.notion.site/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736
5,page,52024903-b5ec-47c0-83bf-a5f65531d5f8,2023-10-26T03:19:00.000Z,2023-10-27T20:58:00.000Z,"{'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S', 'link': {'url': 'https://tangshitao.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S', 'href': 'https://tangshitao.github.io/'}, {'type': 'text', 'text': {'content': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper introduces \\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\url{', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper introduces \\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\url{', 'href': None}, {'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}, {'type': 'text', 'text': {'content': '}.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '}.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'mvdiffusion_teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1ae24cf4-18ab-4c9b-a147-dd9297c251bd/mvdiffusion_teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=37d40d1a3d00736e7d447b5ca28746d2a2fb72b5b3a9d9e5538d4ad7263543b8&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.01097', 'link': {'url': 'https://arxiv.org/abs/2307.01097'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.01097', 'href': 'https://arxiv.org/abs/2307.01097'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'href': None}]}}",https://www.notion.so/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8,https://yanxg.notion.site/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8
6,page,e465f55d-74c4-42c4-bb93-c056ef05e56a,2023-10-26T02:49:00.000Z,2023-10-26T02:51:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'href': None}]}}",https://www.notion.so/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a,https://yanxg.notion.site/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a
7,page,e40d79ba-70ee-4aee-86a4-023047c0b26d,2023-10-26T02:48:00.000Z,2023-10-26T02:51:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/LDM_correspondences/', 'link': {'url': 'https://ubc-vision.github.io/LDM_correspondences/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/LDM_correspondences/', 'href': 'https://ubc-vision.github.io/LDM_correspondences/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2305.15581', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2305.15581', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'href': None}]}}",https://www.notion.so/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d,https://yanxg.notion.site/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d
8,page,062f2450-a093-4a1d-afc7-127b9ff2af39,2023-10-26T02:00:00.000Z,2023-10-26T02:09:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\xa0Joakim Bruslund Haurum,\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\xa0Joakim Bruslund Haurum,\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Graham W Taylor,\xa0Paul Fieguth', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Graham W Taylor,\xa0Paul Fieguth', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'bioscan1m.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f8b23333-750f-442c-88b7-76d10c28bb28/bioscan1m.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=213476c9a39cf2f07a062d84e84c67d52b809d11d166ed65934208086fd5fb3d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://biodiversitygenomics.net/1M_insects/', 'link': {'url': 'https://biodiversitygenomics.net/1M_insects/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://biodiversitygenomics.net/1M_insects/', 'href': 'https://biodiversitygenomics.net/1M_insects/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.10455', 'link': {'url': 'https://arxiv.org/abs/2307.10455'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.10455', 'href': 'https://arxiv.org/abs/2307.10455'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'href': None}]}}",https://www.notion.so/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39,https://yanxg.notion.site/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39
9,page,b595d4de-624c-430c-a679-0429d50269dc,2023-10-25T23:13:00.000Z,2023-10-25T23:25:00.000Z,"{'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}","{'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yanshu Zhang*,\xa0Shichong Peng*,\xa0Seyed Alireza Moazenipourasil,\xa0Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yanshu Zhang*,\xa0Shichong Peng*,\xa0Seyed Alireza Moazenipourasil,\xa0Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\xa0geometry editing,\xa0object manipulation,\xa0texture transfer, and\xa0exposure control.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\xa0geometry editing,\xa0object manipulation,\xa0texture transfer, and\xa0exposure control.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'papr-thumbnail2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/cb623732-7a24-44bf-b9de-f5bd5be90bad/papr-thumbnail2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=c82ba1965211881d35efb48dfbc9b09d0f6d60b961f273b265ee73f926c0e3ab&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://zvict.github.io/papr/', 'link': {'url': 'https://zvict.github.io/papr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://zvict.github.io/papr/', 'href': 'https://zvict.github.io/papr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.11086', 'link': {'url': 'https://arxiv.org/abs/2307.11086'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.11086', 'href': 'https://arxiv.org/abs/2307.11086'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PAPR: Proximity Attention Point Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PAPR: Proximity Attention Point Rendering', 'href': None}]}}",https://www.notion.so/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc,https://yanxg.notion.site/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc
10,page,e9f11fd2-9460-41d8-8179-5cd059dcdb98,2023-10-25T17:45:00.000Z,2023-11-05T06:27:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural architecture based on Diffusion Models\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\nIn the latter task, for instance, the proposed system takes a set of room layouts\nas polygonal curves in the top-down view and aligns the room layout pieces by\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\nof room layouts. A surprising discovery of the paper is that the simple use of\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\nconditional generation process. To enable learning of an end-to-end neural system,\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\nits production pipeline, where pieces are room layouts constructed by augmented\nreality App by real-estate consumers. The qualitative and quantitative evaluations\ndemonstrate that our approach outperforms the competing methods by significant\nmargins in all the tasks. We will publicly share all our code and data.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural architecture based on Diffusion Models\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\nIn the latter task, for instance, the proposed system takes a set of room layouts\nas polygonal curves in the top-down view and aligns the room layout pieces by\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\nof room layouts. A surprising discovery of the paper is that the simple use of\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\nconditional generation process. To enable learning of an end-to-end neural system,\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\nits production pipeline, where pieces are room layouts constructed by augmented\nreality App by real-estate consumers. The qualitative and quantitative evaluations\ndemonstrate that our approach outperforms the competing methods by significant\nmargins in all the tasks. We will publicly share all our code and data.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'dataset2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1dfce877-56d4-4f3c-9c8d-283e92dd4c10/dataset2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=d9dc10e4585ad1c1c10154db44773cc1b2b0889f899ab3a6ca0ba5b035e5536c&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/sepidsh/PuzzleFussion', 'link': {'url': 'https://github.com/sepidsh/PuzzleFussion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/sepidsh/PuzzleFussion', 'href': 'https://github.com/sepidsh/PuzzleFussion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'link': {'url': 'https://arxiv.org/pdf/2211.13785v2.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'href': 'https://arxiv.org/pdf/2211.13785v2.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'href': None}]}}",https://www.notion.so/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98,https://yanxg.notion.site/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98
11,page,10ad0ead-0668-44d3-8be8-e1627048574a,2023-10-25T18:01:00.000Z,2023-10-26T02:17:00.000Z,"{'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-20', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0c1a87a7-75d8-4b7f-8a5c-9cd4a02feca6/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=493e09e3a57acaa906c18c9750b4f36996ea4f5a07bce2b2610f15e0edcfd417&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sepidsh.github.io/floorplan_restore/', 'link': {'url': 'https://sepidsh.github.io/floorplan_restore/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sepidsh.github.io/floorplan_restore/', 'href': 'https://sepidsh.github.io/floorplan_restore/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'BMVC 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BMVC 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2c73299d-5d90-4ce0-bdf0-5a4fb4bf7e49', 'name': 'BMVC', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2206.00645.pdf', 'link': {'url': 'https://arxiv.org/pdf/2206.00645.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2206.00645.pdf', 'href': 'https://arxiv.org/pdf/2206.00645.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'href': None}]}}",https://www.notion.so/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a,https://yanxg.notion.site/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a
12,page,1b12c251-91ff-4e6d-9098-46e325abba8f,2023-10-26T01:54:00.000Z,2023-10-26T02:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-06', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sriram Yenamandra, Arun Ramachandran,\xa0Karmesh Yadav, Austin Wang,\xa0Mukul Khanna,\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\xa0Zsolt Kira,\xa0Manolis Savva,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sriram Yenamandra, Arun Ramachandran,\xa0Karmesh Yadav, Austin Wang,\xa0Mukul Khanna,\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\xa0Zsolt Kira,\xa0Manolis Savva,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Devendra Chaplot,\xa0Dhruv Batra,\xa0Roozbeh Mottaghi,\xa0Yonatan Bisk,\xa0Chris Paxton', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Devendra Chaplot,\xa0Dhruv Batra,\xa0Roozbeh Mottaghi,\xa0Yonatan Bisk,\xa0Chris Paxton', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'this https URL', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'this https URL', 'href': 'https://ovmm.github.io/'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'ovmm.jpeg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/5540b6c9-77d5-4d68-89cd-07a836f8d185/ovmm.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=cf8b67a1e2b3727dc96cdb0dc3f004ffe482f0723b346954f10bebcb7f29ec29&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ovmm.github.io/', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ovmm.github.io/', 'href': 'https://ovmm.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CoRL 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CoRL 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'link': {'url': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'href': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.11565', 'link': {'url': 'https://arxiv.org/abs/2306.11565'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.11565', 'href': 'https://arxiv.org/abs/2306.11565'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'href': None}]}}",https://www.notion.so/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f,https://yanxg.notion.site/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f
13,page,cdc1a96a-7fa0-4e3b-8f2b-4ba676ac8d73,2023-10-26T01:54:00.000Z,2023-10-26T21:01:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Enrico Cancelli,\xa0Tommaso Campari,\xa0Luciano Serafini,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Enrico Cancelli,\xa0Tommaso Campari,\xa0Luciano Serafini,\xa0Angel X. Chang', 'href': None}, {'type': 'text', 'text': {'content': ',\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ',\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Lamberto Ballan', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Lamberto Ballan', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'prox.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/98c951ee-51cf-449f-b00f-3974721133f9/prox.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=de850a0eedadd5f88694363c3e6e131cc79d83a3de7b23dcd7b2883a5cc93b96&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'href': None}]}}",https://www.notion.so/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73,https://yanxg.notion.site/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73
14,page,96b1c296-9893-4784-801a-f0daed0f43ef,2023-09-18T23:26:00.000Z,2023-09-25T23:41:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen,\xa0Ronghang Hu,\xa0Xinlei Chen,\xa0Matthias Nießner,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen,\xa0Ronghang Hu,\xa0Xinlei Chen,\xa0Matthias Nießner,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'unit3d.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/412818c8-c973-4209-b9a6-8cad6cbdf2c4/unit3d.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=92cd64d437338f51a0caa07c43f7bb00be24ed60cc79ca82142872bbe8673a05&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'href': None}]}}",https://www.notion.so/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef,https://yanxg.notion.site/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef
15,page,97a335a8-5de9-4af8-97fe-b8cdbbc02c06,2023-09-18T22:17:00.000Z,2023-09-18T22:23:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'href': None}]}}",https://www.notion.so/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06,https://yanxg.notion.site/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06
16,page,c34ba525-ac8d-4b9b-91e7-d412e7fce348,2023-09-18T21:52:00.000Z,2023-09-18T22:26:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiayi Liu,\xa0Ali Mahdavi-Amiri, Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiayi Liu,\xa0Ali Mahdavi-Amiri, Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-09-18_14-56-16.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/93befd22-b013-4613-bee2-c9e2bb53c1a5/Snipaste_2023-09-18_14-56-16.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=591c80209aa838cfd21374e8348e97ab70e9dce74e3d7e0263d0f5b63e259674&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.139Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/paris/', 'link': {'url': 'https://3dlg-hcvc.github.io/paris/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/paris/', 'href': 'https://3dlg-hcvc.github.io/paris/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735', 'link': {'url': 'https://arxiv.org/abs/2308.07391'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735', 'href': 'https://arxiv.org/abs/2308.07391'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'href': None}]}}",https://www.notion.so/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348,https://yanxg.notion.site/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348
17,page,76c3667b-3a91-4d35-94e6-0d33b0b6a293,2023-09-18T21:52:00.000Z,2023-09-18T22:24:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://sked-paper.github.io/resources/teaser.png', 'type': 'external', 'external': {'url': 'https://sked-paper.github.io/resources/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sked-paper.github.io/', 'link': {'url': 'https://sked-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sked-paper.github.io/', 'href': 'https://sked-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735v3', 'link': {'url': 'https://arxiv.org/abs/2303.10735v3'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735v3', 'href': 'https://arxiv.org/abs/2303.10735v3'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SKED: Sketch-guided Text-based 3D Editing', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SKED: Sketch-guided Text-based 3D Editing', 'href': None}]}}",https://www.notion.so/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293,https://yanxg.notion.site/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293
18,page,40055d39-bb6d-44ea-8d68-453f503ef32d,2023-09-11T20:58:00.000Z,2023-10-26T02:00:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Maham Tanveer, Yizhi Wang,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Maham Tanveer, Yizhi Wang,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'href': None}]}}",https://www.notion.so/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d,https://yanxg.notion.site/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d
19,page,770d6d8f-f95b-46d8-8cab-db33928836de,2023-09-11T20:58:00.000Z,2023-10-26T01:58:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'abstract-b1185106.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/aaeef2a9-fa43-468e-a201-2ed47a1c9214/abstract-b1185106.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=27647c2d57d35d0f1b2edc7e26ae08a11c6af8516a0c2b358daa13571caf3a66&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multi3drefer/', 'link': {'url': 'https://3dlg-hcvc.github.io/multi3drefer/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multi3drefer/', 'href': 'https://3dlg-hcvc.github.io/multi3drefer/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2309.05251', 'link': {'url': 'https://arxiv.org/abs/2309.05251'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2309.05251', 'href': 'https://arxiv.org/abs/2309.05251'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'href': None}]}}",https://www.notion.so/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de,https://yanxg.notion.site/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de
20,page,fd9e8fb3-ecc3-4c62-abb8-631e62ebd6e1,2023-08-05T03:12:00.000Z,2023-09-09T22:41:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Xue Bin Peng', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xue Bin Peng', 'href': None}, {'type': 'text', 'text': {'content': ', Kayvon Fatahalian', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ', Kayvon Fatahalian', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'href': None}]}}",https://www.notion.so/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1,https://yanxg.notion.site/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1
21,page,d34f0d36-07ec-4c96-a246-3213aa65f9e0,2023-08-05T03:11:00.000Z,2023-08-05T03:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Synthesizing Physical Character-Scene Interactions', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Synthesizing Physical Character-Scene Interactions', 'href': None}]}}",https://www.notion.so/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0,https://yanxg.notion.site/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0
22,page,2a5a8d54-c945-4a54-bc8f-ea9130197bc3,2023-08-05T03:08:00.000Z,2023-08-05T03:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'href': None}]}}",https://www.notion.so/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3,https://yanxg.notion.site/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3
23,page,59e4ea73-0aa4-4d38-8eb7-88a3428b942e,2023-06-14T19:18:00.000Z,2023-06-14T22:27:00.000Z,"{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': ""Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations."", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations."", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'realisticEditing.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/ea8fb772-c58b-4bcd-b06a-ffb4eddbd84b/realisticEditing.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=af6722bb420ae4764f62e06934b9dcaea7adb633d09fd0d0504ec666c51f022a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'link': {'url': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'href': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Realistic Saliency Guided Image Enhancement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Realistic Saliency Guided Image Enhancement', 'href': None}]}}",https://www.notion.so/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e,https://yanxg.notion.site/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e
24,page,6a1f6f77-f792-415f-a4b9-15a19cda6d15,2023-05-13T22:38:00.000Z,2023-06-14T19:28:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'intrinsicFlash.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4ce64201-44b3-4acf-a0e1-02215d5c0add/intrinsicFlash.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=13916ad0396d78b88f0538700320906f854128971145a5f98ae0f4b5814063bf&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Computational Flash Photography through Intrinsics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Computational Flash Photography through Intrinsics', 'href': None}]}}",https://www.notion.so/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15,https://yanxg.notion.site/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15
25,page,2ff4a77a-2833-429c-b89c-3c0c2873aeae,2023-10-25T19:16:00.000Z,2023-10-25T19:19:00.000Z,"{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}","{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'href': None}, {'type': 'text', 'text': {'content': 'https://github.com/mehranagh20/AdaIMLE', 'link': {'url': 'https://github.com/mehranagh20/AdaIMLE'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/mehranagh20/AdaIMLE', 'href': 'https://github.com/mehranagh20/AdaIMLE'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2023-10-25 at 12.18.12 PM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f6dc87d0-a612-4235-a8f1-cd49492d7456/Screenshot_2023-10-25_at_12.18.12_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=df7e081e9444f46aa561d690f823817e62a75e24c8b2e6e9d278befb7ddabce4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mehranagh20.github.io/AdaIMLE/', 'link': {'url': 'https://mehranagh20.github.io/AdaIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mehranagh20.github.io/AdaIMLE/', 'href': 'https://mehranagh20.github.io/AdaIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/xKt6YYY4hq8', 'link': {'url': 'https://youtu.be/xKt6YYY4hq8'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/xKt6YYY4hq8', 'href': 'https://youtu.be/xKt6YYY4hq8'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'link': {'url': 'https://openreview.net/pdf?id=CNq0JvrDfw'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'href': 'https://openreview.net/pdf?id=CNq0JvrDfw'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'href': None}]}}",https://www.notion.so/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae,https://yanxg.notion.site/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae
26,page,c08df0c7-8898-4d63-95ee-20e93f9f5b8f,2023-05-15T03:18:00.000Z,2023-10-26T02:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'eom-teaser.jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4419448b-ae1c-4c46-9acc-f315f86a53fd/eom-teaser.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=d47986d2ed7ef2dd1f70a287565e9b7bbeed05ada002ec96f13b39f4b7e1ace9&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://wijmans.xyz/publication/eom/', 'link': {'url': 'https://wijmans.xyz/publication/eom/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://wijmans.xyz/publication/eom/', 'href': 'https://wijmans.xyz/publication/eom/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.13261', 'link': {'url': 'https://arxiv.org/abs/2301.13261'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.13261', 'href': 'https://arxiv.org/abs/2301.13261'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'href': None}]}}",https://www.notion.so/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f,https://yanxg.notion.site/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f
27,page,a71873ed-851a-4a82-a39c-d96ac1486e2b,2023-05-14T00:54:00.000Z,2023-10-25T19:13:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-24', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\neration tasks.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\neration tasks.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/32c28145-f878-45b9-8c86-e4d6cc3c82dd/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=6afd17cdfc282ac5721a47146151acb3c5942ebff1cfc283894eeb57f75500cb&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://hnc-cad.github.io/', 'link': {'url': 'https://hnc-cad.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://hnc-cad.github.io/', 'href': 'https://hnc-cad.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'link': {'url': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'href': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2307.00149.pdf', 'link': {'url': 'https://arxiv.org/pdf/2307.00149.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2307.00149.pdf', 'href': 'https://arxiv.org/pdf/2307.00149.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'href': None}]}}",https://www.notion.so/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b,https://yanxg.notion.site/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b
28,page,4e0e1234-968a-4b5b-af83-3f46de5449b5,2023-05-14T00:34:00.000Z,2023-10-26T02:18:00.000Z,"{'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Karmesh Yadav,\xa0Ram Ramrakhya,\xa0Santhosh Kumar Ramakrishnan,\xa0Theo Gervet,\xa0John Turner,\xa0Aaron Gokaslan,\xa0Noah Maestre,\xa0Angel Xuan Chang,\xa0Dhruv Batra,\xa0Manolis Savva,\xa0Alexander William Clegg,\xa0Devendra Singh Chaplot', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Karmesh Yadav,\xa0Ram Ramrakhya,\xa0Santhosh Kumar Ramakrishnan,\xa0Theo Gervet,\xa0John Turner,\xa0Aaron Gokaslan,\xa0Noah Maestre,\xa0Angel Xuan Chang,\xa0Dhruv Batra,\xa0Manolis Savva,\xa0Alexander William Clegg,\xa0Devendra Singh Chaplot', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://angelxuanchang.github.io/files/hm3dsem.png', 'type': 'external', 'external': {'url': 'https://angelxuanchang.github.io/files/hm3dsem.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d-semantics/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d-semantics/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d-semantics/', 'href': 'https://aihabitat.org/datasets/hm3d-semantics/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2210.05633.pdf', 'link': {'url': 'https://arxiv.org/pdf/2210.05633.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2210.05633.pdf', 'href': 'https://arxiv.org/pdf/2210.05633.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Semantics Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Semantics Dataset', 'href': None}]}}",https://www.notion.so/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5,https://yanxg.notion.site/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5
29,page,09e8bfb7-cc10-4c12-91be-940f77be6a1e,2023-05-13T23:06:00.000Z,2023-05-13T23:20:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'aronet[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4f445b0e-49d2-48f5-a046-7eede103bfb8/aronet1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=1a22ec31a8e251c066feec8a0fb86ef913c8516e6a7d72ac2a1b37a73481da95&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.139Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'href': None}]}}",https://www.notion.so/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e,https://yanxg.notion.site/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e
30,page,fb5c2a9b-b3da-4f72-9213-ab918b301345,2023-05-13T23:03:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/', 'link': {'url': 'https://cuf-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/', 'href': 'https://cuf-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/cuf.pdf', 'link': {'url': 'https://cuf-paper.github.io/cuf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/cuf.pdf', 'href': 'https://cuf-paper.github.io/cuf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CUF: Continuous Upsampling Filters', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CUF: Continuous Upsampling Filters', 'href': None}]}}",https://www.notion.so/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345,https://yanxg.notion.site/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345
31,page,d050430b-4429-465d-9ad0-3697d4124f37,2023-05-13T23:01:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'href': None}]}}",https://www.notion.so/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37,https://yanxg.notion.site/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37
32,page,14c20053-18fd-405a-bcd5-64ff012adf1c,2023-05-13T23:01:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'href': None}]}}",https://www.notion.so/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c,https://yanxg.notion.site/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c
33,page,c5d52e42-8a31-439a-b18a-110cbaf1692d,2023-05-13T23:01:00.000Z,2023-06-13T03:48:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'href': None}]}}",https://www.notion.so/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d,https://yanxg.notion.site/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d
34,page,e597ffc2-5b25-4391-b4a1-6f533e25e983,2023-05-13T22:59:00.000Z,2023-05-13T23:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cvpr23_mobilenerf[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e3ff6c29-1c6e-49bd-aadd-192b8d03620b/cvpr23_mobilenerf1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=ae296568fd7b9675dabc9209deeeecbc26c6e1ec124ecf8ba625b8048ce10943&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mobile-nerf.github.io/', 'link': {'url': 'https://mobile-nerf.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mobile-nerf.github.io/', 'href': 'https://mobile-nerf.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'link': {'url': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'href': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'link': None}, 'annotations': {'bold': True, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'href': None}]}}",https://www.notion.so/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983,https://yanxg.notion.site/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983
35,page,11d5ea9f-22c9-42a4-80fb-a9ab21e7b616,2023-05-13T22:56:00.000Z,2023-05-13T23:11:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-event-camera[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f982027f-5810-4b38-9b08-73b69c219ee9/2023-cvpr-event-camera1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=3073d21b71149587df84ce7082728abba313076e97b4d1d3007fbca4463a0be2&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'href': None}]}}",https://www.notion.so/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616,https://yanxg.notion.site/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616
36,page,86fb7dc9-1442-41a1-8d90-8291a3e07865,2023-05-13T22:56:00.000Z,2023-05-13T23:10:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-house-diffusion[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/af4ac260-ebc3-4317-91ce-27e164dbfee3/2023-cvpr-house-diffusion1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=0ef6e9f2e6f48891454beeea02cdf30e1c85ba7c2e56d33c6ac014f6f5f0c99b&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/aminshabani/house_diffusion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/aminshabani/house_diffusion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.13287'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.13287'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'href': None}]}}",https://www.notion.so/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865,https://yanxg.notion.site/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865
37,page,73a510c5-02c6-4a27-9b9a-09e875390a45,2023-05-13T22:38:00.000Z,2023-05-14T00:08:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-neumap[1].jpg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/656beba7-f157-4b8a-ad8c-ee8fd1317813/2023-cvpr-neumap1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=cabb93c59cee1823f17e9306fb724a12ef345d627ac17fda9da6c222d6efe72f&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/Tangshitao/NeuMap'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/Tangshitao/NeuMap'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.11177'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.11177'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'href': None}]}}",https://www.notion.so/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45,https://yanxg.notion.site/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45
38,page,555d2cb2-472e-4c25-bd27-ae0be3ea3524,2023-06-14T22:40:00.000Z,2023-06-15T03:14:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-05', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png', 'type': 'external', 'external': {'url': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Transaction of Visualization and Computer Graphics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Transaction of Visualization and Computer Graphics', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SAC-GAN: Structure-Aware Image Composition', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SAC-GAN: Structure-Aware Image Composition', 'href': None}]}}",https://www.notion.so/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524,https://yanxg.notion.site/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524
39,page,c9b9e48d-4f8c-48b8-ba80-17396fad3344,2023-06-14T20:15:00.000Z,2023-06-14T20:19:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ACCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ACCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1580931b-4de8-4792-97a6-9c715191a9b9', 'name': 'ACCV', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Vectorizing Building Blueprints', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Vectorizing Building Blueprints', 'href': None}]}}",https://www.notion.so/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344,https://yanxg.notion.site/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344
40,page,89b26044-a63b-46bb-8335-b310c47194f2,2023-05-15T03:14:00.000Z,2023-05-15T03:16:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yongsen Mao, Yiming Zhang,\xa0Hanxiao Jiang,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yongsen Mao, Yiming Zhang,\xa0Hanxiao Jiang,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'multiscan.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/27b5a4e7-1a49-4d92-be15-57ac47feaaa6/multiscan.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=b3c7f403eaa048e7eb62c1e8b66b178448f79617ed0de615b73b9fc65ebb8c70&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multiscan/', 'link': {'url': 'https://3dlg-hcvc.github.io/multiscan/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multiscan/', 'href': 'https://3dlg-hcvc.github.io/multiscan/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'link': {'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'href': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'href': None}]}}",https://www.notion.so/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2,https://yanxg.notion.site/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2
41,page,62603cb1-eb87-4492-9360-62a4f88afdc3,2023-10-26T01:45:00.000Z,2023-10-26T01:52:00.000Z,"{'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}","{'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shichong Peng, Alireza Moazeni, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shichong Peng, Alireza Moazeni, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.gif', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a7cc1543-a9bc-4308-b103-9565c87941c6/teaser.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=03e5c06fadf4e8bf6dac1e720a674a155e4ff3d9e9eb74a26dbbcd830423dc25&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://niopeng.github.io/CHIMLE/', 'link': {'url': 'https://niopeng.github.io/CHIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://niopeng.github.io/CHIMLE/', 'href': 'https://niopeng.github.io/CHIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'link': {'url': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'href': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.14286', 'link': {'url': 'https://arxiv.org/abs/2211.14286'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.14286', 'href': 'https://arxiv.org/abs/2211.14286'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'href': None}]}}",https://www.notion.so/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3,https://yanxg.notion.site/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3
42,page,6acd66a6-a889-4438-bcc7-551da232ee3f,2023-05-15T03:07:00.000Z,2023-05-15T03:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen, Qirui Wu,\xa0Matthias Nießner,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen, Qirui Wu,\xa0Matthias Nießner,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'd3net.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/dd7d9ce0-1958-445f-8fdd-9bc398c939ac/d3net.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=bd242b1385bf8a427f08ed7c754cc11082709d01ec7d2d6a23c5af43449c4188&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.140Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://daveredrum.github.io/D3Net/', 'link': {'url': 'https://daveredrum.github.io/D3Net/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://daveredrum.github.io/D3Net/', 'href': 'https://daveredrum.github.io/D3Net/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.01551', 'link': {'url': 'https://arxiv.org/abs/2112.01551'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.01551', 'href': 'https://arxiv.org/abs/2112.01551'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'href': None}]}}",https://www.notion.so/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f,https://yanxg.notion.site/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f
43,page,7cc9a376-f7a0-47e6-b725-ee0a6d7550de,2023-05-15T03:03:00.000Z,2023-05-15T03:08:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hanxiao Jiang,\xa0Yongsen Mao, Manolis Savva,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hanxiao Jiang,\xa0Yongsen Mao, Manolis Savva,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opd.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0858bf93-e96c-477a-9774-efacd682ce29/opd.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=9f69ccfed8b86cebe4edce33da5a857006b0f90f70a67e20af7cb9d85087f530&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPD/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPD/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPD/', 'href': 'https://3dlg-hcvc.github.io/OPD/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2203.16421', 'link': {'url': 'https://arxiv.org/abs/2203.16421'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2203.16421', 'href': 'https://arxiv.org/abs/2203.16421'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPD: Single-view 3D Openable Part Detection', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPD: Single-view 3D Openable Part Detection', 'href': None}]}}",https://www.notion.so/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de,https://yanxg.notion.site/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de
44,page,4ba354c2-1b60-493d-97d3-1d9213f4c3df,2023-06-14T23:20:00.000Z,2023-06-14T23:58:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Robotics and Automation Letters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Robotics and Automation Letters', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'href': None}]}}",https://www.notion.so/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df,https://yanxg.notion.site/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df
45,page,315625ea-6b60-4cf6-b1df-990c9c46a53b,2023-05-15T03:10:00.000Z,2023-05-15T03:13:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-09-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sanjay Haresh,\xa0Xiaohao Sun,\xa0Hanxiao Jiang,\xa0Angel X. Chang,\xa0Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sanjay Haresh,\xa0Xiaohao Sun,\xa0Hanxiao Jiang,\xa0Angel X. Chang,\xa0Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dhoi.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0d1ee7fa-0137-43f3-838a-9fad087503cc/3dhoi.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=9557215d9b1b7e5891f93530643da25942b4686bccf243abc2b2f37c54b1b2e6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.143Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3dhoi/', 'link': {'url': 'https://3dlg-hcvc.github.io/3dhoi/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3dhoi/', 'href': 'https://3dlg-hcvc.github.io/3dhoi/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2209.05612', 'link': {'url': 'https://arxiv.org/abs/2209.05612'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2209.05612', 'href': 'https://arxiv.org/abs/2209.05612'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'href': None}]}}",https://www.notion.so/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b,https://yanxg.notion.site/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b
46,page,d678f388-423a-436a-8fb4-e65ec066498c,2023-06-14T23:13:00.000Z,2023-06-14T23:58:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-07-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'link': {'url': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'href': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2205.14313', 'link': {'url': 'https://arxiv.org/abs/2205.14313'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2205.14313', 'href': 'https://arxiv.org/abs/2205.14313'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'href': None}]}}",https://www.notion.so/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c,https://yanxg.notion.site/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c
47,page,c98548ee-e3c8-4015-8b44-f2af87f8c61f,2023-05-15T02:59:00.000Z,2023-05-15T03:08:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-31', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yasaman Etesam, Leon Kochiev,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yasaman Etesam, Leon Kochiev,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dvqa.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/9525ee57-f626-43f5-a153-603c609aa764/3dvqa.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=09409d2db6071854d280ea23a78e2972339d85878ceebdaa8942156add1a7307&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3DVQA/', 'link': {'url': 'https://3dlg-hcvc.github.io/3DVQA/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3DVQA/', 'href': 'https://3dlg-hcvc.github.io/3DVQA/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CRV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CRV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '8492aa9e-2202-4887-9296-8c313873f664', 'name': 'CRV', 'color': 'green'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ieeexplore.ieee.org/document/9866910', 'link': {'url': 'https://ieeexplore.ieee.org/document/9866910'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ieeexplore.ieee.org/document/9866910', 'href': 'https://ieeexplore.ieee.org/document/9866910'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': '3DVQA: Visual Question Answering for 3D Environments', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DVQA: Visual Question Answering for 3D Environments', 'href': None}]}}",https://www.notion.so/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f,https://yanxg.notion.site/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f
48,page,93aea945-8b99-4fa3-aaef-ae5b7d9e943e,2023-05-14T00:50:00.000Z,2023-06-15T05:34:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cad_rand[1].png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8f4caa5c-6a35-406b-aed4-8a8595b252ec/cad_rand1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=622d9d596658b3d8d7ffdd44e50739a0bef019e38d6807636bfbb8bccbe181d6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://samxuxiang.github.io/skexgen/index.html', 'link': {'url': 'https://samxuxiang.github.io/skexgen/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://samxuxiang.github.io/skexgen/index.html', 'href': 'https://samxuxiang.github.io/skexgen/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2207.04632', 'link': {'url': 'https://arxiv.org/abs/2207.04632'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2207.04632', 'href': 'https://arxiv.org/abs/2207.04632'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'href': None}]}}",https://www.notion.so/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e,https://yanxg.notion.site/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e
49,page,7d93afe8-9aea-4cba-b951-e14e6849e611,2023-06-14T20:19:00.000Z,2023-06-14T20:21:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-04-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://heat-structured-reconstruction.github.io/', 'link': {'url': 'https://heat-structured-reconstruction.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://heat-structured-reconstruction.github.io/', 'href': 'https://heat-structured-reconstruction.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2111.15143', 'link': {'url': 'https://arxiv.org/abs/2111.15143'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2111.15143', 'href': 'https://arxiv.org/abs/2111.15143'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'href': None}]}}",https://www.notion.so/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611,https://yanxg.notion.site/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611
50,page,a0971bb6-97d2-4960-a91b-747093d53f8b,2023-06-14T22:47:00.000Z,2023-06-15T03:14:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://qiminchen.github.io/unist/images/teaser.svg', 'type': 'external', 'external': {'url': 'https://qiminchen.github.io/unist/images/teaser.svg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/unist/', 'link': {'url': 'https://qiminchen.github.io/unist/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/unist/', 'href': 'https://qiminchen.github.io/unist/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'link': {'url': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'href': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'href': None}]}}",https://www.notion.so/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b,https://yanxg.notion.site/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b
51,page,386b9443-f01c-4dbe-b712-18d7b1910e35,2023-06-14T20:21:00.000Z,2023-06-14T20:22:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICRA 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICRA 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2d02ae48-7ae4-4c33-9b05-fb8723245ad7', 'name': 'ICRA', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Single User WiFi Structure from Motion in the Wild', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single User WiFi Structure from Motion in the Wild', 'href': None}]}}",https://www.notion.so/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35,https://yanxg.notion.site/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35
52,page,23711644-bc80-4dc8-b9f6-5a112d0a4200,2023-05-15T03:24:00.000Z,2023-06-14T19:19:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hm3d.jpeg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c4db31f4-116e-4d67-8189-a9eb0e7a1e5e/hm3d.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=e4ee64162ce884cbdc934be0cd841867bce5378b048f345fe20f868d89586084&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d/', 'href': 'https://aihabitat.org/datasets/hm3d/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.08238', 'link': {'url': 'https://arxiv.org/abs/2109.08238'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.08238', 'href': 'https://arxiv.org/abs/2109.08238'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'href': None}]}}",https://www.notion.so/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200,https://yanxg.notion.site/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200
53,page,bcca4a74-3f90-456c-b372-a45393aeb3ca,2023-05-15T03:21:00.000Z,2023-05-15T03:27:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hab2.jpeg', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b5c2b23a-6b97-45c9-bd38-8ff135a65906/hab2.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=22e1991904ec4fb0b81b910f26492a9036128111baa61476e75530eb5ae632f4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.142Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/', 'link': {'url': 'https://aihabitat.org/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/', 'href': 'https://aihabitat.org/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.14405', 'link': {'url': 'https://arxiv.org/abs/2106.14405'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.14405', 'href': 'https://arxiv.org/abs/2106.14405'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'href': None}]}}",https://www.notion.so/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca,https://yanxg.notion.site/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca
54,page,772535f6-7463-4bd9-a12c-36c8030b9b9e,2023-06-14T22:36:00.000Z,2023-06-15T03:13:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-06-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Himanshu Arora,\xa0Saurabh Mishra,\xa0Shichong Peng,\xa0Ke Li,\xa0Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Himanshu Arora,\xa0Saurabh Mishra,\xa0Shichong Peng,\xa0Ke Li,\xa0Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-06-14_20-13-32.png', 'type': 'file', 'file': {'url': 'https://s3.us-west-2.amazonaws.com/secure.notion-static.com/5610ebd8-2712-46d0-93e6-c42a6e3336ac/Snipaste_2023-06-14_20-13-32.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231105%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231105T063644Z&X-Amz-Expires=3600&X-Amz-Signature=95d38198919877b64da023ae1e86278187871c83be9cf87d57eeada82138cb5d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2023-11-05T07:36:44.141Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multimodal Shape Completion via IMLE', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multimodal Shape Completion via IMLE', 'href': None}]}}",https://www.notion.so/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e,https://yanxg.notion.site/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e
55,page,87e25aef-ee38-4efe-ab82-5ad3b3edaebb,2023-06-14T23:59:00.000Z,2023-06-15T00:02:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-05-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://arpspoof.github.io/project/jump/teaser.png', 'type': 'external', 'external': {'url': 'https://arpspoof.github.io/project/jump/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arpspoof.github.io/project/jump/jump.html', 'link': {'url': 'https://arpspoof.github.io/project/jump/jump.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arpspoof.github.io/project/jump/jump.html', 'href': 'https://arpspoof.github.io/project/jump/jump.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'link': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'href': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Discovering Diverse Athletic Jumping Strategies', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Discovering Diverse Athletic Jumping Strategies', 'href': None}]}}",https://www.notion.so/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb,https://yanxg.notion.site/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb
56,page,8e49a3c8-3bc1-4910-ab61-559e02920f96,2023-06-14T22:50:00.000Z,2023-06-15T03:12:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,"{'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-04-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://fenggenyu.github.io/images/capri/teaser.png', 'type': 'external', 'external': {'url': 'https://fenggenyu.github.io/images/capri/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://fenggenyu.github.io/capri.html', 'link': {'url': 'https://fenggenyu.github.io/capri.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://fenggenyu.github.io/capri.html', 'href': 'https://fenggenyu.github.io/capri.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2104.05652.pdf', 'link': {'url': 'https://arxiv.org/pdf/2104.05652.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2104.05652.pdf', 'href': 'https://arxiv.org/pdf/2104.05652.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'href': None}]}}",https://www.notion.so/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96,https://yanxg.notion.site/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96
